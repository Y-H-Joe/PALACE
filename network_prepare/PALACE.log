root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([1, 30])
root - WARNING - train_PALACE:: X is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - =======================PALACE: predicting...=======================
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1376, -4.0070, -2.3943, -2.1454,  2.4868, -1.3179, -1.1943,
           3.0841,  2.5718, -2.6295, -1.7865, -2.9868, -2.6693,  3.1409,
           4.5320,  0.2978, -1.8552,  3.1189, -2.8019, -1.3041, -2.7857,
          -3.0881]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1430, -4.0313, -2.3857, -2.1391,  2.5057, -1.3284, -1.1760,
           3.0318,  2.6059, -2.6251, -1.7658, -3.0251, -2.7278,  3.1121,
           4.5880,  0.3467, -1.7807,  3.0756, -2.8970, -1.3138, -2.7907,
          -3.0867]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1430, -4.0313, -2.3857, -2.1391,  2.5057, -1.3284, -1.1760,
           3.0318,  2.6059, -2.6251, -1.7658, -3.0251, -2.7278,  3.1121,
           4.5880,  0.3467, -1.7807,  3.0756, -2.8970, -1.3138, -2.7907,
          -3.0867]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1430, -4.0313, -2.3857, -2.1391,  2.5057, -1.3284, -1.1760,
           3.0318,  2.6059, -2.6251, -1.7658, -3.0251, -2.7278,  3.1121,
           4.5880,  0.3467, -1.7807,  3.0756, -2.8970, -1.3138, -2.7907,
          -3.0867]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1430, -4.0313, -2.3857, -2.1391,  2.5057, -1.3284, -1.1760,
           3.0318,  2.6059, -2.6251, -1.7658, -3.0251, -2.7278,  3.1121,
           4.5880,  0.3467, -1.7807,  3.0756, -2.8970, -1.3138, -2.7907,
          -3.0867]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1430, -4.0313, -2.3857, -2.1391,  2.5057, -1.3284, -1.1760,
           3.0318,  2.6059, -2.6251, -1.7658, -3.0251, -2.7278,  3.1121,
           4.5880,  0.3467, -1.7807,  3.0756, -2.8970, -1.3138, -2.7907,
          -3.0867]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1430, -4.0313, -2.3857, -2.1391,  2.5057, -1.3284, -1.1760,
           3.0318,  2.6059, -2.6251, -1.7658, -3.0251, -2.7278,  3.1121,
           4.5880,  0.3467, -1.7807,  3.0756, -2.8970, -1.3138, -2.7907,
          -3.0867]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1430, -4.0313, -2.3857, -2.1391,  2.5057, -1.3284, -1.1760,
           3.0318,  2.6059, -2.6251, -1.7658, -3.0251, -2.7278,  3.1121,
           4.5880,  0.3467, -1.7807,  3.0756, -2.8970, -1.3138, -2.7907,
          -3.0867]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1430, -4.0313, -2.3857, -2.1391,  2.5057, -1.3284, -1.1760,
           3.0318,  2.6059, -2.6251, -1.7658, -3.0251, -2.7278,  3.1121,
           4.5880,  0.3467, -1.7807,  3.0756, -2.8970, -1.3138, -2.7907,
          -3.0867]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1430, -4.0313, -2.3857, -2.1391,  2.5057, -1.3284, -1.1760,
           3.0318,  2.6059, -2.6251, -1.7658, -3.0251, -2.7278,  3.1121,
           4.5880,  0.3467, -1.7807,  3.0756, -2.8970, -1.3138, -2.7907,
          -3.0867]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1083, -3.9685, -2.3699, -2.2456,  2.5844, -1.4414, -1.2299,
           3.2240,  2.4680, -2.5656, -1.7918, -2.9508, -2.4038,  3.2169,
           4.4012,  0.4208, -1.8475,  3.1872, -3.0724, -1.2227, -2.6991,
          -3.2809]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1083, -3.9685, -2.3699, -2.2456,  2.5844, -1.4414, -1.2299,
           3.2240,  2.4680, -2.5656, -1.7918, -2.9508, -2.4038,  3.2169,
           4.4012,  0.4208, -1.8475,  3.1872, -3.0724, -1.2227, -2.6991,
          -3.2809]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1083, -3.9685, -2.3699, -2.2456,  2.5844, -1.4414, -1.2299,
           3.2240,  2.4680, -2.5656, -1.7918, -2.9508, -2.4038,  3.2169,
           4.4012,  0.4208, -1.8475,  3.1872, -3.0724, -1.2227, -2.6991,
          -3.2809]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1115, -3.9935, -2.3553, -2.2388,  2.6076, -1.4527, -1.2063,
           3.1758,  2.4973, -2.5572, -1.7769, -2.9971, -2.4678,  3.1829,
           4.4665,  0.4766, -1.7687,  3.1353, -3.1744, -1.2353, -2.7017,
          -3.2888]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 14
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1083, -3.9685, -2.3699, -2.2456,  2.5844, -1.4414, -1.2299,
           3.2240,  2.4680, -2.5656, -1.7918, -2.9508, -2.4038,  3.2169,
           4.4012,  0.4208, -1.8475,  3.1872, -3.0724, -1.2227, -2.6991,
          -3.2809]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 5, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0740, -3.9672, -2.4599, -2.2482,  2.4899, -1.5114, -1.2235,
           3.2718,  2.4460, -2.4789, -1.7784, -3.0445, -2.5052,  3.1994,
           4.4569,  0.4860, -1.7465,  3.0115, -3.1374, -1.2398, -2.7598,
          -3.3821],
         [-2.0972, -3.9879, -2.4446, -2.3354,  2.6605, -1.5729, -1.1794,
           3.2113,  2.5048, -2.4807, -1.8180, -2.9726, -2.4144,  3.1875,
           4.4768,  0.4347, -1.8464,  3.0381, -3.1657, -1.2335, -2.8009,
          -3.3566],
         [-2.0838, -4.0125, -2.4154, -2.2504,  2.5488, -1.5083, -1.1856,
           3.3289,  2.4197, -2.4444, -1.7752, -3.0073, -2.5000,  3.2532,
           4.4458,  0.4242, -1.8441,  3.0323, -3.1104, -1.2257, -2.7308,
          -3.4189],
         [-2.0584, -3.9632, -2.4169, -2.3144,  2.6434, -1.5211, -1.1577,
           3.1847,  2.4963, -2.5386, -1.7627, -2.9269, -2.4736,  3.1785,
           4.4973,  0.4489, -1.8441,  3.0229, -3.1095, -1.2306, -2.7602,
          -3.3833],
         [-2.0653, -3.9892, -2.4498, -2.2042,  2.5471, -1.5410, -1.1608,
           3.3524,  2.4250, -2.4630, -1.8288, -3.0243, -2.4747,  3.2016,
           4.4524,  0.4571, -1.8167,  3.0338, -3.1020, -1.1951, -2.7630,
          -3.3860]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([5, 5])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1083, -3.9685, -2.3699, -2.2456,  2.5844, -1.4414, -1.2299,
           3.2240,  2.4680, -2.5656, -1.7918, -2.9508, -2.4038,  3.2169,
           4.4012,  0.4208, -1.8475,  3.1872, -3.0724, -1.2227, -2.6991,
          -3.2809]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 5, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0740, -3.9672, -2.4599, -2.2482,  2.4899, -1.5114, -1.2235,
           3.2718,  2.4460, -2.4789, -1.7784, -3.0445, -2.5052,  3.1994,
           4.4569,  0.4860, -1.7465,  3.0115, -3.1374, -1.2398, -2.7598,
          -3.3821],
         [-2.0972, -3.9879, -2.4446, -2.3354,  2.6605, -1.5729, -1.1794,
           3.2113,  2.5048, -2.4807, -1.8180, -2.9726, -2.4144,  3.1875,
           4.4768,  0.4347, -1.8464,  3.0381, -3.1657, -1.2335, -2.8009,
          -3.3566],
         [-2.0838, -4.0125, -2.4154, -2.2504,  2.5488, -1.5083, -1.1856,
           3.3289,  2.4197, -2.4444, -1.7752, -3.0073, -2.5000,  3.2532,
           4.4458,  0.4242, -1.8441,  3.0323, -3.1104, -1.2257, -2.7308,
          -3.4189],
         [-2.0584, -3.9632, -2.4169, -2.3144,  2.6434, -1.5211, -1.1577,
           3.1847,  2.4963, -2.5386, -1.7627, -2.9269, -2.4736,  3.1785,
           4.4973,  0.4489, -1.8441,  3.0229, -3.1095, -1.2306, -2.7602,
          -3.3833],
         [-2.0653, -3.9892, -2.4498, -2.2042,  2.5471, -1.5410, -1.1608,
           3.3524,  2.4250, -2.4630, -1.8288, -3.0243, -2.4747,  3.2016,
           4.4524,  0.4571, -1.8167,  3.0338, -3.1020, -1.1951, -2.7630,
          -3.3860]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([5, 5])
root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([1, 30])
root - WARNING - train_PALACE:: X is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - =======================PALACE: predicting...=======================
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-5.7694, -2.0909, -1.0172, -2.3467,  2.8373, -1.3401, -1.9654,
           2.1501,  3.8771, -3.3445, -2.7299, -2.9630, -2.9626,  2.6628,
           2.8079,  2.9680, -4.2746,  1.4543, -2.9027, -0.1432, -1.1231,
          -4.8539]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 5, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-5.6921, -2.0167, -1.0260, -2.2984,  2.9943, -1.3768, -2.0412,
           1.9011,  4.0710, -3.2177, -2.7544, -2.8440, -2.8868,  2.4934,
           2.9782,  3.0644, -4.3613,  1.1676, -2.8409, -0.0829, -1.2586,
          -4.8246],
         [-5.6237, -1.9805, -0.9881, -2.2896,  2.9603, -1.3116, -2.0339,
           1.8536,  4.0890, -3.2261, -2.7604, -2.8876, -2.8927,  2.5062,
           2.9825,  3.1016, -4.3971,  1.1742, -2.8258, -0.0469, -1.2054,
          -4.8038],
         [-5.6396, -2.0339, -1.0140, -2.3455,  2.9084, -1.3448, -2.1457,
           2.0281,  3.9541, -3.2570, -2.7191, -2.8740, -2.8950,  2.5009,
           2.9087,  3.0775, -4.3884,  1.2060, -2.7758, -0.0794, -1.1808,
          -4.8664],
         [-5.6557, -1.9994, -0.9990, -2.3090,  2.9368, -1.3951, -2.0530,
           1.9413,  4.0422, -3.2368, -2.7189, -2.8478, -2.8516,  2.5490,
           2.9205,  3.1377, -4.3906,  1.1774, -2.7831, -0.1352, -1.2036,
          -4.7965],
         [-5.6841, -2.0399, -0.9851, -2.3394,  2.9129, -1.3784, -2.1224,
           2.0487,  3.9575, -3.2442, -2.7361, -2.9055, -2.8785,  2.5087,
           2.9351,  3.0649, -4.3949,  1.2001, -2.8152, -0.0921, -1.1654,
          -4.8842]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([5, 5])
root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([1, 30])
root - WARNING - train_PALACE:: X is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - =======================PALACE: predicting...=======================
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0749, -3.7161, -2.4377, -2.2555,  0.8152, -3.0125, -2.9533,
           2.5015,  3.2819, -4.5357, -3.8721, -3.1090, -3.7600,  0.7484,
           2.3571,  1.4084, -1.8796,  1.5485, -3.4968, -2.5061, -2.8853,
          -3.4732]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 5, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1066, -3.7431, -2.5006, -2.1615,  0.9082, -2.9769, -2.9465,
           2.4869,  3.3497, -4.5241, -3.8934, -3.0908, -3.7534,  0.7142,
           2.2850,  1.3848, -1.8531,  1.4711, -3.4351, -2.4937, -2.8819,
          -3.3807],
         [-2.0878, -3.7282, -2.4656, -2.2088,  0.8702, -2.9779, -2.9265,
           2.5216,  3.3497, -4.5315, -3.8912, -3.0619, -3.7413,  0.7086,
           2.3026,  1.3844, -1.8689,  1.4667, -3.4804, -2.5071, -2.8862,
          -3.4514],
         [-2.0252, -3.6915, -2.4757, -2.2153,  0.8790, -2.9462, -2.9503,
           2.5108,  3.3518, -4.5641, -3.9217, -3.1006, -3.6828,  0.7350,
           2.3053,  1.4179, -1.8357,  1.4624, -3.4628, -2.5065, -2.9249,
          -3.4243],
         [-2.0301, -3.7220, -2.4687, -2.2472,  0.9168, -3.0132, -2.9068,
           2.5059,  3.3182, -4.5033, -3.9053, -3.0602, -3.6534,  0.7170,
           2.2789,  1.4058, -1.8941,  1.4392, -3.4491, -2.5001, -2.8984,
          -3.4318],
         [-2.0504, -3.7276, -2.5108, -2.1691,  0.8861, -2.9844, -2.9463,
           2.4896,  3.3567, -4.5201, -3.8661, -3.0922, -3.7533,  0.7215,
           2.3112,  1.4346, -1.8839,  1.4736, -3.4681, -2.4956, -2.8668,
          -3.4942]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([5, 5])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0749, -3.7161, -2.4377, -2.2555,  0.8152, -3.0125, -2.9533,
           2.5015,  3.2819, -4.5357, -3.8721, -3.1090, -3.7600,  0.7484,
           2.3571,  1.4084, -1.8796,  1.5485, -3.4968, -2.5061, -2.8853,
          -3.4732]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 5, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1066, -3.7431, -2.5006, -2.1615,  0.9082, -2.9769, -2.9465,
           2.4869,  3.3497, -4.5241, -3.8934, -3.0908, -3.7534,  0.7142,
           2.2850,  1.3848, -1.8531,  1.4711, -3.4351, -2.4937, -2.8819,
          -3.3807],
         [-2.0878, -3.7282, -2.4656, -2.2088,  0.8702, -2.9779, -2.9265,
           2.5216,  3.3497, -4.5315, -3.8912, -3.0619, -3.7413,  0.7086,
           2.3026,  1.3844, -1.8689,  1.4667, -3.4804, -2.5071, -2.8862,
          -3.4514],
         [-2.0252, -3.6915, -2.4757, -2.2153,  0.8790, -2.9462, -2.9503,
           2.5108,  3.3518, -4.5641, -3.9217, -3.1006, -3.6828,  0.7350,
           2.3053,  1.4179, -1.8357,  1.4624, -3.4628, -2.5065, -2.9249,
          -3.4243],
         [-2.0301, -3.7220, -2.4687, -2.2472,  0.9168, -3.0132, -2.9068,
           2.5059,  3.3182, -4.5033, -3.9053, -3.0602, -3.6534,  0.7170,
           2.2789,  1.4058, -1.8941,  1.4392, -3.4491, -2.5001, -2.8984,
          -3.4318],
         [-2.0504, -3.7276, -2.5108, -2.1691,  0.8861, -2.9844, -2.9463,
           2.4896,  3.3567, -4.5201, -3.8661, -3.0922, -3.7533,  0.7215,
           2.3112,  1.4346, -1.8839,  1.4736, -3.4681, -2.4956, -2.8668,
          -3.4942]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([5, 5])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0749, -3.7161, -2.4377, -2.2555,  0.8152, -3.0125, -2.9533,
           2.5015,  3.2819, -4.5357, -3.8721, -3.1090, -3.7600,  0.7484,
           2.3571,  1.4084, -1.8796,  1.5485, -3.4968, -2.5061, -2.8853,
          -3.4732]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: pred is: 8
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0749, -3.7161, -2.4377, -2.2555,  0.8152, -3.0125, -2.9533,
           2.5015,  3.2819, -4.5357, -3.8721, -3.1090, -3.7600,  0.7484,
           2.3571,  1.4084, -1.8796,  1.5485, -3.4968, -2.5061, -2.8853,
          -3.4732]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 5])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 5, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 5, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 5, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 5, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.1066, -3.7431, -2.5006, -2.1615,  0.9082, -2.9769, -2.9465,
           2.4869,  3.3497, -4.5241, -3.8934, -3.0908, -3.7534,  0.7142,
           2.2850,  1.3848, -1.8531,  1.4711, -3.4351, -2.4937, -2.8819,
          -3.3807],
         [-2.0878, -3.7282, -2.4656, -2.2088,  0.8702, -2.9779, -2.9265,
           2.5216,  3.3497, -4.5315, -3.8912, -3.0619, -3.7413,  0.7086,
           2.3026,  1.3844, -1.8689,  1.4667, -3.4804, -2.5071, -2.8862,
          -3.4514],
         [-2.0252, -3.6915, -2.4757, -2.2153,  0.8790, -2.9462, -2.9503,
           2.5108,  3.3518, -4.5641, -3.9217, -3.1006, -3.6828,  0.7350,
           2.3053,  1.4179, -1.8357,  1.4624, -3.4628, -2.5065, -2.9249,
          -3.4243],
         [-2.0301, -3.7220, -2.4687, -2.2472,  0.9168, -3.0132, -2.9068,
           2.5059,  3.3182, -4.5033, -3.9053, -3.0602, -3.6534,  0.7170,
           2.2789,  1.4058, -1.8941,  1.4392, -3.4491, -2.5001, -2.8984,
          -3.4318],
         [-2.0504, -3.7276, -2.5108, -2.1691,  0.8861, -2.9844, -2.9463,
           2.4896,  3.3567, -4.5201, -3.8661, -3.0922, -3.7533,  0.7215,
           2.3112,  1.4346, -1.8839,  1.4736, -3.4681, -2.4956, -2.8668,
          -3.4942]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([5, 5])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0749, -3.7161, -2.4377, -2.2555,  0.8152, -3.0125, -2.9533,
           2.5015,  3.2819, -4.5357, -3.8721, -3.1090, -3.7600,  0.7484,
           2.3571,  1.4084, -1.8796,  1.5485, -3.4968, -2.5061, -2.8853,
          -3.4732]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[-2.0868, -3.7001, -2.4548, -2.2550,  0.8505, -3.0263, -2.9911,
           2.4889,  3.3339, -4.5242, -3.8904, -3.1119, -3.7645,  0.6981,
           2.3495,  1.3815, -1.8725,  1.5063, -3.5018, -2.4501, -2.9035,
          -3.3962]]], device='cuda:0', grad_fn=<AddBackward0>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([1, 30])
root - WARNING - train_PALACE:: X is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - =======================PALACE: predicting...=======================
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.4373e-06, 1.6589e-06, 5.8890e-06, 5.3133e-06, 9.6559e-06,
          1.2038e-06, 1.5229e-06, 2.3738e-06, 9.9892e-01, 2.1688e-06,
          1.9442e-06, 4.6417e-06, 2.1437e-06, 8.4905e-04, 1.4392e-04,
          1.7263e-05, 9.2237e-07, 3.1738e-06, 7.7840e-07, 8.5053e-07,
          4.0723e-06, 1.0594e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.4373e-06, 1.6589e-06, 5.8890e-06, 5.3133e-06, 9.6559e-06,
          1.2038e-06, 1.5229e-06, 2.3738e-06, 9.9892e-01, 2.1688e-06,
          1.9442e-06, 4.6417e-06, 2.1437e-06, 8.4905e-04, 1.4392e-04,
          1.7263e-05, 9.2237e-07, 3.1738e-06, 7.7840e-07, 8.5053e-07,
          4.0723e-06, 1.0594e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.7649e-06, 1.5957e-06, 5.9269e-06, 5.1636e-06, 1.0189e-05,
          1.2642e-06, 1.4648e-06, 2.5065e-06, 9.9896e-01, 2.1444e-06,
          1.9978e-06, 4.4094e-06, 1.9612e-06, 8.0929e-04, 1.4507e-04,
          1.7827e-05, 9.2017e-07, 2.9875e-06, 8.1640e-07, 8.8044e-07,
          4.2667e-06, 1.0596e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.7649e-06, 1.5957e-06, 5.9269e-06, 5.1636e-06, 1.0189e-05,
          1.2642e-06, 1.4648e-06, 2.5065e-06, 9.9896e-01, 2.1444e-06,
          1.9978e-06, 4.4094e-06, 1.9612e-06, 8.0929e-04, 1.4507e-04,
          1.7827e-05, 9.2017e-07, 2.9875e-06, 8.1640e-07, 8.8044e-07,
          4.2667e-06, 1.0596e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.7648e-06, 1.5957e-06, 5.9269e-06, 5.1636e-06, 1.0189e-05,
          1.2642e-06, 1.4648e-06, 2.5065e-06, 9.9896e-01, 2.1444e-06,
          1.9978e-06, 4.4094e-06, 1.9611e-06, 8.0929e-04, 1.4507e-04,
          1.7827e-05, 9.2016e-07, 2.9875e-06, 8.1640e-07, 8.8044e-07,
          4.2667e-06, 1.0596e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.7649e-06, 1.5957e-06, 5.9269e-06, 5.1636e-06, 1.0189e-05,
          1.2642e-06, 1.4648e-06, 2.5065e-06, 9.9896e-01, 2.1444e-06,
          1.9978e-06, 4.4094e-06, 1.9612e-06, 8.0929e-04, 1.4507e-04,
          1.7827e-05, 9.2017e-07, 2.9875e-06, 8.1640e-07, 8.8044e-07,
          4.2667e-06, 1.0596e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.7648e-06, 1.5957e-06, 5.9269e-06, 5.1636e-06, 1.0189e-05,
          1.2642e-06, 1.4648e-06, 2.5065e-06, 9.9896e-01, 2.1444e-06,
          1.9978e-06, 4.4094e-06, 1.9612e-06, 8.0929e-04, 1.4507e-04,
          1.7827e-05, 9.2016e-07, 2.9875e-06, 8.1640e-07, 8.8044e-07,
          4.2667e-06, 1.0596e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.7649e-06, 1.5957e-06, 5.9269e-06, 5.1636e-06, 1.0189e-05,
          1.2642e-06, 1.4648e-06, 2.5065e-06, 9.9896e-01, 2.1444e-06,
          1.9978e-06, 4.4094e-06, 1.9612e-06, 8.0929e-04, 1.4507e-04,
          1.7827e-05, 9.2016e-07, 2.9875e-06, 8.1640e-07, 8.8044e-07,
          4.2667e-06, 1.0596e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.7649e-06, 1.5957e-06, 5.9269e-06, 5.1636e-06, 1.0189e-05,
          1.2642e-06, 1.4648e-06, 2.5065e-06, 9.9896e-01, 2.1444e-06,
          1.9978e-06, 4.4094e-06, 1.9612e-06, 8.0929e-04, 1.4507e-04,
          1.7827e-05, 9.2017e-07, 2.9875e-06, 8.1640e-07, 8.8044e-07,
          4.2667e-06, 1.0596e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.7648e-06, 1.5957e-06, 5.9269e-06, 5.1636e-06, 1.0189e-05,
          1.2642e-06, 1.4648e-06, 2.5065e-06, 9.9896e-01, 2.1444e-06,
          1.9978e-06, 4.4094e-06, 1.9611e-06, 8.0929e-04, 1.4507e-04,
          1.7827e-05, 9.2016e-07, 2.9875e-06, 8.1640e-07, 8.8044e-07,
          4.2667e-06, 1.0596e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[6.7649e-06, 1.5957e-06, 5.9269e-06, 5.1636e-06, 1.0189e-05,
          1.2642e-06, 1.4648e-06, 2.5065e-06, 9.9896e-01, 2.1444e-06,
          1.9978e-06, 4.4094e-06, 1.9612e-06, 8.0929e-04, 1.4507e-04,
          1.7827e-05, 9.2016e-07, 2.9875e-06, 8.1640e-07, 8.8044e-07,
          4.2667e-06, 1.0596e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([1, 30])
root - WARNING - train_PALACE:: X is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - =======================PALACE: predicting...=======================
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[2.3020e-06, 4.2944e-06, 2.5221e-06, 1.0164e-06, 3.4494e-05,
          2.5828e-06, 2.1715e-06, 4.6649e-05, 9.9983e-01, 1.1840e-05,
          7.6680e-07, 3.7571e-07, 4.3432e-06, 8.4232e-06, 2.3820e-06,
          1.3160e-05, 1.9722e-06, 1.1583e-05, 5.8031e-06, 1.2337e-05,
          3.2847e-07, 1.6654e-06]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([1, 30])
root - WARNING - train_PALACE:: X is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - =======================PALACE: predicting...=======================
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.9179e-05, 3.6875e-05, 2.9219e-05, 1.9511e-04, 3.2269e-03,
          4.9384e-05, 1.9680e-05, 6.7639e-01, 3.1889e-01, 2.3039e-06,
          4.1904e-05, 2.3056e-04, 2.0856e-04, 2.1373e-06, 8.1992e-05,
          4.0722e-05, 4.1415e-05, 1.5227e-04, 7.3538e-05, 8.9205e-05,
          4.9623e-05, 7.5808e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.4934e-05, 3.8343e-05, 2.8753e-05, 1.9552e-04, 3.2113e-03,
          5.1697e-05, 2.0206e-05, 6.8533e-01, 3.0998e-01, 2.6026e-06,
          4.2952e-05, 2.3971e-04, 1.9994e-04, 2.0982e-06, 8.0172e-05,
          4.2122e-05, 3.9543e-05, 1.3301e-04, 7.4004e-05, 8.9661e-05,
          4.8723e-05, 7.1120e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.4934e-05, 3.8343e-05, 2.8753e-05, 1.9552e-04, 3.2113e-03,
          5.1697e-05, 2.0206e-05, 6.8533e-01, 3.0998e-01, 2.6026e-06,
          4.2952e-05, 2.3971e-04, 1.9994e-04, 2.0982e-06, 8.0172e-05,
          4.2122e-05, 3.9543e-05, 1.3301e-04, 7.4004e-05, 8.9661e-05,
          4.8723e-05, 7.1120e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.4934e-05, 3.8342e-05, 2.8753e-05, 1.9552e-04, 3.2113e-03,
          5.1697e-05, 2.0206e-05, 6.8533e-01, 3.0998e-01, 2.6026e-06,
          4.2952e-05, 2.3971e-04, 1.9994e-04, 2.0982e-06, 8.0172e-05,
          4.2122e-05, 3.9543e-05, 1.3301e-04, 7.4004e-05, 8.9661e-05,
          4.8723e-05, 7.1120e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.4934e-05, 3.8342e-05, 2.8753e-05, 1.9552e-04, 3.2113e-03,
          5.1697e-05, 2.0206e-05, 6.8533e-01, 3.0998e-01, 2.6026e-06,
          4.2952e-05, 2.3971e-04, 1.9994e-04, 2.0982e-06, 8.0172e-05,
          4.2122e-05, 3.9543e-05, 1.3301e-04, 7.4004e-05, 8.9661e-05,
          4.8723e-05, 7.1120e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.4934e-05, 3.8343e-05, 2.8753e-05, 1.9552e-04, 3.2113e-03,
          5.1697e-05, 2.0206e-05, 6.8533e-01, 3.0998e-01, 2.6026e-06,
          4.2952e-05, 2.3971e-04, 1.9994e-04, 2.0982e-06, 8.0172e-05,
          4.2122e-05, 3.9543e-05, 1.3301e-04, 7.4004e-05, 8.9661e-05,
          4.8723e-05, 7.1120e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.4934e-05, 3.8342e-05, 2.8753e-05, 1.9552e-04, 3.2113e-03,
          5.1697e-05, 2.0206e-05, 6.8533e-01, 3.0998e-01, 2.6026e-06,
          4.2952e-05, 2.3971e-04, 1.9994e-04, 2.0982e-06, 8.0172e-05,
          4.2122e-05, 3.9543e-05, 1.3301e-04, 7.4004e-05, 8.9661e-05,
          4.8723e-05, 7.1120e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.4934e-05, 3.8342e-05, 2.8753e-05, 1.9552e-04, 3.2113e-03,
          5.1697e-05, 2.0206e-05, 6.8533e-01, 3.0998e-01, 2.6026e-06,
          4.2952e-05, 2.3971e-04, 1.9994e-04, 2.0982e-06, 8.0171e-05,
          4.2122e-05, 3.9543e-05, 1.3301e-04, 7.4004e-05, 8.9661e-05,
          4.8723e-05, 7.1120e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.4934e-05, 3.8342e-05, 2.8753e-05, 1.9552e-04, 3.2113e-03,
          5.1697e-05, 2.0206e-05, 6.8533e-01, 3.0998e-01, 2.6026e-06,
          4.2952e-05, 2.3971e-04, 1.9994e-04, 2.0982e-06, 8.0172e-05,
          4.2122e-05, 3.9543e-05, 1.3301e-04, 7.4004e-05, 8.9661e-05,
          4.8723e-05, 7.1120e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[7.4934e-05, 3.8343e-05, 2.8753e-05, 1.9552e-04, 3.2113e-03,
          5.1697e-05, 2.0206e-05, 6.8533e-01, 3.0998e-01, 2.6026e-06,
          4.2952e-05, 2.3971e-04, 1.9994e-04, 2.0982e-06, 8.0172e-05,
          4.2122e-05, 3.9543e-05, 1.3301e-04, 7.4004e-05, 8.9661e-05,
          4.8723e-05, 7.1120e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([1, 30])
root - WARNING - train_PALACE:: X is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - =======================PALACE: predicting...=======================
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.8487e-04, 3.0061e-05, 8.8972e-05, 5.8130e-05, 2.1156e-03,
          6.5622e-05, 1.1839e-04, 7.7799e-01, 2.1400e-01, 7.0047e-04,
          5.7099e-05, 1.3407e-04, 2.0081e-04, 4.6418e-04, 4.6589e-05,
          2.5287e-03, 5.8086e-05, 3.0396e-04, 4.0480e-04, 8.5626e-05,
          1.4415e-04, 2.3078e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7830e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7830e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7830e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.8487e-04, 3.0061e-05, 8.8972e-05, 5.8130e-05, 2.1156e-03,
          6.5622e-05, 1.1839e-04, 7.7799e-01, 2.1400e-01, 7.0047e-04,
          5.7099e-05, 1.3407e-04, 2.0081e-04, 4.6418e-04, 4.6589e-05,
          2.5287e-03, 5.8086e-05, 3.0396e-04, 4.0480e-04, 8.5626e-05,
          1.4415e-04, 2.3078e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7830e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7830e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7830e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[3.7888e-04, 3.0020e-05, 9.1615e-05, 5.7192e-05, 2.1411e-03,
          6.4885e-05, 1.1551e-04, 7.7829e-01, 2.1374e-01, 7.0918e-04,
          5.8979e-05, 1.3701e-04, 1.9551e-04, 4.5156e-04, 4.6466e-05,
          2.4533e-03, 5.8117e-05, 3.0890e-04, 4.0576e-04, 8.5222e-05,
          1.4872e-04, 2.3552e-05]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - =======================PALACE: loading data...=======================
root - WARNING - read_data: prot_features after prot_to_features is: torch.Size([9, 30])
root - WARNING - =======================PALACE: building encoder...=======================
root - WARNING - PALACE_Encoder: vocab_size is: 20
root - WARNING - PALACE_Encoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: building decoder...=======================
root - WARNING - PALACE_Decoder: vocab_size is: 22
root - WARNING - PALACE_Decoder: feat_space_dim is: 128
root - WARNING - ProteinEncoding: prot_MLP after __init__ is: [128, 256, 128]
root - WARNING - =======================PALACE: training...=======================
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([2, 30])
root - WARNING - train_PALACE:: X is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([2, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([2, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([2, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([2, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([2, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([2, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([2, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([2, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([2, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([2, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([2, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([2, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([2, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([16, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([16, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([2, 10, 128])
root - WARNING - train_PALACE:: X_prot is: torch.Size([1, 30])
root - WARNING - train_PALACE:: X is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE: enc_outputs is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 10])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: True
root - WARNING - DecoderBlock: dec_valid_lens is: torch.Size([1, 10])
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 10, 128])
root - WARNING - =======================PALACE: predicting...=======================
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[1.0306e-07, 2.6645e-08, 3.8406e-07, 2.5488e-08, 4.3896e-07,
          1.7892e-07, 6.9360e-08, 1.0742e-06, 9.9999e-01, 2.2596e-07,
          2.4739e-07, 9.6670e-08, 2.7995e-07, 3.0398e-07, 6.6525e-08,
          5.9018e-07, 1.2484e-07, 8.7843e-08, 3.9640e-08, 4.3680e-07,
          1.5178e-07, 4.5979e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.7613e-08, 3.5316e-08, 4.4568e-07, 2.4863e-08, 4.8318e-07,
          1.9635e-07, 6.6293e-08, 1.1876e-06, 9.9999e-01, 2.2630e-07,
          2.7877e-07, 1.0015e-07, 3.1566e-07, 2.9599e-07, 7.5396e-08,
          6.4243e-07, 1.3591e-07, 9.1437e-08, 4.5386e-08, 4.7395e-07,
          1.7744e-07, 5.8058e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.7613e-08, 3.5316e-08, 4.4568e-07, 2.4863e-08, 4.8318e-07,
          1.9635e-07, 6.6293e-08, 1.1876e-06, 9.9999e-01, 2.2630e-07,
          2.7877e-07, 1.0015e-07, 3.1566e-07, 2.9599e-07, 7.5396e-08,
          6.4243e-07, 1.3591e-07, 9.1437e-08, 4.5386e-08, 4.7395e-07,
          1.7744e-07, 5.8058e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.7613e-08, 3.5316e-08, 4.4568e-07, 2.4863e-08, 4.8318e-07,
          1.9635e-07, 6.6293e-08, 1.1876e-06, 9.9999e-01, 2.2630e-07,
          2.7877e-07, 1.0015e-07, 3.1566e-07, 2.9599e-07, 7.5396e-08,
          6.4243e-07, 1.3591e-07, 9.1437e-08, 4.5386e-08, 4.7394e-07,
          1.7744e-07, 5.8058e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.7613e-08, 3.5316e-08, 4.4568e-07, 2.4863e-08, 4.8318e-07,
          1.9635e-07, 6.6293e-08, 1.1876e-06, 9.9999e-01, 2.2630e-07,
          2.7877e-07, 1.0015e-07, 3.1566e-07, 2.9599e-07, 7.5396e-08,
          6.4243e-07, 1.3591e-07, 9.1437e-08, 4.5386e-08, 4.7395e-07,
          1.7744e-07, 5.8058e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.7613e-08, 3.5316e-08, 4.4568e-07, 2.4863e-08, 4.8318e-07,
          1.9635e-07, 6.6293e-08, 1.1876e-06, 9.9999e-01, 2.2630e-07,
          2.7877e-07, 1.0015e-07, 3.1566e-07, 2.9599e-07, 7.5396e-08,
          6.4243e-07, 1.3591e-07, 9.1437e-08, 4.5386e-08, 4.7395e-07,
          1.7744e-07, 5.8058e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.7613e-08, 3.5316e-08, 4.4568e-07, 2.4863e-08, 4.8318e-07,
          1.9635e-07, 6.6293e-08, 1.1876e-06, 9.9999e-01, 2.2630e-07,
          2.7877e-07, 1.0015e-07, 3.1566e-07, 2.9599e-07, 7.5395e-08,
          6.4243e-07, 1.3591e-07, 9.1437e-08, 4.5386e-08, 4.7394e-07,
          1.7744e-07, 5.8058e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.7613e-08, 3.5316e-08, 4.4568e-07, 2.4863e-08, 4.8318e-07,
          1.9635e-07, 6.6293e-08, 1.1876e-06, 9.9999e-01, 2.2630e-07,
          2.7877e-07, 1.0015e-07, 3.1566e-07, 2.9599e-07, 7.5396e-08,
          6.4243e-07, 1.3591e-07, 9.1437e-08, 4.5386e-08, 4.7395e-07,
          1.7744e-07, 5.8058e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.7613e-08, 3.5316e-08, 4.4568e-07, 2.4863e-08, 4.8318e-07,
          1.9635e-07, 6.6293e-08, 1.1876e-06, 9.9999e-01, 2.2630e-07,
          2.7877e-07, 1.0015e-07, 3.1566e-07, 2.9599e-07, 7.5396e-08,
          6.4243e-07, 1.3591e-07, 9.1437e-08, 4.5386e-08, 4.7394e-07,
          1.7744e-07, 5.8058e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.7613e-08, 3.5316e-08, 4.4568e-07, 2.4863e-08, 4.8318e-07,
          1.9635e-07, 6.6293e-08, 1.1876e-06, 9.9999e-01, 2.2630e-07,
          2.7877e-07, 1.0015e-07, 3.1566e-07, 2.9599e-07, 7.5396e-08,
          6.4243e-07, 1.3591e-07, 9.1437e-08, 4.5386e-08, 4.7395e-07,
          1.7744e-07, 5.8058e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[1.0523e-07, 2.5586e-08, 4.0634e-07, 2.1107e-08, 4.2630e-07,
          1.7682e-07, 5.3176e-08, 9.0953e-07, 1.0000e+00, 1.9556e-07,
          3.1621e-07, 9.2786e-08, 2.5382e-07, 2.6741e-07, 5.4495e-08,
          5.0598e-07, 1.3307e-07, 7.8341e-08, 3.3307e-08, 3.7243e-07,
          1.1208e-07, 4.3247e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4358e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4358e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[1.0523e-07, 2.5586e-08, 4.0634e-07, 2.1107e-08, 4.2630e-07,
          1.7682e-07, 5.3176e-08, 9.0953e-07, 1.0000e+00, 1.9556e-07,
          3.1621e-07, 9.2786e-08, 2.5382e-07, 2.6741e-07, 5.4495e-08,
          5.0598e-07, 1.3307e-07, 7.8341e-08, 3.3307e-08, 3.7243e-07,
          1.1208e-07, 4.3247e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4358e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4358e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: Y after decoder is: tensor([[[9.9745e-08, 3.3449e-08, 4.6830e-07, 2.0390e-08, 4.6247e-07,
          1.9634e-07, 5.0337e-08, 9.9571e-07, 1.0000e+00, 1.9479e-07,
          3.5161e-07, 9.7401e-08, 2.8342e-07, 2.5974e-07, 6.1383e-08,
          5.4357e-07, 1.4504e-07, 8.1589e-08, 3.7428e-08, 4.0174e-07,
          1.2705e-07, 5.2727e-08]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
root - WARNING - predict_PALACE: dec_X is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([2])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([2, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 52, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 52, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 52, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 52, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 53, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 53, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 53, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 53, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 54, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 54, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 54, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 54, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 54, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 54, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 54, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 54, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 54, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 54, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 54, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 54, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 55, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 55, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 55, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 55, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 55, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 55, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 55, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 55, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 55, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 55, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 55, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 55, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 56, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 56, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 56, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 56, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 56, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 56, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 56, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 56, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 56, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 56, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 56, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 56, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 57, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 57, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 57, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 57, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 57, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 57, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 57, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 57, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 57, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 57, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 57, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 57, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 58, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 58, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 58, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 58, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 58, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 58, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 58, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 58, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 58, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 58, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 58, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 58, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 59, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 59, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 59, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 59, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 59, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 59, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 59, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 59, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 59, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 59, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 59, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 59, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 60, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 60, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 60, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 60, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 60, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 60, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 60, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 60, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 60, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 60, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 60, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 60, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 61, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 61, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 61, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 61, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 61, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 61, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 61, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 61, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 61, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 61, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 61, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 61, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 62, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 62, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 62, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 62, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 62, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 62, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 62, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 62, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 62, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 62, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 62, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 62, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 63, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 63, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 63, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 63, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 63, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 63, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 63, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 63, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 63, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 63, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 63, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 63, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 64, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 64, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 64, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 64, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 64, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 64, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 64, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 64, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 64, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 64, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 64, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 64, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 65, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 65, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 65, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 65, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 65, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 65, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 65, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 65, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 65, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 65, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 65, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 65, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 66, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 66, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 66, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 66, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 66, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 66, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 66, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 66, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 66, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 66, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 66, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 66, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 67, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 67, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 67, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 67, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 67, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 67, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 67, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 67, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 67, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 67, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 67, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 67, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 68, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 68, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 68, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 68, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 68, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 68, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 68, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 68, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 68, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 68, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 68, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 68, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 69, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 69, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 69, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 69, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 69, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 69, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 69, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 69, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 69, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 69, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 69, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 69, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 70, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 70, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 70, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 70, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 70, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 70, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 70, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 70, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 70, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 70, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 70, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 70, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 71, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 71, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 71, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 71, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 71, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 71, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 71, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 71, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 71, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 71, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 71, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 71, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 72, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 72, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 72, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 72, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 72, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 72, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 72, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 72, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 72, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 72, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 72, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 72, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 73, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 73, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 73, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 73, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 73, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 73, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 73, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 73, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 73, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 73, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 73, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 73, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 74, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 74, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 74, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 74, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 74, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 74, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 74, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 74, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 74, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 74, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 74, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 74, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 75, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 75, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 75, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 75, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 75, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 75, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 75, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 75, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 75, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 75, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 75, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 75, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 76, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 76, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 76, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 76, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 76, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 76, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 76, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 76, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 76, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 76, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 76, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 76, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 77, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 77, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 77, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 77, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 77, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 77, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 77, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 77, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 77, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 77, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 77, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 77, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 78, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 78, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 78, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 78, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 78, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 78, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 78, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 78, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 78, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 78, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 78, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 78, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 79, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 79, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 79, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 79, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 79, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 79, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 79, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 79, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 79, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 79, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 79, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 79, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 80, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 80, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 80, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 80, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 80, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 80, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 80, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 80, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 80, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 80, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 80, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 80, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 81, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 81, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 81, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 81, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 81, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 81, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 81, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 81, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 81, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 81, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 81, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 81, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 82, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 82, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 82, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 82, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 82, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 82, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 82, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 82, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 82, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 82, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 82, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 82, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 83, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 83, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 83, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 83, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 83, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 83, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 83, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 83, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 83, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 83, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 83, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 83, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 84, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 84, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 84, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 84, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 84, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 84, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 84, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 84, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 84, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 84, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 84, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 84, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 85, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 85, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 85, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 85, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 85, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 85, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 85, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 85, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 85, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 85, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 85, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 85, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 86, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 86, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 86, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 86, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 86, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 86, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 86, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 86, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 86, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 86, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 86, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 86, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 87, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 87, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 87, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 87, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 87, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 87, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 87, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 87, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 87, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 87, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 87, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 87, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 88, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 88, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 88, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 88, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 88, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 88, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 88, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 88, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 88, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 88, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 88, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 88, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 89, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 89, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 89, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 89, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 89, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 89, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 89, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 89, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 89, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 89, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 89, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 89, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 90, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 90, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 90, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 90, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 90, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 90, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 90, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 90, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 90, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 90, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 90, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 90, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 91, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 91, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 91, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 91, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 91, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 91, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 91, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 91, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 91, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 91, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 91, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 91, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 92, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 92, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 92, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 92, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 92, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 92, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 92, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 92, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 92, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 92, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 92, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 92, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 93, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 93, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 93, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 93, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 93, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 93, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 93, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 93, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 93, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 93, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 93, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 93, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 94, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 94, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 94, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 94, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 94, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 94, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 94, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 94, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 94, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 94, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 94, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 94, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 95, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 95, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 95, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 95, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 95, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 95, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 95, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 95, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 95, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 95, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 95, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 95, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 96, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 96, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 96, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 96, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 96, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 96, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 96, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 96, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 96, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 96, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 96, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 96, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 97, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 97, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 97, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 97, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 97, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 97, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 97, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 97, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 97, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 97, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 97, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 97, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 98, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 98, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 98, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 98, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 98, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 98, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 98, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 98, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 98, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 98, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 98, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 98, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 99, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 99, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 99, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 99, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 99, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 99, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 99, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 99, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 99, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 99, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 99, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 99, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 100, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 100, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 100, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 100, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 100, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 100, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 100, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 100, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 100, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 100, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 100, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 100, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 101, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 101, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 101, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 101, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 101, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 101, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 101, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 101, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 101, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 101, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 101, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 101, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 102, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 102, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 102, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 102, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 102, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 102, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 102, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 102, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 102, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 102, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 102, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 102, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 103, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 103, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 103, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 103, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 103, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 103, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 103, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 103, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 103, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 103, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 103, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 103, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 104, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 104, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 104, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 104, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 104, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 104, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 104, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 104, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 104, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 104, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 104, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 104, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 105, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 105, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 105, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 105, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 105, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 105, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 105, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 105, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 105, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 105, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 105, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 105, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 106, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 106, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 106, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 106, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 106, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 106, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 106, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 106, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 106, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 106, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 106, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 106, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 107, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 107, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 107, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 107, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 107, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 107, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 107, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 107, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 107, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 107, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 107, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 107, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 108, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 108, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 108, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 108, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 108, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 108, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 108, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 108, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 108, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 108, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 108, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 108, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 109, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 109, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 109, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 109, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 109, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 109, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 109, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 109, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 109, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 109, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 109, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 109, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 110, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 110, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 110, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 110, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 110, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 110, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 110, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 110, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 110, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 110, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 110, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 110, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 111, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 111, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 111, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 111, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 111, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 111, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 111, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 111, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 111, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 111, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 111, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 111, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 112, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 112, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 112, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 112, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 112, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 112, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 112, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 112, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 112, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 112, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 112, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 112, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 113, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 113, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 113, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 113, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 113, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 113, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 113, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 113, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 113, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 113, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 113, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 113, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 114, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 114, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 114, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 114, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 114, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 114, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 114, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 114, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 114, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 114, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 114, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 114, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 115, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 115, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 115, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 115, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 115, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 115, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 115, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 115, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 115, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 115, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 115, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 115, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 116, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 116, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 116, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 116, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 116, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 116, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 116, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 116, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 116, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 116, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 116, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 116, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 117, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 117, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 117, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 117, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 117, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 117, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 117, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 117, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 117, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 117, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 117, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 117, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 118, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 118, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 118, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 118, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 118, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 118, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 118, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 118, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 118, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 118, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 118, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 118, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 119, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 119, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 119, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 119, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 119, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 119, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 119, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 119, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 119, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 119, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 119, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 119, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 120, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 120, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 120, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 120, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 120, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 120, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 120, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 120, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 120, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 120, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 120, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 120, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 121, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 121, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 121, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 121, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 121, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 121, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 121, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 121, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 121, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 121, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 121, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 121, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 122, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 122, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 122, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 122, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 122, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 122, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 122, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 122, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 122, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 122, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 122, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 122, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 123, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 123, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 123, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 123, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 123, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 123, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 123, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 123, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 123, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 123, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 123, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 123, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 124, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 124, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 124, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 124, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 124, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 124, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 124, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 124, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 124, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 124, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 124, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 124, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 125, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 125, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 125, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 125, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 125, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 125, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 125, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 125, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 125, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 125, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 125, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 125, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 126, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 126, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 126, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 126, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 126, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 126, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 126, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 126, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 126, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 126, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 126, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 126, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 127, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 127, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 127, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 127, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 127, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 127, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 127, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 127, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 127, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 127, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 127, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 127, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 128, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 128, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 128, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 128, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 128, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 128, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 128, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 128, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 128, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 128, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 128, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 128, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 129, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 129, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 129, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 129, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 129, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 129, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 129, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 129, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 129, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 129, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 129, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 129, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 130, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 130, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 130, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 130, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 130, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 130, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 130, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 130, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 130, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 130, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 130, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 130, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 131, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 131, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 131, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 131, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 131, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 131, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 131, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 131, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 131, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 131, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 131, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 131, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 132, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 132, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 132, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 132, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 132, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 132, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 132, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 132, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 132, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 132, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 132, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 132, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 133, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 133, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 133, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 133, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 133, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 133, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 133, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 133, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 133, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 133, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 133, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 133, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 134, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 134, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 134, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 134, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 134, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 134, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 134, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 134, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 134, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 134, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 134, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 134, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 135, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 135, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 135, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 135, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 135, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 135, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 135, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 135, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 135, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 135, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 135, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 135, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 136, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 136, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 136, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 136, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 136, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 136, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 136, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 136, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 136, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 136, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 136, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 136, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 137, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 137, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 137, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 137, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 137, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 137, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 137, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 137, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 137, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 137, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 137, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 137, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 138, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 138, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 138, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 138, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 138, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 138, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 138, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 138, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 138, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 138, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 138, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 138, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 139, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 139, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 139, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 139, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 139, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 139, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 139, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 139, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 139, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 139, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 139, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 139, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 140, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 140, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 140, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 140, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 140, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 140, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 140, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 140, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 140, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 140, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 140, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 140, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 141, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 141, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 141, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 141, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 141, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 141, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 141, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 141, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 141, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 141, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 141, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 141, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 142, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 142, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 142, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 142, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 142, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 142, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 142, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 142, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 142, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 142, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 142, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 142, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 143, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 143, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 143, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 143, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 143, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 143, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 143, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 143, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 143, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 143, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 143, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 143, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 144, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 144, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 144, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 144, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 144, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 144, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 144, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 144, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 144, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 144, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 144, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 144, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 145, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 145, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 145, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 145, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 145, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 145, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 145, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 145, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 145, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 145, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 145, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 145, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 146, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 146, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 146, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 146, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 146, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 146, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 146, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 146, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 146, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 146, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 146, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 146, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 147, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 147, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 147, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 147, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 147, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 147, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 147, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 147, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 147, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 147, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 147, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 147, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 148, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 148, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 148, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 148, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 148, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 148, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 148, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 148, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 148, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 148, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 148, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 148, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 149, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 149, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 149, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 149, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 149, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 149, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 149, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 149, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 149, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 149, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 149, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 149, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 150, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 150, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 150, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 150, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 150, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 150, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 150, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 150, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 150, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 150, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 150, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 150, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 151, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 151, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 151, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 151, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 151, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 151, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 151, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 151, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 151, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 151, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 151, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 151, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 152, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 152, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 152, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 152, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 152, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 152, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 152, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 152, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 152, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 152, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 152, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 152, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 153, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 153, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 153, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 153, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 153, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 153, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 153, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 153, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 153, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 153, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 153, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 153, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 154, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 154, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 154, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 154, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 154, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 154, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 154, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 154, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 154, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 154, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 154, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 154, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 155, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 155, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 155, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 155, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 155, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 155, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 155, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 155, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 155, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 155, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 155, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 155, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 156, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 156, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 156, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 156, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 156, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 156, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 156, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 156, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 156, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 156, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 156, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 156, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 52, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 52, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 52, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 52, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 53, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 53, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 53, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 53, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 52, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 52, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 52, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 52, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 52, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 52, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 53, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 53, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 53, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 53, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 53, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 53, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 54, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 54, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 54, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 54, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 54, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 54, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 54, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 54, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 54, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 54, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 54, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 54, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 55, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 55, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 55, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 55, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 55, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 55, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 55, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 55, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 55, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 55, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 55, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 55, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 56, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 56, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 56, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 56, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 56, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 56, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 56, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 56, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 56, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 56, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 56, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 56, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 57, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 57, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 57, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 57, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 57, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 57, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 57, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 57, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 57, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 57, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 57, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 57, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 58, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 58, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 58, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 58, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 58, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 58, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 58, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 58, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 58, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 58, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 58, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 58, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 59, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 59, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 59, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 59, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 59, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 59, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 59, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 59, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 59, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 59, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 59, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 59, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 60, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 60, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 60, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 60, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 60, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 60, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 60, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 60, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 60, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 60, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 60, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 60, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 61, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 61, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 61, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 61, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 61, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 61, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 61, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 61, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 61, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 61, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 61, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 61, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 62, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 62, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 62, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 62, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 62, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 62, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 62, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 62, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 62, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 62, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 62, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 62, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 63, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 63, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 63, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 63, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 63, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 63, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 63, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 63, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 63, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 63, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 63, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 63, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 64, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 64, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 64, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 64, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 64, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 64, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 64, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 64, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 64, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 64, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 64, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 64, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 65, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 65, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 65, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 65, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 65, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 65, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 65, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 65, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 65, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 65, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 65, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 65, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 66, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 66, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 66, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 66, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 66, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 66, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 66, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 66, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 66, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 66, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 66, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 66, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 67, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 67, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 67, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 67, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 67, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 67, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 67, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 67, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 67, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 67, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 67, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 67, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 68, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 68, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 68, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 68, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 68, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 68, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 68, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 68, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 68, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 68, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 68, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 68, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 69, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 69, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 69, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 69, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 69, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 69, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 69, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 69, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 69, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 69, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 69, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 69, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 70, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 70, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 70, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 70, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 70, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 70, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 70, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 70, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 70, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 70, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 70, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 70, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 71, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 71, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 71, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 71, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 71, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 71, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 71, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 71, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 71, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 71, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 71, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 71, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 72, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 72, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 72, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 72, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 72, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 72, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 72, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 72, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 72, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 72, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 72, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 72, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 73, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 73, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 73, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 73, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 73, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 73, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 73, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 73, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 73, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 73, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 73, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 73, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 74, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 74, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 74, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 74, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 74, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 74, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 74, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 74, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 74, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 74, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 74, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 74, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 75, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 75, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 75, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 75, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 75, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 75, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 75, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 75, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 75, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 75, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 75, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 75, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 76, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 76, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 76, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 76, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 76, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 76, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 76, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 76, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 76, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 76, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 76, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 76, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 77, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 77, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 77, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 77, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 77, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 77, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 77, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 77, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 77, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 77, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 77, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 77, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 78, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 78, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 78, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 78, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 78, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 78, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 78, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 78, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 78, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 78, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 78, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 78, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 79, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 79, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 79, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 79, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 79, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 79, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 79, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 79, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 79, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 79, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 79, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 79, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 80, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 80, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 80, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 80, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 80, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 80, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 80, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 80, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 80, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 80, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 80, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 80, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 81, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 81, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 81, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 81, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 81, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 81, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 81, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 81, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 81, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 81, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 81, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 81, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 82, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 82, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 82, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 82, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 82, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 82, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 82, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 82, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 82, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 82, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 82, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 82, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 83, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 83, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 83, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 83, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 83, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 83, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 83, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 83, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 83, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 83, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 83, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 83, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 84, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 84, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 84, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 84, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 84, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 84, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 84, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 84, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 84, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 84, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 84, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 84, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 85, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 85, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 85, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 85, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 85, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 85, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 85, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 85, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 85, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 85, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 85, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 85, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 86, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 86, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 86, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 86, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 86, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 86, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 86, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 86, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 86, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 86, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 86, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 86, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 87, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 87, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 87, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 87, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 87, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 87, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 87, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 87, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 87, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 87, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 87, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 87, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 88, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 88, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 88, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 88, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 88, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 88, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 88, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 88, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 88, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 88, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 88, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 88, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 89, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 89, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 89, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 89, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 89, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 89, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 89, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 89, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 89, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 89, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 89, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 89, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 90, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 90, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 90, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 90, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 90, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 90, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 90, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 90, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 90, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 90, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 90, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 90, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 91, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 91, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 91, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 91, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 91, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 91, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 91, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 91, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 91, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 91, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 91, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 91, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 92, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 92, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 92, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 92, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 92, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 92, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 92, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 92, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 92, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 92, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 92, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 92, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 93, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 93, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 93, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 93, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 93, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 93, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 93, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 93, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 93, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 93, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 93, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 93, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 94, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 94, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 94, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 94, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 94, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 94, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 94, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 94, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 94, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 94, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 94, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 94, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 95, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 95, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 95, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 95, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 95, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 95, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 95, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 95, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 95, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 95, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 95, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 95, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 96, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 96, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 96, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 96, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 96, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 96, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 96, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 96, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 96, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 96, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 96, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 96, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 97, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 97, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 97, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 97, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 97, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 97, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 97, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 97, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 97, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 97, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 97, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 97, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 98, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 98, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 98, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 98, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 98, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 98, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 98, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 98, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 98, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 98, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 98, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 98, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 99, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 99, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 99, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 99, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 99, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 99, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 99, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 99, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 99, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 99, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 99, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 99, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 100, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 100, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 100, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 100, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 100, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 100, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 100, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 100, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 100, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 100, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 100, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 100, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 101, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 101, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 101, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 101, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 101, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 101, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 101, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 101, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 101, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 101, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 101, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 101, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - predict_PALACE: X_prot is: torch.Size([1, 30])
root - WARNING - predict_PALACE: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Encoder: X_smi is: torch.Size([1, 10])
root - WARNING - PALACE_Encoder: X_smi after embedding is: torch.Size([1, 10, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi after pos_encoding is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: X_smi.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: X_prot.device after pos_encoding is: cuda:0
root - WARNING - PALACE_Encoder: self.convT1d.device is: cuda:0
root - WARNING - PALACE_Encoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Encoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Encoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 10, 16])
root - WARNING - EncoderBlock: X after MultiHeadExternalMixAttention is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after AddNorm is: torch.Size([1, 10, 128])
root - WARNING - EncoderBlock: X after PositionWiseFFN is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Encoder: output is: torch.Size([1, 10, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 1, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 2, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 2, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 2, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 2, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 3, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 3, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 3, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 3, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 4, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 4, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 4, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 4, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 5, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 5, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 5, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 5, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 6, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 6, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 6, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 6, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 7, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 7, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 7, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 7, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 8, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 8, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 8, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 8, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 9, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 9, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 9, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 9, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 10, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 10, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 11, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 11, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 11, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 11, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 12, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 12, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 12, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 12, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 13, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 13, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 13, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 13, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 14, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 14, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 14, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 14, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 15, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 15, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 15, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 15, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 16, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 16, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 16, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 16, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 17, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 17, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 17, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 17, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 18, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 18, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 18, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 18, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 19, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 19, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 19, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 19, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 20, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 20, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 20, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 20, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 21, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 21, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 21, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 21, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 22, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 22, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 22, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 22, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 23, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 23, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 23, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 23, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 24, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 24, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 24, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 24, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 25, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 25, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 25, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 25, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 26, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 26, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 26, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 26, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 27, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 27, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 27, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 27, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 28, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 28, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 28, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 28, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 29, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 29, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 29, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 29, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 30, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 30, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 30, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 30, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 31, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 31, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 31, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 31, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 32, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 32, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 32, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 32, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 33, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 33, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 33, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 33, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 34, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 34, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 34, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 34, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 35, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 35, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 35, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 35, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 36, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 36, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 36, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 36, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 37, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 37, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 37, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 37, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 38, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 38, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 38, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 38, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 39, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 39, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 39, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 39, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 40, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 40, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 40, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 40, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 41, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 41, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 41, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 41, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 42, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 42, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 42, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 42, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 43, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 43, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 43, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 43, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 44, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 44, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 44, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 44, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 45, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 45, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 45, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 45, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 46, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 46, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 46, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 46, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 47, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 47, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 47, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 47, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 48, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 48, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 48, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 48, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 49, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 49, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 49, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 49, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 50, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 50, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 50, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 50, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot is: torch.Size([1, 30])
root - WARNING - PALACE_Decoder: X_smi_y is: torch.Size([1, 1])
root - WARNING - PALACE_Decoder: X_smi_y after embedding is: torch.Size([1, 1, 128])
root - WARNING - PositionalEncoding: output is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_smi_y after pos_encoding is: torch.Size([1, 1, 128])
root - WARNING - PALACE_Decoder: X_prot after convT1d is: torch.Size([1, 30, 1000])
root - WARNING - PALACE_Decoder: X_prot after transpose is: torch.Size([1, 1000, 30])
root - WARNING - ProteinEncoding: X after dense1 and relu1 is: torch.Size([1, 1000, 128])
root - WARNING - PALACE_Decoder: X_prot after prot_encoding is: torch.Size([1, 1000, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: k after concatenation is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: self.training is: False
root - WARNING - DecoderBlock: k after state is: torch.Size([1, 51, 128])
root - WARNING - DecoderBlock: X_smi_y after state is: torch.Size([1, 1, 128])
root - WARNING - DecoderBlock: X_prot after state is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_prot is: torch.Size([1, 1000, 128])
root - WARNING - MultiHeadExternalMixAttention: key_smi is: torch.Size([1, 51, 128])
root - WARNING - MultiHeadExternalMixAttention: key after transpose_qkv is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: query after transpose_qkv is: torch.Size([8, 1, 16])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 51, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 51, 16])
root - WARNING - MultiHeadExternalMixAttention: output after DotProductMixAttention is: torch.Size([8, 1, 16])
root - WARNING - DecoderBlock: X_smi_y2 is: torch.Size([1, 1, 128])
root - WARNING - MultiHeadAttention: keys is: torch.Size([1, 10, 128])
root - WARNING - DotProductMixAttention: k is: torch.Size([8, 10, 16])
root - WARNING - DotProductMixAttention: v is: torch.Size([8, 10, 16])
root - WARNING - DecoderBlock: X_smi_y3 after second attention is: torch.Size([1, 1, 128])
