smi_encoder.embedding.weight shape: torch.Size([166, 4])
tensor([[ 1.0131e+00, -2.6993e-01, -1.8087e+00, -3.0472e-01],
        [ 4.5046e-02, -7.6588e-01,  1.3841e-01, -9.3697e-01],
        [-7.3201e-01, -1.6325e-02, -5.6221e-01, -1.5081e+00],
        [-7.7636e-01, -6.6997e-02, -9.6584e-01,  5.4833e-01],
        [-6.5382e-01,  6.7241e-01, -5.1072e-01,  6.3218e-01],
        [-9.0128e-01, -3.6002e-01, -1.1526e+00, -3.5705e+00],
        [-1.5554e+00,  3.3941e-01,  1.2918e+00, -1.0412e+00],
        [-7.4214e-01, -1.9910e-01,  6.7717e-01, -8.8192e-01],
        [-6.1041e-01,  6.5888e-01, -1.9129e-01, -4.4840e-01],
        [-1.6815e+00,  7.9804e-01, -1.2627e+00,  1.0787e+00],
        [-1.1053e-01, -4.1094e-01,  8.1839e-01, -7.6385e-01],
        [ 6.9270e-01, -7.4990e-01, -9.4435e-01,  3.9496e-01],
        [-1.1244e+00,  7.3405e-02,  2.2270e-01,  8.8916e-03],
        [ 8.0568e-01,  6.8755e-01, -1.4880e-01,  7.9142e-01],
        [ 1.0505e+00,  1.3373e+00,  9.3571e-01,  1.2042e+00],
        [-1.6805e+00,  9.5555e-01,  1.4440e+00,  4.5779e-01],
        [-8.1629e-01,  5.4240e-01, -1.4801e+00,  5.2152e-01],
        [-9.9097e-01,  3.1792e-01, -1.5732e-01,  7.3041e-01],
        [-1.0533e+00, -1.0273e-01,  1.7114e-01,  2.6196e-01],
        [-8.3795e-02, -2.0079e+00, -8.4574e-01,  1.5912e+00],
        [-9.0754e-01, -1.1962e+00, -4.1816e-01, -5.8005e-01],
        [-1.1493e+00, -8.5665e-01,  7.5397e-01, -1.2221e+00],
        [ 1.3754e-02,  6.9965e-01, -8.8917e-01, -7.7130e-01],
        [-6.3248e-01,  9.7289e-01, -5.5024e-01,  8.4759e-02],
        [ 3.6785e-01, -8.8945e-02,  2.0360e-01,  4.7266e-01],
        [ 1.2599e+00, -1.9724e-01,  3.0838e-01, -4.1376e-02],
        [-5.7375e-01, -9.7793e-01, -6.1429e-01, -1.6486e+00],
        [ 8.6263e-02, -9.5812e-02,  6.9204e-01, -1.5664e+00],
        [ 4.7389e-01, -2.5814e+00,  1.3854e+00, -1.2993e+00],
        [ 1.1450e+00, -5.7840e-01,  7.5166e-01,  9.9818e-02],
        [ 8.2079e-01, -3.8253e-01,  4.4788e-01, -8.1493e-02],
        [ 7.6541e-01, -3.2951e-01, -7.4855e-01, -1.9519e+00],
        [ 2.5204e-02,  2.6584e-01,  8.0772e-01,  1.5891e-01],
        [ 9.4150e-01,  1.1322e+00, -1.0530e+00,  1.1068e+00],
        [-9.7048e-01, -7.4434e-01,  1.0291e+00, -1.2716e+00],
        [-2.3408e-01,  7.7225e-01,  6.0721e-02, -1.3471e+00],
        [ 3.8304e-01,  3.8185e-01,  9.9900e-01,  3.3287e-01],
        [-3.9382e-01,  2.1914e+00, -1.3212e+00, -5.5615e-01],
        [ 6.6313e-01,  6.9354e-01, -1.1154e+00, -4.0640e-01],
        [-1.4911e+00, -2.7717e-01,  3.7379e-02,  2.4310e-01],
        [ 1.9300e-01, -1.2070e+00,  3.4850e-01, -4.0633e-01],
        [-1.1838e-01,  5.4434e-01, -6.4252e-01, -4.8612e-01],
        [ 2.5975e-01,  1.3158e+00,  1.1017e+00,  3.9242e-01],
        [-2.5379e-01,  1.4969e-01,  5.8308e-01, -4.9759e-01],
        [ 5.9685e-01, -6.7464e-01,  8.8331e-01, -1.9961e-01],
        [ 2.2215e-01,  9.2874e-01,  3.4473e-01, -7.0404e-01],
        [-1.8701e+00,  1.7933e+00,  2.1865e+00, -6.2543e-01],
        [ 9.8028e-01,  3.7641e-01, -2.3035e+00,  1.3408e+00],
        [ 4.8464e-01,  4.2158e-01, -2.4880e-01, -2.7321e-01],
        [-2.8908e-01,  6.3486e-01,  3.0519e-01, -7.2633e-01],
        [ 8.4956e-01,  1.3242e+00,  3.0940e-01,  1.0487e+00],
        [-2.0583e+00,  7.1041e-01,  5.4650e-01, -4.1758e-02],
        [-9.5407e-01,  5.2850e-01,  1.3224e+00, -8.4517e-01],
        [-5.4076e-01, -1.3140e+00, -1.0480e+00,  1.5353e+00],
        [ 1.6525e-01, -3.6483e-01,  8.1038e-01, -6.0322e-01],
        [-7.2243e-01,  4.1988e-01,  1.3474e+00, -1.6139e-01],
        [-4.7548e-02,  2.0577e+00, -5.2772e-01, -1.0493e-01],
        [-9.6045e-01,  1.1856e+00,  8.7157e-01, -1.9912e+00],
        [-4.5569e-02, -1.3173e-01, -7.6868e-01, -3.1409e-01],
        [-3.6437e-01,  1.5061e+00, -3.3070e-01, -7.4016e-01],
        [ 6.5208e-01, -6.4558e-01, -1.5131e-01, -1.2792e-01],
        [-8.3989e-01,  1.1647e-01, -1.8613e-01,  8.6007e-01],
        [ 1.6512e-01, -1.8730e+00,  5.0541e-01,  1.7715e+00],
        [-2.3607e-01, -9.4699e-01, -1.1543e+00, -1.0745e-01],
        [ 2.4957e-01, -1.1481e+00, -6.8214e-01,  6.2154e-01],
        [-5.9355e-01,  1.6278e+00,  6.2225e-01, -6.5819e-01],
        [-2.5460e+00,  3.7842e-01, -7.1091e-01,  1.2539e+00],
        [ 2.4542e-01,  3.8710e-01, -1.0473e+00, -1.6596e+00],
        [-8.3727e-01,  2.4443e-01,  5.2656e-01,  1.8652e+00],
        [-1.7596e-01,  1.4683e+00,  1.9253e+00, -1.5229e+00],
        [ 9.5958e-02, -3.7848e-01, -1.5286e+00,  4.3830e-01],
        [ 5.2900e-01, -1.7084e-01,  1.4231e+00,  1.0266e+00],
        [-4.4118e-01, -3.7744e-01, -7.5030e-01,  8.3035e-01],
        [ 1.7628e+00,  2.3401e-01, -1.5115e+00, -1.4291e+00],
        [ 1.0971e+00,  1.7432e+00,  1.5822e+00, -7.7300e-01],
        [-1.4133e-02, -8.4483e-01,  1.8396e-01, -3.9365e-01],
        [-6.4570e-01, -6.0166e-01, -4.0005e-01,  1.6650e-01],
        [ 6.4332e-01, -3.8758e-01,  5.3413e-01,  1.0238e+00],
        [-1.2635e+00, -7.3291e-01, -7.2287e-01,  2.9252e-01],
        [-8.9328e-01,  8.6759e-01,  3.0666e-01,  1.7418e+00],
        [-1.3945e-01,  3.1649e+00, -1.3029e+00,  1.6186e+00],
        [ 1.2865e+00, -8.8561e-01,  3.8581e-01, -5.9722e-02],
        [ 1.5660e-01,  2.6593e-01, -1.0842e+00, -4.8537e-01],
        [-2.7944e-01, -3.6641e-01, -1.8581e-01,  6.1672e-01],
        [-1.0982e+00,  3.6427e-01, -9.6361e-01, -2.7267e+00],
        [-1.2004e-01, -6.2556e-01, -6.8437e-02, -1.7542e+00],
        [ 1.5178e-01, -7.1699e-01,  4.6891e-02,  1.6803e-01],
        [ 7.6951e-01,  2.3283e+00, -4.0291e-01,  2.2452e-02],
        [ 1.0175e+00, -5.7009e-01, -9.9324e-01, -9.7507e-01],
        [ 6.5383e-01,  5.1581e-01,  7.0012e-01,  1.3480e+00],
        [-1.0245e+00, -3.5522e-01, -1.2153e+00, -2.6703e-01],
        [ 1.5093e+00, -1.8297e-01, -7.4929e-01, -2.2571e+00],
        [-1.1842e-01, -3.2992e-01, -2.6125e-02,  5.2798e-01],
        [-6.4389e-01,  1.2603e-01, -2.5973e-01, -7.6342e-01],
        [-3.3603e-01, -4.4927e-01, -1.2304e+00,  3.9846e-01],
        [-1.7652e+00, -5.8589e-01,  6.5017e-01, -3.6379e-02],
        [ 1.0759e+00,  3.7974e+00,  1.0696e+00, -5.3811e-01],
        [-1.0961e+00,  6.3686e-01, -3.3300e-01, -8.7964e-01],
        [ 7.9414e-01,  5.9006e-01,  3.2857e-01,  3.3201e-02],
        [-1.6286e+00,  1.3453e-01,  1.8440e+00,  1.4561e+00],
        [-1.1912e+00,  4.7195e-01,  3.9100e-01,  7.1482e-01],
        [ 2.8145e-01,  1.5132e+00,  1.0349e+00,  6.8134e-01],
        [ 2.0735e-01, -7.1718e-01,  1.0621e+00,  5.6786e-02],
        [-1.4558e+00, -4.0942e-01,  8.6929e-01,  1.2653e+00],
        [-6.5452e-01,  9.7074e-03, -3.1745e-01,  1.3057e-01],
        [-1.1139e+00, -1.7350e+00,  3.4868e-01, -1.9361e+00],
        [ 1.4797e+00,  4.4370e-01, -9.7386e-01, -2.7284e-01],
        [-2.8814e-01,  1.3470e+00, -6.2055e-01,  1.0262e+00],
        [-1.6662e+00, -1.4632e-01, -2.5573e-01,  2.9434e-01],
        [ 2.6148e-01,  5.3582e-01, -1.8129e-01,  9.7581e-01],
        [ 6.8185e-01, -1.1463e+00, -4.6554e-01,  7.6143e-01],
        [ 1.2197e-01, -1.7045e+00,  6.8611e-01,  4.2773e-01],
        [ 3.7457e-01, -4.5867e-01, -9.8755e-01,  1.0579e-01],
        [-7.5393e-01,  2.1373e-01,  8.6656e-01,  9.2454e-01],
        [ 1.1276e+00,  5.9563e-01, -6.5324e-02,  7.3156e-01],
        [-6.6423e-02, -1.1889e+00,  5.5827e-01, -5.3598e-01],
        [ 6.1100e-01,  1.2608e-01, -2.1567e-01, -4.0231e-01],
        [-6.9156e-01, -1.2429e+00, -1.4603e+00, -5.3779e-01],
        [-1.0161e-01,  3.9701e-01, -1.5166e-01, -5.1606e-01],
        [ 5.9023e-03,  8.1569e-01,  2.0376e+00, -2.2426e+00],
        [ 4.6220e-01,  8.5042e-01,  1.1872e+00, -3.7650e-01],
        [ 9.7320e-02,  3.4538e-01,  1.7431e+00, -9.9991e-03],
        [ 1.1162e+00, -7.5952e-01,  7.8386e-02,  8.0588e-01],
        [-4.5416e-01,  4.7940e-01, -2.4754e-02,  7.7448e-01],
        [-4.1892e-01,  3.6056e-01,  6.5907e-01, -4.3547e-01],
        [-1.9647e+00, -5.3310e-01, -1.9000e-01, -5.3393e-01],
        [ 1.9628e-01, -1.9690e+00, -8.0523e-01, -4.2153e-03],
        [-1.3090e+00, -2.1178e+00, -6.4361e-01, -1.3667e+00],
        [ 6.4657e-01, -3.7327e-01, -1.2329e-01, -7.6379e-01],
        [ 1.5109e+00,  1.2825e+00, -1.1684e+00, -2.4499e-01],
        [-2.9964e-02,  7.3207e-01, -1.1377e+00, -1.1668e+00],
        [ 2.3365e-02, -1.4335e+00,  3.5683e-01,  1.6054e+00],
        [ 3.2136e-01,  5.3508e-02,  9.0148e-01, -1.1303e+00],
        [ 8.2394e-01,  1.2965e+00,  1.5062e+00,  6.6523e-01],
        [ 1.5187e-01, -2.5614e-01,  2.0775e-02, -4.4587e-01],
        [-2.8666e-01, -2.2180e-01,  7.6945e-01,  1.5405e-01],
        [ 8.8875e-01,  1.7435e-02, -1.3869e+00,  6.7617e-01],
        [ 1.3716e-01,  1.1459e+00,  6.1049e-01,  1.2803e+00],
        [-1.9769e+00,  7.1451e-01,  1.2227e+00,  7.0288e-01],
        [-8.9315e-01, -4.2809e-02,  7.0177e-01, -1.5000e+00],
        [ 6.1636e-01, -2.6393e-01,  7.9300e-01,  6.2592e-02],
        [-7.5907e-01, -6.6632e-01, -1.4819e+00,  1.2169e-01],
        [-1.4683e+00,  1.9978e-01, -2.2164e-01,  3.3839e-01],
        [-1.1188e+00, -2.2691e-01,  3.8166e-01, -4.4427e-01],
        [-7.0641e-01,  3.1847e-01, -1.5320e-01, -1.2494e-01],
        [ 1.3269e+00,  9.6433e-01,  2.8677e-01,  5.7407e-01],
        [ 1.3655e-01, -1.7122e+00, -2.0037e-01,  8.5971e-01],
        [ 2.6728e+00,  1.0386e-01, -7.1405e-01,  4.7165e-01],
        [-9.2907e-01,  1.3353e+00,  6.4790e-01,  9.0971e-02],
        [ 9.1598e-01, -7.6363e-02,  6.2318e-01, -4.3269e-02],
        [-9.0088e-01, -1.3379e+00, -8.4119e-01, -6.0386e-01],
        [-4.2908e-01,  2.2600e-01, -1.3858e+00, -8.3831e-01],
        [ 1.4827e+00, -1.8623e+00, -2.7756e-01, -1.4766e+00],
        [-2.1084e-01,  1.2317e+00,  1.2027e+00, -7.9652e-01],
        [ 2.2316e+00,  2.5303e-01, -4.1316e-01, -3.4680e-01],
        [ 9.3971e-01, -1.9872e-03, -1.0787e+00,  8.5000e-01],
        [-4.9030e-01, -5.2751e-01, -2.2845e-01, -1.7396e+00],
        [-1.4763e+00,  1.0694e+00, -5.5077e-01, -1.0309e-01],
        [-2.6407e+00, -7.0880e-01, -1.8564e-01, -2.9272e-01],
        [-3.4716e-01,  3.6107e-01,  8.5933e-01, -1.6352e-01],
        [-1.1132e-01, -1.6304e+00, -6.0746e-01,  2.2077e-01],
        [-7.7208e-01, -4.6686e-01,  9.1119e-02, -8.7455e-01],
        [-3.9112e-01, -1.0575e+00,  1.0913e+00,  2.4273e-01],
        [ 1.3946e-01, -4.4136e-01,  2.0537e-01,  3.1411e-01],
        [ 5.1993e-01, -1.5443e-01,  1.7097e+00,  5.5037e-01],
        [-1.6654e+00, -3.8551e-01,  8.8131e-01, -1.1999e+00]])
tensor([[ 1.0131e+00, -2.6993e-01, -1.8087e+00, -3.0472e-01],
        [ 3.2767e-02, -7.7640e-01,  1.3821e-01, -9.3373e-01],
        [-7.3201e-01, -1.6325e-02, -5.6221e-01, -1.5081e+00],
        [-7.7585e-01, -6.9127e-02, -9.6604e-01,  5.4959e-01],
        [-6.5793e-01,  6.6744e-01, -5.0970e-01,  6.3346e-01],
        [-9.0146e-01, -3.6002e-01, -1.1540e+00, -3.5703e+00],
        [-1.5523e+00,  3.4713e-01,  1.2929e+00, -1.0448e+00],
        [-7.4332e-01, -1.9216e-01,  6.7440e-01, -8.8360e-01],
        [-6.0959e-01,  6.5977e-01, -1.9133e-01, -4.4976e-01],
        [-1.6799e+00,  7.9823e-01, -1.2629e+00,  1.0784e+00],
        [-1.0949e-01, -4.0790e-01,  8.1643e-01, -7.6473e-01],
        [ 6.9267e-01, -7.5037e-01, -9.4433e-01,  3.9495e-01],
        [-1.1218e+00,  7.4706e-02,  2.2239e-01,  6.5931e-03],
        [ 8.0587e-01,  6.8627e-01, -1.4928e-01,  7.9161e-01],
        [ 1.0538e+00,  1.3384e+00,  9.3683e-01,  1.2093e+00],
        [-1.6812e+00,  9.5830e-01,  1.4417e+00,  4.5378e-01],
        [-8.1607e-01,  5.4089e-01, -1.4801e+00,  5.2207e-01],
        [-9.8959e-01,  3.1921e-01, -1.5756e-01,  7.2982e-01],
        [-1.0530e+00, -1.0272e-01,  1.7089e-01,  2.6192e-01],
        [-8.3633e-02, -2.0079e+00, -8.4554e-01,  1.5916e+00],
        [-9.0723e-01, -1.1961e+00, -4.1840e-01, -5.8000e-01],
        [-1.1489e+00, -8.5604e-01,  7.5433e-01, -1.2224e+00],
        [ 1.3750e-02,  6.9965e-01, -8.8916e-01, -7.7131e-01],
        [-6.3249e-01,  9.7290e-01, -5.5024e-01,  8.4739e-02],
        [ 3.6788e-01, -8.8781e-02,  2.0380e-01,  4.7233e-01],
        [ 1.2600e+00, -1.9708e-01,  3.0835e-01, -4.1431e-02],
        [-5.7379e-01, -9.7783e-01, -6.1429e-01, -1.6486e+00],
        [ 8.6267e-02, -9.5365e-02,  6.9192e-01, -1.5666e+00],
        [ 4.7385e-01, -2.5815e+00,  1.3854e+00, -1.2993e+00],
        [ 1.1450e+00, -5.7832e-01,  7.5167e-01,  9.9757e-02],
        [ 8.2080e-01, -3.8254e-01,  4.4787e-01, -8.1491e-02],
        [ 7.6539e-01, -3.2946e-01, -7.4859e-01, -1.9519e+00],
        [ 2.5007e-02,  2.6543e-01,  8.0782e-01,  1.5916e-01],
        [ 9.4150e-01,  1.1322e+00, -1.0530e+00,  1.1068e+00],
        [-9.7048e-01, -7.4434e-01,  1.0291e+00, -1.2716e+00],
        [-2.3410e-01,  7.7225e-01,  6.0721e-02, -1.3471e+00],
        [ 3.8330e-01,  3.8225e-01,  9.9900e-01,  3.3180e-01],
        [-3.9380e-01,  2.1914e+00, -1.3212e+00, -5.5616e-01],
        [ 6.6313e-01,  6.9354e-01, -1.1154e+00, -4.0640e-01],
        [-1.4911e+00, -2.7716e-01,  3.7378e-02,  2.4310e-01],
        [ 1.9300e-01, -1.2070e+00,  3.4851e-01, -4.0634e-01],
        [-1.1838e-01,  5.4434e-01, -6.4252e-01, -4.8612e-01],
        [ 2.5971e-01,  1.3158e+00,  1.1017e+00,  3.9238e-01],
        [-2.5379e-01,  1.4969e-01,  5.8308e-01, -4.9759e-01],
        [ 5.9683e-01, -6.7466e-01,  8.8329e-01, -1.9957e-01],
        [ 2.2215e-01,  9.2874e-01,  3.4473e-01, -7.0404e-01],
        [-1.8700e+00,  1.7932e+00,  2.1864e+00, -6.2554e-01],
        [ 9.8028e-01,  3.7641e-01, -2.3035e+00,  1.3408e+00],
        [ 4.8463e-01,  4.2159e-01, -2.4880e-01, -2.7321e-01],
        [-2.8908e-01,  6.3486e-01,  3.0519e-01, -7.2633e-01],
        [ 8.4956e-01,  1.3242e+00,  3.0940e-01,  1.0487e+00],
        [-2.0583e+00,  7.1041e-01,  5.4650e-01, -4.1758e-02],
        [-9.5407e-01,  5.2850e-01,  1.3224e+00, -8.4517e-01],
        [-5.4076e-01, -1.3140e+00, -1.0480e+00,  1.5353e+00],
        [ 1.6525e-01, -3.6483e-01,  8.1038e-01, -6.0322e-01],
        [-7.2244e-01,  4.1989e-01,  1.3474e+00, -1.6138e-01],
        [-4.7548e-02,  2.0577e+00, -5.2772e-01, -1.0493e-01],
        [-9.6045e-01,  1.1856e+00,  8.7157e-01, -1.9912e+00],
        [-4.5569e-02, -1.3173e-01, -7.6868e-01, -3.1409e-01],
        [-3.6437e-01,  1.5061e+00, -3.3070e-01, -7.4016e-01],
        [ 6.5208e-01, -6.4558e-01, -1.5131e-01, -1.2792e-01],
        [-8.3989e-01,  1.1647e-01, -1.8613e-01,  8.6007e-01],
        [ 1.6512e-01, -1.8730e+00,  5.0541e-01,  1.7715e+00],
        [-2.3607e-01, -9.4699e-01, -1.1543e+00, -1.0745e-01],
        [ 2.4957e-01, -1.1481e+00, -6.8214e-01,  6.2154e-01],
        [-5.9355e-01,  1.6278e+00,  6.2225e-01, -6.5819e-01],
        [-2.5460e+00,  3.7842e-01, -7.1091e-01,  1.2539e+00],
        [ 2.4542e-01,  3.8710e-01, -1.0473e+00, -1.6596e+00],
        [-8.3725e-01,  2.4437e-01,  5.2652e-01,  1.8652e+00],
        [-1.7596e-01,  1.4683e+00,  1.9253e+00, -1.5229e+00],
        [ 9.5958e-02, -3.7848e-01, -1.5286e+00,  4.3830e-01],
        [ 5.2900e-01, -1.7084e-01,  1.4231e+00,  1.0266e+00],
        [-4.4118e-01, -3.7744e-01, -7.5030e-01,  8.3035e-01],
        [ 1.7628e+00,  2.3401e-01, -1.5115e+00, -1.4291e+00],
        [ 1.0971e+00,  1.7432e+00,  1.5822e+00, -7.7300e-01],
        [-1.4133e-02, -8.4483e-01,  1.8396e-01, -3.9365e-01],
        [-6.4570e-01, -6.0166e-01, -4.0005e-01,  1.6650e-01],
        [ 6.4332e-01, -3.8758e-01,  5.3413e-01,  1.0238e+00],
        [-1.2635e+00, -7.3291e-01, -7.2287e-01,  2.9252e-01],
        [-8.9328e-01,  8.6759e-01,  3.0666e-01,  1.7418e+00],
        [-1.3945e-01,  3.1649e+00, -1.3029e+00,  1.6186e+00],
        [ 1.2865e+00, -8.8561e-01,  3.8581e-01, -5.9722e-02],
        [ 1.5660e-01,  2.6593e-01, -1.0842e+00, -4.8537e-01],
        [-2.7944e-01, -3.6641e-01, -1.8581e-01,  6.1672e-01],
        [-1.0982e+00,  3.6427e-01, -9.6361e-01, -2.7267e+00],
        [-1.2004e-01, -6.2556e-01, -6.8437e-02, -1.7542e+00],
        [ 1.5178e-01, -7.1699e-01,  4.6891e-02,  1.6803e-01],
        [ 7.6951e-01,  2.3283e+00, -4.0291e-01,  2.2452e-02],
        [ 1.0175e+00, -5.7009e-01, -9.9324e-01, -9.7507e-01],
        [ 6.5383e-01,  5.1581e-01,  7.0012e-01,  1.3480e+00],
        [-1.0245e+00, -3.5522e-01, -1.2153e+00, -2.6703e-01],
        [ 1.5093e+00, -1.8297e-01, -7.4929e-01, -2.2571e+00],
        [-1.1842e-01, -3.2992e-01, -2.6125e-02,  5.2798e-01],
        [-6.4389e-01,  1.2603e-01, -2.5973e-01, -7.6342e-01],
        [-3.3603e-01, -4.4927e-01, -1.2304e+00,  3.9846e-01],
        [-1.7652e+00, -5.8589e-01,  6.5017e-01, -3.6379e-02],
        [ 1.0759e+00,  3.7974e+00,  1.0696e+00, -5.3811e-01],
        [-1.0961e+00,  6.3686e-01, -3.3300e-01, -8.7964e-01],
        [ 7.9414e-01,  5.9006e-01,  3.2857e-01,  3.3201e-02],
        [-1.6286e+00,  1.3453e-01,  1.8440e+00,  1.4561e+00],
        [-1.1912e+00,  4.7195e-01,  3.9100e-01,  7.1482e-01],
        [ 2.8145e-01,  1.5132e+00,  1.0349e+00,  6.8134e-01],
        [ 2.0735e-01, -7.1718e-01,  1.0621e+00,  5.6786e-02],
        [-1.4558e+00, -4.0942e-01,  8.6929e-01,  1.2653e+00],
        [-6.5452e-01,  9.7074e-03, -3.1745e-01,  1.3057e-01],
        [-1.1139e+00, -1.7350e+00,  3.4868e-01, -1.9361e+00],
        [ 1.4797e+00,  4.4370e-01, -9.7386e-01, -2.7284e-01],
        [-2.8814e-01,  1.3470e+00, -6.2055e-01,  1.0262e+00],
        [-1.6662e+00, -1.4632e-01, -2.5573e-01,  2.9434e-01],
        [ 2.6148e-01,  5.3582e-01, -1.8129e-01,  9.7581e-01],
        [ 6.8185e-01, -1.1463e+00, -4.6554e-01,  7.6143e-01],
        [ 1.2197e-01, -1.7045e+00,  6.8611e-01,  4.2773e-01],
        [ 3.7457e-01, -4.5867e-01, -9.8755e-01,  1.0579e-01],
        [-7.5393e-01,  2.1373e-01,  8.6656e-01,  9.2454e-01],
        [ 1.1276e+00,  5.9563e-01, -6.5324e-02,  7.3156e-01],
        [-6.6423e-02, -1.1889e+00,  5.5827e-01, -5.3598e-01],
        [ 6.1100e-01,  1.2608e-01, -2.1567e-01, -4.0231e-01],
        [-6.9156e-01, -1.2429e+00, -1.4603e+00, -5.3779e-01],
        [-1.0161e-01,  3.9701e-01, -1.5166e-01, -5.1606e-01],
        [ 5.9023e-03,  8.1569e-01,  2.0376e+00, -2.2426e+00],
        [ 4.6220e-01,  8.5042e-01,  1.1872e+00, -3.7650e-01],
        [ 9.7320e-02,  3.4538e-01,  1.7431e+00, -9.9991e-03],
        [ 1.1162e+00, -7.5952e-01,  7.8386e-02,  8.0588e-01],
        [-4.5416e-01,  4.7940e-01, -2.4754e-02,  7.7448e-01],
        [-4.1892e-01,  3.6056e-01,  6.5907e-01, -4.3547e-01],
        [-1.9647e+00, -5.3310e-01, -1.9000e-01, -5.3393e-01],
        [ 1.9628e-01, -1.9690e+00, -8.0523e-01, -4.2153e-03],
        [-1.3090e+00, -2.1178e+00, -6.4361e-01, -1.3667e+00],
        [ 6.4657e-01, -3.7327e-01, -1.2329e-01, -7.6379e-01],
        [ 1.5109e+00,  1.2825e+00, -1.1684e+00, -2.4499e-01],
        [-2.9964e-02,  7.3207e-01, -1.1377e+00, -1.1668e+00],
        [ 2.3365e-02, -1.4335e+00,  3.5683e-01,  1.6054e+00],
        [ 3.2136e-01,  5.3508e-02,  9.0148e-01, -1.1303e+00],
        [ 8.2394e-01,  1.2965e+00,  1.5062e+00,  6.6523e-01],
        [ 1.5187e-01, -2.5614e-01,  2.0775e-02, -4.4587e-01],
        [-2.8666e-01, -2.2180e-01,  7.6945e-01,  1.5405e-01],
        [ 8.8875e-01,  1.7435e-02, -1.3869e+00,  6.7617e-01],
        [ 1.3716e-01,  1.1459e+00,  6.1049e-01,  1.2803e+00],
        [-1.9769e+00,  7.1451e-01,  1.2227e+00,  7.0288e-01],
        [-8.9315e-01, -4.2809e-02,  7.0177e-01, -1.5000e+00],
        [ 6.1636e-01, -2.6393e-01,  7.9300e-01,  6.2592e-02],
        [-7.5907e-01, -6.6632e-01, -1.4819e+00,  1.2169e-01],
        [-1.4683e+00,  1.9978e-01, -2.2164e-01,  3.3839e-01],
        [-1.1188e+00, -2.2691e-01,  3.8166e-01, -4.4427e-01],
        [-7.0641e-01,  3.1847e-01, -1.5320e-01, -1.2494e-01],
        [ 1.3269e+00,  9.6433e-01,  2.8677e-01,  5.7407e-01],
        [ 1.3655e-01, -1.7122e+00, -2.0037e-01,  8.5971e-01],
        [ 2.6728e+00,  1.0386e-01, -7.1405e-01,  4.7165e-01],
        [-9.2907e-01,  1.3353e+00,  6.4790e-01,  9.0971e-02],
        [ 9.1598e-01, -7.6363e-02,  6.2318e-01, -4.3269e-02],
        [-9.0088e-01, -1.3379e+00, -8.4119e-01, -6.0386e-01],
        [-4.2908e-01,  2.2600e-01, -1.3858e+00, -8.3831e-01],
        [ 1.4827e+00, -1.8623e+00, -2.7756e-01, -1.4766e+00],
        [-2.1084e-01,  1.2317e+00,  1.2027e+00, -7.9652e-01],
        [ 2.2316e+00,  2.5303e-01, -4.1316e-01, -3.4680e-01],
        [ 9.3971e-01, -1.9872e-03, -1.0787e+00,  8.5000e-01],
        [-4.9030e-01, -5.2751e-01, -2.2845e-01, -1.7396e+00],
        [-1.4763e+00,  1.0694e+00, -5.5077e-01, -1.0309e-01],
        [-2.6407e+00, -7.0880e-01, -1.8564e-01, -2.9272e-01],
        [-3.4716e-01,  3.6107e-01,  8.5933e-01, -1.6352e-01],
        [-1.1132e-01, -1.6304e+00, -6.0746e-01,  2.2077e-01],
        [-7.7208e-01, -4.6686e-01,  9.1119e-02, -8.7455e-01],
        [-3.9112e-01, -1.0575e+00,  1.0913e+00,  2.4273e-01],
        [ 1.3946e-01, -4.4136e-01,  2.0537e-01,  3.1411e-01],
        [ 5.1993e-01, -1.5443e-01,  1.7097e+00,  5.5037e-01],
        [-1.6654e+00, -3.8551e-01,  8.8131e-01, -1.1999e+00]])
##########################################################
smi_encoder.encoder.layers.0.self_attn.in_proj_weight shape: torch.Size([12, 4])
tensor([[ 0.1354,  0.0935,  0.5083,  0.2373],
        [-0.2088, -0.4596,  0.5459, -0.4810],
        [ 0.1761,  0.3796,  0.3331,  0.2478],
        [ 0.4537,  0.1246, -0.2429, -0.2106],
        [-0.5046,  0.5150, -0.2538, -0.2880],
        [-0.0499,  0.3464,  0.4363,  0.4903],
        [ 0.3856, -0.2899, -0.4789, -0.4150],
        [ 0.2491,  0.4767, -0.5088, -0.2479],
        [ 0.3864, -0.2620, -0.2108,  0.4861],
        [ 0.6120,  0.4929, -0.2528, -0.1705],
        [ 0.4245, -0.0739, -0.5035, -0.0305],
        [-0.2691, -0.5042, -0.4044, -0.1646]])
tensor([[ 0.1291,  0.1062,  0.5138,  0.2489],
        [-0.2083, -0.4612,  0.5424, -0.4822],
        [ 0.1773,  0.3831,  0.3328,  0.2502],
        [ 0.4538,  0.1242, -0.2430, -0.2114],
        [-0.5147,  0.5140, -0.2508, -0.2955],
        [-0.0403,  0.3431,  0.4380,  0.4905],
        [ 0.3847, -0.2926, -0.4798, -0.4179],
        [ 0.2496,  0.4765, -0.5089, -0.2486],
        [ 0.3909, -0.2542, -0.2102,  0.4927],
        [ 0.6144,  0.4938, -0.2513, -0.1679],
        [ 0.4217, -0.0704, -0.5034, -0.0279],
        [-0.2636, -0.5114, -0.4056, -0.1706]])
##########################################################
smi_encoder.encoder.layers.0.self_attn.in_proj_bias shape: torch.Size([12])
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
tensor([-1.8886e-03,  3.0372e-04, -1.7351e-03,  2.1558e-04, -6.1388e-12,
        -4.0294e-10, -3.8519e-12,  8.3921e-12, -1.8708e-04,  1.0610e-05,
        -2.9151e-04,  4.1991e-04])
##########################################################
smi_encoder.encoder.layers.0.self_attn.out_proj.weight shape: torch.Size([4, 4])
tensor([[-0.4867,  0.3804, -0.3928,  0.2311],
        [-0.0695,  0.0858, -0.2365,  0.2840],
        [-0.0662, -0.2072, -0.0525,  0.2938],
        [ 0.4363,  0.4625, -0.2027, -0.0905]])
tensor([[-0.4864,  0.3771, -0.3868,  0.2346],
        [-0.0780,  0.0786, -0.2301,  0.2944],
        [-0.0640, -0.2043, -0.0545,  0.2899],
        [ 0.4420,  0.4702, -0.2131, -0.1004]])
##########################################################
smi_encoder.encoder.layers.0.self_attn.out_proj.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([-0.0007,  0.0038, -0.0020, -0.0009])
##########################################################
smi_encoder.encoder.layers.0.linear1.weight shape: torch.Size([8, 4])
tensor([[-0.1779, -0.5301, -0.1414, -0.4070],
        [ 0.6997,  0.6914, -0.2610, -0.6719],
        [-0.0862, -0.5048,  0.1879, -0.2492],
        [ 0.5302,  0.5604, -0.6513, -0.3394],
        [ 0.0607,  0.3345, -0.1089,  0.4988],
        [ 0.4874,  0.1768,  0.1906, -0.3215],
        [ 0.0957,  0.2267, -0.1887, -0.6469],
        [-0.6524,  0.6788,  0.2802, -0.2575]])
tensor([[-0.1829, -0.5278, -0.1395, -0.4062],
        [ 0.6994,  0.6923, -0.2594, -0.6741],
        [-0.0786, -0.5096,  0.1832, -0.2475],
        [ 0.5320,  0.5612, -0.6484, -0.3449],
        [ 0.0628,  0.3344, -0.1095,  0.4974],
        [ 0.4867,  0.1771,  0.1909, -0.3213],
        [ 0.0897,  0.2298, -0.1850, -0.6476],
        [-0.6549,  0.6789,  0.2827, -0.2575]])
##########################################################
smi_encoder.encoder.layers.0.linear1.bias shape: torch.Size([8])
tensor([ 0.4415, -0.1470,  0.1741,  0.3550,  0.4807, -0.2365,  0.1034, -0.3309])
tensor([ 0.4427, -0.1458,  0.1710,  0.3563,  0.4793, -0.2369,  0.1059, -0.3292])
##########################################################
smi_encoder.encoder.layers.0.linear2.weight shape: torch.Size([4, 8])
tensor([[-0.2295, -0.1840, -0.0956, -0.2003,  0.1119,  0.2491,  0.3398, -0.1179],
        [-0.2561,  0.6424,  0.0038,  0.1509,  0.2842, -0.5461,  0.4223,  0.2371],
        [-0.5015,  0.2130, -0.5327,  0.4563,  0.3960,  0.5274,  0.5103, -0.5191],
        [-0.4742, -0.1392,  0.6926,  0.1355,  0.3657, -0.4257, -0.1223,  0.1588]])
tensor([[-0.2325, -0.1896, -0.0982, -0.2039,  0.1161,  0.2456,  0.3349, -0.1162],
        [-0.2542,  0.6421,  0.0053,  0.1512,  0.2840, -0.5469,  0.4233,  0.2498],
        [-0.5005,  0.2165, -0.5317,  0.4583,  0.3936,  0.5295,  0.5132, -0.5210],
        [-0.4741, -0.1369,  0.6928,  0.1364,  0.3639, -0.4235, -0.1215,  0.1461]])
##########################################################
smi_encoder.encoder.layers.0.linear2.bias shape: torch.Size([4])
tensor([ 0.3419, -0.3036, -0.1018,  0.0962])
tensor([ 0.3419, -0.2976, -0.1029,  0.0910])
##########################################################
smi_encoder.encoder.layers.0.norm1.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9921, 1.0105, 0.9970, 1.0030])
##########################################################
smi_encoder.encoder.layers.0.norm1.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0005,  0.0096, -0.0029, -0.0086])
##########################################################
smi_encoder.encoder.layers.0.norm2.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9898, 1.0099, 1.0030, 0.9973])
##########################################################
smi_encoder.encoder.layers.0.norm2.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([-0.0031,  0.0015,  0.0007, -0.0014])
##########################################################
smi_encoder.encoder.layers.1.self_attn.in_proj_weight shape: torch.Size([12, 4])
tensor([[ 0.1354,  0.0935,  0.5083,  0.2373],
        [-0.2088, -0.4596,  0.5459, -0.4810],
        [ 0.1761,  0.3796,  0.3331,  0.2478],
        [ 0.4537,  0.1246, -0.2429, -0.2106],
        [-0.5046,  0.5150, -0.2538, -0.2880],
        [-0.0499,  0.3464,  0.4363,  0.4903],
        [ 0.3856, -0.2899, -0.4789, -0.4150],
        [ 0.2491,  0.4767, -0.5088, -0.2479],
        [ 0.3864, -0.2620, -0.2108,  0.4861],
        [ 0.6120,  0.4929, -0.2528, -0.1705],
        [ 0.4245, -0.0739, -0.5035, -0.0305],
        [-0.2691, -0.5042, -0.4044, -0.1646]])
tensor([[ 0.1326,  0.0947,  0.5080,  0.2392],
        [-0.2094, -0.4589,  0.5460, -0.4812],
        [ 0.1761,  0.3799,  0.3333,  0.2473],
        [ 0.4539,  0.1244, -0.2429, -0.2106],
        [-0.5046,  0.5150, -0.2538, -0.2881],
        [-0.0494,  0.3452,  0.4360,  0.4913],
        [ 0.3857, -0.2900, -0.4790, -0.4149],
        [ 0.2492,  0.4767, -0.5088, -0.2480],
        [ 0.3806, -0.2547, -0.2133,  0.4872],
        [ 0.6145,  0.4891, -0.2522, -0.1697],
        [ 0.4221, -0.0702, -0.5044, -0.0308],
        [-0.2678, -0.5059, -0.4035, -0.1651]])
##########################################################
smi_encoder.encoder.layers.1.self_attn.in_proj_bias shape: torch.Size([12])
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
tensor([-4.4720e-04,  5.6255e-05,  1.7808e-04,  6.5161e-05, -1.1264e-12,
        -7.0316e-13,  1.2605e-14,  4.6428e-13,  1.8979e-04, -1.5598e-03,
         5.2151e-04,  6.6691e-04])
##########################################################
smi_encoder.encoder.layers.1.self_attn.out_proj.weight shape: torch.Size([4, 4])
tensor([[-0.4867,  0.3804, -0.3928,  0.2311],
        [-0.0695,  0.0858, -0.2365,  0.2840],
        [-0.0662, -0.2072, -0.0525,  0.2938],
        [ 0.4363,  0.4625, -0.2027, -0.0905]])
tensor([[-0.4814,  0.3792, -0.3903,  0.2336],
        [-0.0690,  0.0860, -0.2360,  0.2845],
        [-0.0703, -0.2062, -0.0545,  0.2918],
        [ 0.4347,  0.4626, -0.2037, -0.0914]])
##########################################################
smi_encoder.encoder.layers.1.self_attn.out_proj.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([-0.0023,  0.0027,  0.0010, -0.0015])
##########################################################
smi_encoder.encoder.layers.1.linear1.weight shape: torch.Size([8, 4])
tensor([[ 0.1655, -0.6345,  0.4418,  0.7044],
        [-0.4581, -0.2811, -0.6480, -0.6824],
        [ 0.6380,  0.2604,  0.2128,  0.2502],
        [-0.2303, -0.3175, -0.5566,  0.5581],
        [-0.0345,  0.0535, -0.6558, -0.3956],
        [-0.5047,  0.2688,  0.0010, -0.4337],
        [ 0.4923,  0.5620, -0.6144, -0.1643],
        [ 0.0011, -0.0585, -0.6989,  0.3548]])
tensor([[ 0.1548, -0.6310,  0.4384,  0.7152],
        [-0.4579, -0.2808, -0.6482, -0.6825],
        [ 0.6398,  0.2610,  0.2138,  0.2468],
        [-0.2154, -0.3210, -0.5537,  0.5437],
        [-0.0310,  0.0544, -0.6538, -0.4020],
        [-0.5137,  0.2735,  0.0020, -0.4304],
        [ 0.4896,  0.5633, -0.6162, -0.1611],
        [-0.0027, -0.0592, -0.7014,  0.3617]])
##########################################################
smi_encoder.encoder.layers.1.linear1.bias shape: torch.Size([8])
tensor([ 0.4415, -0.1470,  0.1741,  0.3550,  0.4807, -0.2365,  0.1034, -0.3309])
tensor([ 0.4482, -0.1467,  0.1764,  0.3447,  0.4820, -0.2298,  0.1024, -0.3276])
##########################################################
smi_encoder.encoder.layers.1.linear2.weight shape: torch.Size([4, 8])
tensor([[ 0.3892, -0.2787, -0.0482, -0.6351,  0.1303, -0.2320,  0.2444,  0.3695],
        [ 0.2083, -0.4755, -0.6831, -0.6286,  0.4102,  0.5726,  0.3714,  0.1537],
        [-0.4922, -0.1936,  0.3458,  0.6922,  0.5195, -0.3970,  0.1336, -0.4650],
        [ 0.3095, -0.1594, -0.4599, -0.2275,  0.3919,  0.2746, -0.4222,  0.4242]])
tensor([[ 0.3914, -0.2806, -0.0516, -0.6278,  0.1245, -0.2343,  0.2387,  0.3722],
        [ 0.2037, -0.4752, -0.6856, -0.6251,  0.4121,  0.5780,  0.3730,  0.1548],
        [-0.4979, -0.1919,  0.3482,  0.6801,  0.5222, -0.3971,  0.1380, -0.4691],
        [ 0.3175, -0.1594, -0.4563, -0.2264,  0.3928,  0.2714, -0.4225,  0.4245]])
##########################################################
smi_encoder.encoder.layers.1.linear2.bias shape: torch.Size([4])
tensor([ 0.3419, -0.3036, -0.1018,  0.0962])
tensor([ 0.3381, -0.2993, -0.1056,  0.0991])
##########################################################
smi_encoder.encoder.layers.1.norm1.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9884, 1.0141, 1.0012, 1.0003])
##########################################################
smi_encoder.encoder.layers.1.norm1.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([-0.0029,  0.0050,  0.0028,  0.0005])
##########################################################
smi_encoder.encoder.layers.1.norm2.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9645, 1.0299, 1.0002, 1.0055])
##########################################################
smi_encoder.encoder.layers.1.norm2.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0089, -0.0043, -0.0048, -0.0201])
##########################################################
prot_encoder.embedding.weight shape: torch.Size([166, 4])
tensor([[-1.8644e+00,  3.7705e-01, -2.2484e+00, -9.1572e-01],
        [-5.6899e-02, -6.3255e-01,  2.1041e-01, -1.9431e+00],
        [-7.3469e-02,  6.1472e-01, -1.4582e+00, -4.4833e-01],
        [ 2.0210e-01,  6.6697e-01, -1.5160e+00, -1.3400e+00],
        [ 2.0156e+00, -5.3391e-01, -2.9000e-01, -2.8906e+00],
        [ 5.7361e-01,  3.2049e-01, -6.2160e-01,  1.0768e+00],
        [ 1.7104e-01,  9.3137e-03,  7.4950e-01, -1.3619e+00],
        [ 8.1322e-01, -2.6990e+00, -3.2526e-01,  1.9606e+00],
        [-6.2601e-01, -4.4899e-01,  1.1180e+00,  1.0247e+00],
        [-3.6765e-01,  5.6901e-01, -8.6035e-02,  1.4087e+00],
        [-1.4622e+00, -1.3210e+00, -1.1693e+00, -2.4946e-01],
        [ 1.0365e-01, -1.3072e-01, -1.9194e-02,  5.9288e-01],
        [ 2.4042e+00,  8.2379e-01,  1.1206e+00,  4.7186e-01],
        [ 2.9067e-01,  2.0294e+00,  8.3379e-01,  8.2568e-01],
        [ 1.3443e+00,  1.1339e+00,  2.2029e+00, -1.3935e+00],
        [ 7.3267e-01,  8.6056e-01, -3.7104e-01,  7.2927e-01],
        [ 8.0854e-01,  1.1520e-01, -2.2800e+00, -3.5769e-01],
        [ 2.2725e+00, -2.1749e-01,  1.9818e-01,  1.5564e+00],
        [-5.0612e-01, -4.7656e-01, -3.1234e-01, -8.9018e-01],
        [ 6.1389e-01, -1.2501e-01, -3.1142e+00,  5.5700e-01],
        [ 1.2651e-01, -8.5039e-01,  8.4784e-01, -6.8849e-01],
        [ 9.9817e-01,  1.0895e+00,  1.5442e+00,  1.1176e+00],
        [-1.1087e+00, -1.0796e-01, -1.2968e+00, -4.8157e-01],
        [-6.5682e-01,  6.9242e-01, -6.1162e-01,  1.1064e+00],
        [ 5.2305e-01,  9.8255e-01,  6.6593e-01, -1.4977e+00],
        [ 1.1341e+00,  1.6935e+00,  3.9521e-01,  4.1435e-01],
        [ 9.3683e-01,  3.7658e-01, -9.2956e-01, -1.4870e+00],
        [-1.9267e+00, -1.1021e+00,  7.4282e-01, -3.3229e-01],
        [-1.7913e+00,  1.6274e-01,  1.3894e+00,  9.5697e-02],
        [ 1.0449e+00,  1.4803e+00, -2.2399e+00,  2.1954e+00],
        [-1.8349e+00, -1.6576e+00,  1.5621e+00, -5.6326e-01],
        [ 2.2950e-01, -6.6362e-01, -1.6732e-01, -2.8443e-01],
        [-1.6103e-01, -8.1412e-01, -1.5430e+00,  7.9492e-01],
        [ 1.2041e+00, -1.0845e+00,  3.8837e-01, -7.7902e-01],
        [ 4.7827e-01, -1.7641e+00, -4.9121e-01,  9.3954e-01],
        [-7.1302e-01,  5.7801e-01, -1.1783e-01,  1.1443e+00],
        [-5.2274e-01,  1.4776e+00, -1.0581e+00,  4.2126e-01],
        [-9.0982e-01,  4.4148e-01,  5.2698e-01,  1.0804e-01],
        [-2.3179e+00, -1.9013e-01,  3.5717e-01, -1.2424e+00],
        [-8.2650e-01,  7.7781e-01,  8.6851e-01, -4.9198e-01],
        [ 1.5997e+00,  2.0195e+00,  2.1451e-01,  2.6038e-02],
        [-7.1597e-01, -6.5931e-01,  1.5297e-01,  5.8283e-01],
        [-1.2318e-01,  1.7289e+00, -1.6433e+00,  4.6767e-01],
        [-4.1029e-01, -5.7924e-01,  2.5850e-01, -1.1498e+00],
        [-6.5188e-01,  2.9491e+00, -1.0428e+00,  1.1110e+00],
        [-1.5661e+00, -8.2523e-01, -1.0149e+00, -1.2910e-01],
        [-7.0363e-01,  5.5829e-02, -3.8785e-01, -6.8412e-01],
        [-2.3589e-01, -8.9438e-01, -2.0495e-01, -4.4717e-01],
        [-2.4601e-01, -2.7093e+00,  2.9603e-01, -1.2749e+00],
        [ 1.2152e-01,  2.1536e+00, -6.5354e-01, -8.7705e-01],
        [-4.4340e-01,  2.9071e-01,  4.1781e-01,  6.8150e-01],
        [ 4.1313e-01,  5.5134e-02,  2.8539e-01, -2.5071e-01],
        [-1.2409e+00, -1.9794e+00,  8.7629e-01,  4.9914e-01],
        [ 1.3918e+00, -1.2065e+00,  3.6488e-01,  7.9920e-01],
        [-7.4527e-01, -2.6599e-01, -1.1833e+00,  1.4825e+00],
        [-8.8964e-01, -9.8513e-01,  2.2654e+00, -8.1898e-01],
        [-8.4931e-01, -1.3688e-02, -2.0549e+00,  1.3359e+00],
        [-9.5069e-01, -2.3383e-01,  1.3054e+00, -1.6423e+00],
        [ 9.4747e-01, -9.4711e-01,  8.2060e-01,  8.2311e-01],
        [ 2.7049e+00,  1.0347e+00,  1.0635e+00,  1.4016e+00],
        [-2.9290e+00, -1.1550e+00,  1.5674e-01,  1.1667e+00],
        [ 6.7571e-01,  1.2627e+00,  3.4410e-01, -1.5645e-02],
        [ 7.7400e-01,  9.9933e-01, -6.9940e-02,  4.1190e-01],
        [ 1.1418e+00,  5.0972e-01,  2.9882e-01, -9.0137e-01],
        [ 1.2421e+00,  1.0139e+00, -1.4264e+00, -2.2744e-01],
        [ 2.3063e+00, -3.4425e-01,  8.1208e-01,  5.9367e-01],
        [ 2.1644e-01,  1.8334e+00,  3.4490e-01, -2.4149e+00],
        [ 1.9077e+00, -2.7052e-01,  4.0147e-02, -7.9008e-01],
        [ 1.0489e+00, -9.3888e-02,  1.8182e+00, -7.7080e-01],
        [ 1.6466e+00,  6.7944e-01, -4.2819e-01,  2.2326e+00],
        [-6.2800e-01, -3.3169e-01,  2.4456e+00, -6.5741e-01],
        [-2.0368e+00, -8.8974e-01, -8.6844e-01, -8.4863e-01],
        [ 2.9050e-01, -9.3160e-01,  1.2590e+00, -1.0432e+00],
        [ 6.6585e-01,  1.0346e-04, -5.7617e-01,  1.1794e+00],
        [ 2.2772e-01,  1.7760e+00,  1.3110e+00, -1.0299e-02],
        [-9.6586e-01,  1.5876e+00, -1.0246e+00,  1.4940e+00],
        [ 6.3834e-01, -1.4709e+00,  1.3694e+00,  1.1534e-01],
        [-4.4972e-01, -2.2856e-01, -5.1660e-01,  1.2509e+00],
        [-6.1820e-01,  1.5218e+00, -3.3922e-01,  5.3906e-01],
        [-2.7316e+00, -6.4734e-01, -3.8023e-01, -1.5074e+00],
        [-5.7204e-01, -3.0412e-01, -3.5391e-01, -1.0328e+00],
        [-3.6084e-01,  8.3830e-01,  6.9786e-01,  6.9378e-01],
        [ 4.1640e-01, -6.2676e-01,  1.6862e-01,  2.3369e-01],
        [-6.5202e-01,  3.2978e-01,  1.5572e+00, -1.7236e+00],
        [-2.9170e-01,  9.8541e-01,  8.5646e-01,  2.0513e-01],
        [ 3.3177e-01, -1.2218e-01,  6.5729e-01,  4.4347e-01],
        [ 9.7934e-01, -1.1275e+00,  9.0332e-01, -3.9596e-01],
        [ 8.2975e-01, -8.7132e-01,  1.3503e+00, -2.5046e+00],
        [-2.9352e-02, -5.7366e-01, -9.4650e-03, -3.0499e-01],
        [-4.2991e-01,  2.5330e-01, -5.2220e-01,  3.0541e-01],
        [-6.4460e-01,  7.9563e-01,  7.1085e-01,  4.4284e-01],
        [-1.2644e-01, -2.7306e-01, -1.3925e+00, -6.2923e-01],
        [-1.0585e+00, -1.2693e-01,  8.8663e-02, -2.2462e-01],
        [-7.7637e-01,  1.0187e-01, -1.7175e-01,  1.2524e+00],
        [ 1.2016e+00, -2.6008e-01,  9.2748e-01, -2.4809e-01],
        [ 4.5790e-01, -7.2052e-01,  1.1045e+00,  1.1347e+00],
        [ 1.0849e+00,  2.9698e-02, -6.3573e-01,  1.3885e+00],
        [ 6.6372e-01,  9.8492e-01, -1.2754e+00, -9.1688e-01],
        [-4.1809e-01, -6.7380e-01,  1.4838e-01, -8.2432e-01],
        [ 1.2636e+00,  9.7854e-03, -2.8879e-01,  3.1589e-01],
        [ 1.2116e+00, -3.9023e-01, -5.2851e-01,  1.9893e+00],
        [-1.4959e+00, -1.4395e+00, -1.7226e-01,  1.0498e+00],
        [ 1.8828e-01,  3.8592e-01, -2.6074e-02, -1.7461e+00],
        [ 2.4937e-01, -2.5295e-01, -4.8333e-01,  7.7862e-01],
        [ 1.4127e+00, -9.7811e-01,  4.1214e-01,  3.8828e-01],
        [ 9.2703e-02, -2.1680e-01,  7.0553e-01,  7.8444e-01],
        [ 1.0206e+00,  1.4274e-01,  6.3721e-01, -8.9507e-01],
        [-1.3137e-01, -6.9252e-02, -1.4587e-01,  3.4252e-01],
        [-1.9972e-01,  1.2712e+00,  2.8607e-01, -2.0937e-01],
        [ 9.8252e-01, -6.5446e-01,  6.0600e-01,  6.1967e-01],
        [ 1.5729e-01, -3.3993e-01, -1.5224e+00, -8.2873e-01],
        [ 3.0420e-01,  1.0703e+00, -7.7062e-02, -4.2812e-01],
        [ 1.4747e+00, -2.8497e+00,  8.7404e-01,  7.7426e-01],
        [-1.5852e+00,  3.5255e-01, -6.2593e-01,  8.2279e-01],
        [ 2.4750e-01,  5.8881e-01, -1.5412e-01,  7.3387e-01],
        [ 1.1204e+00, -1.0482e+00, -8.5989e-01, -8.7111e-01],
        [-7.2771e-01,  9.9104e-01, -2.3141e-01,  1.3819e+00],
        [ 7.3018e-01,  3.3205e-01,  3.0716e-01, -3.7426e-01],
        [-7.0926e-01, -1.2803e+00, -1.0423e+00,  4.6467e-01],
        [ 4.5741e-01, -4.3823e-01,  5.4929e-01,  8.3431e-01],
        [ 5.2874e-01, -8.7362e-02,  9.5680e-01,  5.3162e-01],
        [ 2.3496e+00,  8.4476e-01,  4.0391e-01, -8.7919e-02],
        [ 1.5250e-02, -1.0890e-01, -1.3236e+00,  9.6474e-01],
        [ 1.1922e-01, -1.4293e+00,  4.8705e-01, -7.6754e-01],
        [-1.0935e+00, -3.9005e-01,  1.4350e+00, -1.1054e+00],
        [ 1.1271e+00,  1.0442e-01,  3.5794e-01, -8.6337e-01],
        [ 2.2689e-01, -1.1520e+00,  5.2672e-01, -4.1337e-01],
        [-1.7355e+00,  7.4338e-01, -2.3248e+00, -2.7039e-01],
        [-1.6798e-01,  5.4462e-01,  6.9749e-01,  1.9761e+00],
        [-3.6509e-01, -1.0327e+00, -6.1984e-01,  7.3896e-01],
        [-1.1553e+00, -6.6510e-01,  1.3334e+00, -5.5794e-02],
        [ 1.2346e+00,  1.1739e+00,  1.0924e+00, -9.8825e-01],
        [-1.1577e-01,  1.5384e-01, -2.4188e+00,  2.5167e+00],
        [ 2.6743e-01,  2.2145e-01, -5.5868e-01,  4.4082e-01],
        [ 1.3327e+00, -1.2344e+00, -5.1991e-01,  5.7349e-01],
        [-1.3054e+00,  2.5247e-01,  9.8259e-01,  7.7583e-01],
        [ 1.4239e+00, -2.4496e-01, -1.5369e+00, -3.3709e+00],
        [-3.5371e-02,  4.3562e-01,  1.6064e+00, -1.3444e+00],
        [ 4.6492e-01, -1.0306e+00,  1.2017e-01,  1.5349e+00],
        [ 1.5806e+00, -8.7479e-01, -2.5141e-01, -3.1173e-01],
        [ 4.4010e-01,  3.7667e-01,  1.4262e+00, -8.8262e-01],
        [-1.1072e+00,  1.2140e+00, -5.7822e-01,  8.6666e-01],
        [-1.1008e-01,  8.1962e-01, -1.8496e+00, -1.5796e-01],
        [ 2.6598e-01, -1.3017e+00,  1.4128e+00, -6.2400e-01],
        [ 1.2926e+00,  1.4190e+00, -8.2741e-01, -1.5132e-01],
        [ 1.3310e+00,  5.2501e-01,  1.0636e+00, -9.1884e-01],
        [ 6.6505e-01,  8.4839e-01,  8.6342e-01, -9.9147e-01],
        [ 4.0294e-01, -2.8860e-01,  5.5267e-01,  9.9719e-01],
        [ 9.3848e-01,  1.2862e+00, -1.6576e-01, -1.1530e+00],
        [-1.1100e+00,  2.1269e+00,  1.2424e+00, -1.3451e+00],
        [-8.7422e-02,  8.6884e-01,  2.0006e+00, -1.4669e-01],
        [-7.8708e-01, -6.5998e-01,  3.1507e-01, -6.0341e-01],
        [ 1.0862e+00, -1.0019e+00,  2.6920e+00,  4.7791e-01],
        [-9.6086e-01, -2.4008e-01,  1.5815e-01, -8.1872e-01],
        [ 2.6910e-01, -1.5410e+00, -2.2098e+00, -8.9089e-01],
        [ 2.2318e-01,  5.7760e-01, -6.4184e-02,  2.9341e-01],
        [ 7.2113e-01,  8.2423e-01,  4.5935e-01,  5.0567e-01],
        [ 1.3384e+00,  1.8098e+00,  2.0720e-01, -1.4231e+00],
        [-1.1367e-01,  1.5576e+00, -2.0237e+00, -5.4175e-01],
        [ 5.3915e-02,  1.1503e+00,  1.8495e+00,  1.0206e+00],
        [ 2.0134e+00, -9.4020e-01,  4.0485e-01, -1.3960e-01],
        [ 2.6788e-01, -5.9090e-01,  1.7391e+00,  2.7540e-02],
        [ 8.5989e-01,  5.1443e-01,  1.1093e+00, -1.7609e+00],
        [-2.9667e-01,  3.9402e-01, -1.3304e+00,  5.5826e-01],
        [ 1.0769e+00, -4.0288e-01, -1.8447e+00, -3.2750e-02],
        [ 1.0479e+00,  6.0611e-01,  5.4795e-01, -4.4605e-01]])
tensor([[-1.8644e+00,  3.7705e-01, -2.2484e+00, -9.1572e-01],
        [-7.5227e-02, -6.4833e-01,  2.2389e-01, -1.9301e+00],
        [-7.2877e-02,  6.1393e-01, -1.4578e+00, -4.4932e-01],
        [ 2.0236e-01,  6.6648e-01, -1.5158e+00, -1.3403e+00],
        [ 2.0154e+00, -5.3505e-01, -2.8988e-01, -2.8898e+00],
        [ 5.7398e-01,  3.2080e-01, -6.2162e-01,  1.0766e+00],
        [ 1.7119e-01,  1.0151e-02,  7.4923e-01, -1.3623e+00],
        [ 8.1280e-01, -2.6983e+00, -3.2547e-01,  1.9610e+00],
        [-6.2578e-01, -4.4760e-01,  1.1175e+00,  1.0244e+00],
        [-3.6685e-01,  5.6885e-01, -8.6066e-02,  1.4081e+00],
        [-1.4617e+00, -1.3206e+00, -1.1697e+00, -2.4966e-01],
        [ 1.0392e-01, -1.3095e-01, -1.9194e-02,  5.9289e-01],
        [ 2.4047e+00,  8.2419e-01,  1.1204e+00,  4.7142e-01],
        [ 2.9106e-01,  2.0292e+00,  8.3378e-01,  8.2533e-01],
        [ 1.3445e+00,  1.1337e+00,  2.2029e+00, -1.3936e+00],
        [ 7.3295e-01,  8.6069e-01, -3.7105e-01,  7.2891e-01],
        [ 8.0862e-01,  1.1455e-01, -2.2798e+00, -3.5772e-01],
        [ 2.2726e+00, -2.1767e-01,  1.9808e-01,  1.5563e+00],
        [-5.0635e-01, -4.7623e-01, -3.1239e-01, -8.9023e-01],
        [ 6.1406e-01, -1.2491e-01, -3.1142e+00,  5.5685e-01],
        [ 1.2654e-01, -8.5024e-01,  8.4774e-01, -6.8842e-01],
        [ 9.9799e-01,  1.0895e+00,  1.5443e+00,  1.1177e+00],
        [-1.1087e+00, -1.0797e-01, -1.2968e+00, -4.8157e-01],
        [-6.5682e-01,  6.9242e-01, -6.1162e-01,  1.1064e+00],
        [ 5.2305e-01,  9.8255e-01,  6.6593e-01, -1.4977e+00],
        [ 1.1341e+00,  1.6935e+00,  3.9521e-01,  4.1435e-01],
        [ 9.3683e-01,  3.7658e-01, -9.2956e-01, -1.4870e+00],
        [-1.9267e+00, -1.1021e+00,  7.4282e-01, -3.3229e-01],
        [-1.7913e+00,  1.6274e-01,  1.3894e+00,  9.5697e-02],
        [ 1.0449e+00,  1.4803e+00, -2.2399e+00,  2.1954e+00],
        [-1.8349e+00, -1.6576e+00,  1.5621e+00, -5.6326e-01],
        [ 2.2950e-01, -6.6362e-01, -1.6732e-01, -2.8443e-01],
        [-1.6103e-01, -8.1412e-01, -1.5430e+00,  7.9492e-01],
        [ 1.2041e+00, -1.0845e+00,  3.8837e-01, -7.7902e-01],
        [ 4.7827e-01, -1.7641e+00, -4.9121e-01,  9.3954e-01],
        [-7.1302e-01,  5.7801e-01, -1.1783e-01,  1.1443e+00],
        [-5.2274e-01,  1.4776e+00, -1.0581e+00,  4.2126e-01],
        [-9.0982e-01,  4.4148e-01,  5.2698e-01,  1.0804e-01],
        [-2.3179e+00, -1.9013e-01,  3.5717e-01, -1.2424e+00],
        [-8.2650e-01,  7.7781e-01,  8.6851e-01, -4.9198e-01],
        [ 1.5997e+00,  2.0195e+00,  2.1451e-01,  2.6038e-02],
        [-7.1597e-01, -6.5931e-01,  1.5297e-01,  5.8283e-01],
        [-1.2318e-01,  1.7289e+00, -1.6433e+00,  4.6767e-01],
        [-4.1029e-01, -5.7924e-01,  2.5850e-01, -1.1498e+00],
        [-6.5188e-01,  2.9491e+00, -1.0428e+00,  1.1110e+00],
        [-1.5661e+00, -8.2523e-01, -1.0149e+00, -1.2910e-01],
        [-7.0363e-01,  5.5829e-02, -3.8785e-01, -6.8412e-01],
        [-2.3589e-01, -8.9438e-01, -2.0495e-01, -4.4717e-01],
        [-2.4601e-01, -2.7093e+00,  2.9603e-01, -1.2749e+00],
        [ 1.2152e-01,  2.1536e+00, -6.5354e-01, -8.7705e-01],
        [-4.4340e-01,  2.9071e-01,  4.1781e-01,  6.8150e-01],
        [ 4.1313e-01,  5.5134e-02,  2.8539e-01, -2.5071e-01],
        [-1.2409e+00, -1.9794e+00,  8.7629e-01,  4.9914e-01],
        [ 1.3918e+00, -1.2065e+00,  3.6488e-01,  7.9920e-01],
        [-7.4527e-01, -2.6599e-01, -1.1833e+00,  1.4825e+00],
        [-8.8964e-01, -9.8513e-01,  2.2654e+00, -8.1898e-01],
        [-8.4931e-01, -1.3688e-02, -2.0549e+00,  1.3359e+00],
        [-9.5069e-01, -2.3383e-01,  1.3054e+00, -1.6423e+00],
        [ 9.4747e-01, -9.4711e-01,  8.2060e-01,  8.2311e-01],
        [ 2.7049e+00,  1.0347e+00,  1.0635e+00,  1.4016e+00],
        [-2.9290e+00, -1.1550e+00,  1.5674e-01,  1.1667e+00],
        [ 6.7571e-01,  1.2627e+00,  3.4410e-01, -1.5645e-02],
        [ 7.7400e-01,  9.9933e-01, -6.9940e-02,  4.1190e-01],
        [ 1.1418e+00,  5.0972e-01,  2.9882e-01, -9.0137e-01],
        [ 1.2421e+00,  1.0139e+00, -1.4264e+00, -2.2744e-01],
        [ 2.3063e+00, -3.4425e-01,  8.1208e-01,  5.9367e-01],
        [ 2.1644e-01,  1.8334e+00,  3.4490e-01, -2.4149e+00],
        [ 1.9077e+00, -2.7052e-01,  4.0147e-02, -7.9008e-01],
        [ 1.0489e+00, -9.3888e-02,  1.8182e+00, -7.7080e-01],
        [ 1.6466e+00,  6.7944e-01, -4.2819e-01,  2.2326e+00],
        [-6.2800e-01, -3.3169e-01,  2.4456e+00, -6.5741e-01],
        [-2.0368e+00, -8.8974e-01, -8.6844e-01, -8.4863e-01],
        [ 2.9050e-01, -9.3160e-01,  1.2590e+00, -1.0432e+00],
        [ 6.6585e-01,  1.0346e-04, -5.7617e-01,  1.1794e+00],
        [ 2.2772e-01,  1.7760e+00,  1.3110e+00, -1.0299e-02],
        [-9.6586e-01,  1.5876e+00, -1.0246e+00,  1.4940e+00],
        [ 6.3834e-01, -1.4709e+00,  1.3694e+00,  1.1534e-01],
        [-4.4972e-01, -2.2856e-01, -5.1660e-01,  1.2509e+00],
        [-6.1820e-01,  1.5218e+00, -3.3922e-01,  5.3906e-01],
        [-2.7316e+00, -6.4734e-01, -3.8023e-01, -1.5074e+00],
        [-5.7204e-01, -3.0412e-01, -3.5391e-01, -1.0328e+00],
        [-3.6084e-01,  8.3830e-01,  6.9786e-01,  6.9378e-01],
        [ 4.1640e-01, -6.2676e-01,  1.6862e-01,  2.3369e-01],
        [-6.5202e-01,  3.2978e-01,  1.5572e+00, -1.7236e+00],
        [-2.9170e-01,  9.8541e-01,  8.5646e-01,  2.0513e-01],
        [ 3.3177e-01, -1.2218e-01,  6.5729e-01,  4.4347e-01],
        [ 9.7934e-01, -1.1275e+00,  9.0332e-01, -3.9596e-01],
        [ 8.2975e-01, -8.7132e-01,  1.3503e+00, -2.5046e+00],
        [-2.9352e-02, -5.7366e-01, -9.4650e-03, -3.0499e-01],
        [-4.2991e-01,  2.5330e-01, -5.2220e-01,  3.0541e-01],
        [-6.4460e-01,  7.9563e-01,  7.1085e-01,  4.4284e-01],
        [-1.2644e-01, -2.7306e-01, -1.3925e+00, -6.2923e-01],
        [-1.0585e+00, -1.2693e-01,  8.8663e-02, -2.2462e-01],
        [-7.7637e-01,  1.0187e-01, -1.7175e-01,  1.2524e+00],
        [ 1.2016e+00, -2.6008e-01,  9.2748e-01, -2.4809e-01],
        [ 4.5790e-01, -7.2052e-01,  1.1045e+00,  1.1347e+00],
        [ 1.0849e+00,  2.9698e-02, -6.3573e-01,  1.3885e+00],
        [ 6.6372e-01,  9.8492e-01, -1.2754e+00, -9.1688e-01],
        [-4.1809e-01, -6.7380e-01,  1.4838e-01, -8.2432e-01],
        [ 1.2636e+00,  9.7854e-03, -2.8879e-01,  3.1589e-01],
        [ 1.2116e+00, -3.9023e-01, -5.2851e-01,  1.9893e+00],
        [-1.4959e+00, -1.4395e+00, -1.7226e-01,  1.0498e+00],
        [ 1.8828e-01,  3.8592e-01, -2.6074e-02, -1.7461e+00],
        [ 2.4937e-01, -2.5295e-01, -4.8333e-01,  7.7862e-01],
        [ 1.4127e+00, -9.7811e-01,  4.1214e-01,  3.8828e-01],
        [ 9.2703e-02, -2.1680e-01,  7.0553e-01,  7.8444e-01],
        [ 1.0206e+00,  1.4274e-01,  6.3721e-01, -8.9507e-01],
        [-1.3137e-01, -6.9252e-02, -1.4587e-01,  3.4252e-01],
        [-1.9972e-01,  1.2712e+00,  2.8607e-01, -2.0937e-01],
        [ 9.8252e-01, -6.5446e-01,  6.0600e-01,  6.1967e-01],
        [ 1.5729e-01, -3.3993e-01, -1.5224e+00, -8.2873e-01],
        [ 3.0420e-01,  1.0703e+00, -7.7062e-02, -4.2812e-01],
        [ 1.4747e+00, -2.8497e+00,  8.7404e-01,  7.7426e-01],
        [-1.5852e+00,  3.5255e-01, -6.2593e-01,  8.2279e-01],
        [ 2.4750e-01,  5.8881e-01, -1.5412e-01,  7.3387e-01],
        [ 1.1204e+00, -1.0482e+00, -8.5989e-01, -8.7111e-01],
        [-7.2771e-01,  9.9104e-01, -2.3141e-01,  1.3819e+00],
        [ 7.3018e-01,  3.3205e-01,  3.0716e-01, -3.7426e-01],
        [-7.0926e-01, -1.2803e+00, -1.0423e+00,  4.6467e-01],
        [ 4.5741e-01, -4.3823e-01,  5.4929e-01,  8.3431e-01],
        [ 5.2874e-01, -8.7362e-02,  9.5680e-01,  5.3162e-01],
        [ 2.3496e+00,  8.4476e-01,  4.0391e-01, -8.7919e-02],
        [ 1.5250e-02, -1.0890e-01, -1.3236e+00,  9.6474e-01],
        [ 1.1922e-01, -1.4293e+00,  4.8705e-01, -7.6754e-01],
        [-1.0935e+00, -3.9005e-01,  1.4350e+00, -1.1054e+00],
        [ 1.1271e+00,  1.0442e-01,  3.5794e-01, -8.6337e-01],
        [ 2.2689e-01, -1.1520e+00,  5.2672e-01, -4.1337e-01],
        [-1.7355e+00,  7.4338e-01, -2.3248e+00, -2.7039e-01],
        [-1.6798e-01,  5.4462e-01,  6.9749e-01,  1.9761e+00],
        [-3.6509e-01, -1.0327e+00, -6.1984e-01,  7.3896e-01],
        [-1.1553e+00, -6.6510e-01,  1.3334e+00, -5.5794e-02],
        [ 1.2346e+00,  1.1739e+00,  1.0924e+00, -9.8825e-01],
        [-1.1577e-01,  1.5384e-01, -2.4188e+00,  2.5167e+00],
        [ 2.6743e-01,  2.2145e-01, -5.5868e-01,  4.4082e-01],
        [ 1.3327e+00, -1.2344e+00, -5.1991e-01,  5.7349e-01],
        [-1.3054e+00,  2.5247e-01,  9.8259e-01,  7.7583e-01],
        [ 1.4239e+00, -2.4496e-01, -1.5369e+00, -3.3709e+00],
        [-3.5371e-02,  4.3562e-01,  1.6064e+00, -1.3444e+00],
        [ 4.6492e-01, -1.0306e+00,  1.2017e-01,  1.5349e+00],
        [ 1.5806e+00, -8.7479e-01, -2.5141e-01, -3.1173e-01],
        [ 4.4010e-01,  3.7667e-01,  1.4262e+00, -8.8262e-01],
        [-1.1072e+00,  1.2140e+00, -5.7822e-01,  8.6666e-01],
        [-1.1008e-01,  8.1962e-01, -1.8496e+00, -1.5796e-01],
        [ 2.6598e-01, -1.3017e+00,  1.4128e+00, -6.2400e-01],
        [ 1.2926e+00,  1.4190e+00, -8.2741e-01, -1.5132e-01],
        [ 1.3310e+00,  5.2501e-01,  1.0636e+00, -9.1884e-01],
        [ 6.6505e-01,  8.4839e-01,  8.6342e-01, -9.9147e-01],
        [ 4.0294e-01, -2.8860e-01,  5.5267e-01,  9.9719e-01],
        [ 9.3848e-01,  1.2862e+00, -1.6576e-01, -1.1530e+00],
        [-1.1100e+00,  2.1269e+00,  1.2424e+00, -1.3451e+00],
        [-8.7422e-02,  8.6884e-01,  2.0006e+00, -1.4669e-01],
        [-7.8708e-01, -6.5998e-01,  3.1507e-01, -6.0341e-01],
        [ 1.0862e+00, -1.0019e+00,  2.6920e+00,  4.7791e-01],
        [-9.6086e-01, -2.4008e-01,  1.5815e-01, -8.1872e-01],
        [ 2.6910e-01, -1.5410e+00, -2.2098e+00, -8.9089e-01],
        [ 2.2318e-01,  5.7760e-01, -6.4184e-02,  2.9341e-01],
        [ 7.2113e-01,  8.2423e-01,  4.5935e-01,  5.0567e-01],
        [ 1.3384e+00,  1.8098e+00,  2.0720e-01, -1.4231e+00],
        [-1.1367e-01,  1.5576e+00, -2.0237e+00, -5.4175e-01],
        [ 5.3915e-02,  1.1503e+00,  1.8495e+00,  1.0206e+00],
        [ 2.0134e+00, -9.4020e-01,  4.0485e-01, -1.3960e-01],
        [ 2.6788e-01, -5.9090e-01,  1.7391e+00,  2.7540e-02],
        [ 8.5989e-01,  5.1443e-01,  1.1093e+00, -1.7609e+00],
        [-2.9667e-01,  3.9402e-01, -1.3304e+00,  5.5826e-01],
        [ 1.0769e+00, -4.0288e-01, -1.8447e+00, -3.2750e-02],
        [ 1.0479e+00,  6.0611e-01,  5.4795e-01, -4.4605e-01]])
##########################################################
prot_encoder.conv1d.weight shape: torch.Size([300, 2500, 1])
tensor([[[ 0.0191],
         [ 0.0018],
         [-0.0087],
         ...,
         [-0.0136],
         [ 0.0021],
         [-0.0170]],

        [[-0.0159],
         [ 0.0072],
         [ 0.0196],
         ...,
         [-0.0002],
         [ 0.0152],
         [-0.0019]],

        [[ 0.0150],
         [ 0.0142],
         [ 0.0035],
         ...,
         [ 0.0167],
         [ 0.0073],
         [-0.0030]],

        ...,

        [[ 0.0187],
         [-0.0091],
         [ 0.0015],
         ...,
         [-0.0070],
         [ 0.0145],
         [ 0.0161]],

        [[ 0.0089],
         [ 0.0137],
         [ 0.0131],
         ...,
         [-0.0079],
         [-0.0157],
         [-0.0082]],

        [[ 0.0159],
         [-0.0110],
         [-0.0018],
         ...,
         [-0.0040],
         [-0.0009],
         [-0.0045]]])
tensor([[[ 0.0193],
         [ 0.0023],
         [-0.0074],
         ...,
         [-0.0135],
         [ 0.0022],
         [-0.0169]],

        [[-0.0160],
         [ 0.0069],
         [ 0.0185],
         ...,
         [-0.0001],
         [ 0.0153],
         [-0.0018]],

        [[ 0.0153],
         [ 0.0137],
         [ 0.0047],
         ...,
         [ 0.0170],
         [ 0.0077],
         [-0.0026]],

        ...,

        [[ 0.0186],
         [-0.0092],
         [ 0.0016],
         ...,
         [-0.0072],
         [ 0.0143],
         [ 0.0159]],

        [[ 0.0088],
         [ 0.0137],
         [ 0.0132],
         ...,
         [-0.0080],
         [-0.0157],
         [-0.0082]],

        [[ 0.0158],
         [-0.0109],
         [-0.0018],
         ...,
         [-0.0041],
         [-0.0011],
         [-0.0046]]])
##########################################################
prot_encoder.conv1d.bias shape: torch.Size([300])
tensor([-1.9772e-02,  6.0115e-03,  8.7698e-03, -1.4861e-02,  1.9905e-02,
        -1.2452e-02,  1.0567e-03, -1.2100e-02, -6.0875e-03,  1.4830e-02,
        -1.3165e-02, -1.9084e-02,  1.0388e-02,  1.7080e-02, -6.7183e-03,
         1.0697e-02, -9.8398e-03,  1.5933e-02,  1.1683e-02,  2.0624e-03,
         1.5563e-02, -4.6462e-03,  1.0680e-02, -1.3923e-02,  1.5123e-02,
        -1.3246e-03, -1.7115e-02, -3.0598e-03,  1.6173e-02,  4.5470e-03,
         6.0686e-03,  1.1600e-02,  5.8920e-03,  1.5993e-02,  3.2031e-03,
        -1.8402e-02, -8.5853e-03, -9.2943e-03, -1.4007e-03,  2.1113e-03,
         1.7083e-02,  8.5009e-03,  1.6494e-02, -4.0565e-03,  5.8800e-03,
         1.6200e-02, -2.7401e-03, -1.9612e-02,  9.1249e-03, -1.9403e-02,
        -1.6487e-02, -1.8857e-02,  1.1969e-02,  5.0918e-03, -1.2411e-02,
         8.6718e-03, -1.1766e-02, -4.9214e-03, -1.4054e-02,  1.0075e-02,
        -3.4666e-04, -2.7591e-03,  1.4243e-02,  6.2934e-03,  6.8556e-03,
         1.5884e-02, -8.3735e-03, -5.7955e-03, -1.1498e-02,  3.0092e-03,
        -1.6253e-02,  4.0308e-03,  1.8314e-02,  7.4644e-04, -1.8812e-02,
         1.4100e-02,  1.7858e-02,  1.6671e-02, -7.9575e-03,  1.0311e-02,
        -1.1424e-02, -1.4368e-02, -1.4230e-02, -6.2938e-03,  2.5608e-03,
         6.5225e-03,  1.6655e-02,  7.1881e-03,  1.7653e-03, -1.5524e-02,
        -3.4066e-03, -1.2227e-02, -1.4691e-02,  1.4601e-04, -1.2811e-02,
        -5.2004e-03, -1.1966e-02,  1.8536e-03,  8.5145e-03, -1.3272e-02,
         4.9370e-03, -9.5326e-03,  1.6933e-02, -7.8561e-03,  2.0911e-03,
        -2.2667e-04,  1.7852e-03,  8.0181e-03, -1.4453e-02, -1.8449e-02,
        -5.2987e-03,  1.0743e-03,  1.1486e-03,  6.6414e-03,  8.2205e-03,
         1.5940e-02, -6.1951e-03, -7.8010e-04, -2.3551e-03, -1.6992e-02,
         5.9553e-03, -1.3827e-02,  1.9529e-02, -7.1387e-03, -3.1645e-03,
         1.4242e-03,  6.3890e-03,  5.1927e-03, -1.7465e-02, -5.3778e-03,
         1.1317e-02,  1.8093e-02,  1.6890e-02, -1.9739e-02,  3.2106e-03,
        -4.3061e-03,  1.3513e-02, -1.8277e-02, -7.9510e-03,  1.4690e-02,
        -1.9011e-02,  3.6532e-03,  1.4033e-03,  2.3186e-03, -9.3111e-03,
        -9.9083e-03, -1.3843e-02, -1.3093e-02, -1.4562e-02,  2.7084e-03,
         1.7516e-02,  5.5819e-03,  5.0463e-03, -2.8653e-03, -1.2892e-02,
         1.7223e-02,  1.4774e-02, -3.8322e-03,  4.4938e-03, -1.1116e-02,
         5.4831e-03, -6.2732e-04, -8.1509e-03,  1.8852e-02,  1.2172e-02,
         1.8650e-02, -5.7396e-04,  1.4596e-03,  1.2936e-02, -1.2866e-02,
         1.1187e-02, -7.7438e-03, -9.0290e-03, -5.8007e-03, -1.9665e-02,
        -1.1874e-02, -1.5073e-02, -7.3012e-03,  1.1662e-02,  1.9025e-02,
        -1.8112e-02,  5.0616e-03, -7.6108e-03, -3.9121e-03,  9.9147e-04,
        -1.2057e-02, -1.2108e-02, -1.2494e-02, -6.3101e-03,  1.2531e-02,
        -9.8861e-03,  1.7985e-02,  1.0819e-02,  9.0091e-03, -1.2645e-02,
         1.4202e-02,  1.6284e-02, -1.4562e-02, -1.0187e-02, -6.4803e-03,
        -1.3731e-02, -9.3501e-03, -6.2905e-03, -1.2636e-02,  1.7233e-02,
        -1.4986e-02,  1.7376e-02, -6.3685e-03, -9.0762e-03,  1.6552e-02,
         8.7849e-03, -5.6434e-03,  1.3640e-02,  1.6112e-02, -1.8536e-02,
         7.3164e-03, -8.3119e-03, -7.9852e-03,  1.3441e-02, -9.4703e-03,
         9.4851e-03, -1.8871e-02, -1.1925e-02,  6.1647e-03,  9.2362e-03,
        -1.7784e-02,  1.6343e-02,  1.6861e-02, -1.5966e-02, -8.9882e-03,
         3.6478e-03, -1.2854e-02, -7.9935e-03, -1.1788e-02,  1.7752e-02,
        -3.1074e-03, -1.1809e-02,  1.4988e-02, -1.2325e-02, -1.5703e-02,
        -7.4238e-03,  9.4932e-04,  1.2566e-02,  1.7934e-02, -7.9495e-03,
        -1.3903e-02,  6.6798e-03, -1.6800e-02, -1.2639e-02, -1.6995e-02,
        -1.5981e-02, -1.0567e-02,  1.8538e-02,  8.3835e-03, -1.9796e-03,
        -8.6436e-03, -1.5972e-02, -1.9674e-02, -9.6519e-03,  1.5723e-02,
         8.3726e-06, -2.3792e-03,  1.1625e-02, -8.8379e-03,  1.6488e-02,
         6.9748e-03, -7.2832e-03, -1.8870e-02,  1.9469e-02,  2.8441e-03,
        -5.0720e-04,  2.4034e-03,  1.7086e-02, -5.6812e-03, -7.6737e-03,
         1.2190e-02, -7.1306e-03,  8.3241e-03,  4.8507e-03,  3.6083e-03,
         3.6363e-03, -9.6618e-04, -1.7333e-02,  1.1060e-02, -1.5915e-02,
         1.9937e-02, -3.8619e-03,  7.2283e-03,  1.4762e-02, -1.8511e-02,
        -5.7765e-03, -1.1813e-02, -4.7340e-03, -1.1121e-02,  1.1152e-02,
         1.0699e-03, -1.6899e-02,  1.2259e-03, -4.8683e-03, -6.6000e-03])
tensor([-0.0202,  0.0065,  0.0080, -0.0155,  0.0196, -0.0128,  0.0008, -0.0124,
        -0.0065,  0.0144, -0.0135, -0.0197,  0.0101,  0.0167, -0.0104,  0.0106,
        -0.0099,  0.0155,  0.0110,  0.0018,  0.0152, -0.0047,  0.0102, -0.0140,
         0.0141, -0.0016, -0.0173, -0.0034,  0.0160,  0.0042,  0.0054,  0.0113,
         0.0058,  0.0162,  0.0031, -0.0185, -0.0091, -0.0100, -0.0017,  0.0020,
         0.0154,  0.0087,  0.0167, -0.0044,  0.0057,  0.0162, -0.0028, -0.0198,
         0.0091, -0.0200, -0.0171, -0.0191,  0.0115,  0.0049, -0.0126,  0.0082,
        -0.0125, -0.0051, -0.0141,  0.0100, -0.0004, -0.0029,  0.0141,  0.0058,
         0.0071,  0.0153, -0.0087, -0.0060, -0.0117,  0.0029, -0.0168,  0.0040,
         0.0181,  0.0006, -0.0190,  0.0139,  0.0177,  0.0166, -0.0080,  0.0102,
        -0.0119, -0.0148, -0.0143, -0.0063,  0.0027,  0.0066,  0.0163,  0.0068,
         0.0016, -0.0156, -0.0034, -0.0120, -0.0149,  0.0002, -0.0128, -0.0052,
        -0.0119,  0.0017,  0.0084, -0.0134,  0.0047, -0.0095,  0.0170, -0.0079,
         0.0023, -0.0006,  0.0017,  0.0083, -0.0143, -0.0185, -0.0052,  0.0010,
         0.0011,  0.0067,  0.0083,  0.0162, -0.0062, -0.0007, -0.0023, -0.0170,
         0.0067, -0.0138,  0.0196, -0.0073, -0.0034,  0.0014,  0.0064,  0.0053,
        -0.0174, -0.0053,  0.0114,  0.0179,  0.0170, -0.0196,  0.0032, -0.0043,
         0.0136, -0.0182, -0.0079,  0.0147, -0.0185,  0.0037,  0.0020,  0.0025,
        -0.0094, -0.0097, -0.0140, -0.0130, -0.0146,  0.0028,  0.0180,  0.0057,
         0.0051, -0.0026, -0.0127,  0.0172,  0.0148, -0.0037,  0.0044, -0.0108,
         0.0055, -0.0006, -0.0080,  0.0189,  0.0124,  0.0185, -0.0002,  0.0016,
         0.0130, -0.0126,  0.0114, -0.0076, -0.0087, -0.0056, -0.0196, -0.0120,
        -0.0142, -0.0070,  0.0119,  0.0191, -0.0180,  0.0054, -0.0075, -0.0038,
         0.0013, -0.0120, -0.0119, -0.0124, -0.0063,  0.0126, -0.0097,  0.0182,
         0.0110,  0.0090, -0.0124,  0.0142,  0.0164, -0.0145, -0.0101, -0.0063,
        -0.0135, -0.0090, -0.0056, -0.0124,  0.0175, -0.0149,  0.0176, -0.0058,
        -0.0086,  0.0169,  0.0090, -0.0056,  0.0137,  0.0165, -0.0185,  0.0076,
        -0.0081, -0.0079,  0.0137, -0.0093,  0.0096, -0.0188, -0.0118,  0.0064,
         0.0094, -0.0178,  0.0166,  0.0169, -0.0154, -0.0088,  0.0039, -0.0128,
        -0.0078, -0.0117,  0.0178, -0.0029, -0.0117,  0.0153, -0.0117, -0.0155,
        -0.0072,  0.0011,  0.0129,  0.0182, -0.0078, -0.0138,  0.0069, -0.0165,
        -0.0123, -0.0168, -0.0159, -0.0104,  0.0187,  0.0086, -0.0018, -0.0085,
        -0.0159, -0.0197, -0.0095,  0.0158,  0.0002, -0.0024,  0.0118, -0.0088,
         0.0167,  0.0071, -0.0069, -0.0186,  0.0196,  0.0029, -0.0003,  0.0025,
         0.0173, -0.0054, -0.0076,  0.0124, -0.0069,  0.0085,  0.0050,  0.0038,
         0.0039, -0.0008, -0.0173,  0.0114, -0.0158,  0.0200, -0.0038,  0.0076,
         0.0148, -0.0184, -0.0056, -0.0117, -0.0043, -0.0109,  0.0118,  0.0012,
        -0.0167,  0.0014, -0.0048, -0.0064])
##########################################################
prot_encoder.ffn.dense1.weight shape: torch.Size([8, 4])
tensor([[-0.2595,  0.5051,  0.4208, -0.1773],
        [-0.6451,  0.3749, -0.4622, -0.6689],
        [ 0.2893, -0.1562, -0.3385,  0.0109],
        [ 0.3899, -0.2134, -0.0640, -0.6028],
        [-0.4284,  0.4793,  0.5274, -0.2522],
        [-0.1233,  0.2243,  0.6721,  0.4801],
        [ 0.2806,  0.6928, -0.5819, -0.6931],
        [ 0.2559,  0.4592,  0.2270, -0.2067]])
tensor([[-0.2596,  0.5042,  0.4212, -0.1794],
        [-0.6451,  0.3749, -0.4625, -0.6703],
        [ 0.2893, -0.1562, -0.3385,  0.0110],
        [ 0.3898, -0.2133, -0.0638, -0.6025],
        [-0.4303,  0.4824,  0.5281, -0.2294],
        [-0.1231,  0.2259,  0.6727,  0.4846],
        [ 0.2807,  0.6944, -0.5820, -0.6844],
        [ 0.2562,  0.4608,  0.2275, -0.1954]])
##########################################################
prot_encoder.ffn.dense1.bias shape: torch.Size([8])
tensor([-0.1938, -0.4735, -0.4429,  0.4917,  0.0044, -0.2706, -0.3153,  0.0069])
tensor([-1.9375e-01, -4.7269e-01, -4.4286e-01,  4.9331e-01, -2.7313e-04,
        -2.6553e-01, -3.2217e-01,  4.0089e-03])
##########################################################
prot_encoder.ffn.dense2.weight shape: torch.Size([4, 8])
tensor([[ 0.1159,  0.6992,  0.5949, -0.0106,  0.3784, -0.2145, -0.1068, -0.4406],
        [ 0.5769,  0.0803, -0.4426,  0.2126,  0.6769, -0.0268, -0.2089,  0.5921],
        [ 0.4749,  0.4871, -0.3743,  0.0376, -0.3888, -0.6629, -0.5407, -0.5208],
        [ 0.3850,  0.3893, -0.5248, -0.0867,  0.6009, -0.0220,  0.0464, -0.2891]])
tensor([[ 0.1156,  0.6977,  0.5949, -0.0197,  0.3775, -0.2053, -0.1080, -0.4405],
        [ 0.5768,  0.0783, -0.4426,  0.2079,  0.6767, -0.0251, -0.2113,  0.5913],
        [ 0.4757,  0.4922, -0.3743,  0.0570, -0.3858, -0.6680, -0.5350, -0.5182],
        [ 0.3846,  0.3877, -0.5248, -0.0923,  0.5989, -0.0277,  0.0444, -0.2910]])
##########################################################
prot_encoder.ffn.dense2.bias shape: torch.Size([4])
tensor([ 0.2476,  0.1800,  0.0157, -0.3371])
tensor([ 0.2685,  0.1903,  0.0091, -0.3617])
##########################################################
prot_encoder.addnorm.ln.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9939, 1.0022, 1.0152, 0.9908])
##########################################################
prot_encoder.addnorm.ln.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0070,  0.0022,  0.0039, -0.0070])
##########################################################
prot_encoder.encoder.layers.0.self_attn.in_proj_weight shape: torch.Size([12, 4])
tensor([[ 0.5527,  0.3823,  0.5969, -0.1098],
        [ 0.5175,  0.6029,  0.2267,  0.0285],
        [-0.5421,  0.2452, -0.5845, -0.2410],
        [ 0.3238,  0.3059, -0.4285,  0.5016],
        [ 0.3140,  0.2831, -0.4714,  0.0456],
        [-0.2320, -0.4402, -0.3595,  0.2621],
        [-0.3104, -0.0449,  0.1951,  0.3830],
        [-0.4469, -0.2422, -0.4460, -0.5756],
        [-0.5627,  0.4255,  0.2016,  0.4596],
        [-0.0958, -0.5457,  0.5949,  0.6099],
        [ 0.2996,  0.1461,  0.5725,  0.1430],
        [ 0.2338,  0.0208, -0.0393,  0.5297]])
tensor([[ 0.5532,  0.3832,  0.5962, -0.1105],
        [ 0.5172,  0.6044,  0.2241,  0.0299],
        [-0.5423,  0.2466, -0.5863, -0.2404],
        [ 0.3238,  0.3054, -0.4279,  0.5016],
        [ 0.3144,  0.2844, -0.4711,  0.0435],
        [-0.2332, -0.4386, -0.3607,  0.2628],
        [-0.3117, -0.0447,  0.1923,  0.3869],
        [-0.4483, -0.2418, -0.4493, -0.5714],
        [-0.5573,  0.4342,  0.1939,  0.4532],
        [-0.0983, -0.5500,  0.6004,  0.6113],
        [ 0.2996,  0.1430,  0.5756,  0.1430],
        [ 0.2366,  0.0252, -0.0416,  0.5249]])
##########################################################
prot_encoder.encoder.layers.0.self_attn.in_proj_bias shape: torch.Size([12])
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
tensor([ 1.1181e-03,  1.9035e-03,  1.3168e-03, -5.0788e-04, -3.5609e-12,
         5.1483e-12,  1.7328e-12, -7.1774e-12,  1.3507e-02, -6.2513e-03,
        -3.7664e-03,  6.0965e-03])
##########################################################
prot_encoder.encoder.layers.0.self_attn.out_proj.weight shape: torch.Size([4, 4])
tensor([[ 0.2631,  0.0741, -0.2957,  0.0992],
        [-0.4073,  0.3560, -0.4970,  0.0380],
        [ 0.2161, -0.2285,  0.1019,  0.3542],
        [-0.4660,  0.2238,  0.4130, -0.2437]])
tensor([[ 0.2584,  0.0623, -0.2992,  0.0987],
        [-0.4060,  0.3638, -0.4960,  0.0397],
        [ 0.2157, -0.2371,  0.1023,  0.3508],
        [-0.4622,  0.2359,  0.4149, -0.2416]])
##########################################################
prot_encoder.encoder.layers.0.self_attn.out_proj.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0131, -0.0083,  0.0068, -0.0113])
##########################################################
prot_encoder.encoder.layers.0.linear1.weight shape: torch.Size([8, 4])
tensor([[ 0.1318,  0.5187,  0.1760, -0.6335],
        [ 0.0492,  0.2571, -0.4628,  0.0970],
        [ 0.5035,  0.4059,  0.7047,  0.0336],
        [ 0.4072,  0.3400, -0.4915,  0.5108],
        [-0.5770, -0.1752, -0.1294,  0.3934],
        [-0.0380, -0.4433, -0.4424, -0.2544],
        [-0.1285, -0.3815, -0.4237, -0.5319],
        [ 0.2669, -0.6886,  0.4605,  0.0496]])
tensor([[ 0.1428,  0.5236,  0.1794, -0.6524],
        [ 0.0427,  0.2628, -0.4786,  0.1132],
        [ 0.4983,  0.4015,  0.7009,  0.0466],
        [ 0.4117,  0.3298, -0.4688,  0.4942],
        [-0.5787, -0.1700, -0.1336,  0.3942],
        [-0.0543, -0.4529, -0.4516, -0.2202],
        [-0.1381, -0.3878, -0.4266, -0.5136],
        [ 0.2642, -0.6891,  0.4589,  0.0543]])
##########################################################
prot_encoder.encoder.layers.0.linear1.bias shape: torch.Size([8])
tensor([-0.0768,  0.1680, -0.3049, -0.0418,  0.2326,  0.4874, -0.2523, -0.0361])
tensor([-0.0657,  0.1756, -0.3130, -0.0588,  0.2355,  0.4710, -0.2638, -0.0392])
##########################################################
prot_encoder.encoder.layers.0.linear2.weight shape: torch.Size([4, 8])
tensor([[ 0.2977,  0.6523,  0.5517, -0.5641, -0.1183, -0.0901, -0.3620,  0.4830],
        [-0.4046,  0.6546,  0.3120, -0.0409,  0.5266,  0.6920,  0.3945,  0.6134],
        [ 0.2655, -0.1036, -0.4346,  0.6975, -0.1400, -0.6380, -0.2058,  0.2141],
        [ 0.4184,  0.2933, -0.0246, -0.3148, -0.5123,  0.2741, -0.1904,  0.0743]])
tensor([[ 0.3054,  0.6749,  0.5531, -0.5402, -0.1021, -0.0760, -0.3609,  0.4833],
        [-0.4282,  0.6520,  0.3037, -0.0422,  0.5255,  0.6823,  0.3920,  0.6118],
        [ 0.2807, -0.1141, -0.4292,  0.6837, -0.1500, -0.6405, -0.2048,  0.2143],
        [ 0.4180,  0.2837, -0.0234, -0.3235, -0.5171,  0.2721, -0.1899,  0.0754]])
##########################################################
prot_encoder.encoder.layers.0.linear2.bias shape: torch.Size([4])
tensor([-0.1466, -0.2029, -0.1826,  0.2733])
tensor([-0.1222, -0.2206, -0.1835,  0.2669])
##########################################################
prot_encoder.encoder.layers.0.norm1.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([1.0021, 0.9982, 1.0182, 0.9822])
##########################################################
prot_encoder.encoder.layers.0.norm1.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0158, -0.0054,  0.0103, -0.0108])
##########################################################
prot_encoder.encoder.layers.0.norm2.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9953, 1.0033, 1.0210, 0.9885])
##########################################################
prot_encoder.encoder.layers.0.norm2.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0273, -0.0063, -0.0086, -0.0022])
##########################################################
prot_encoder.encoder.layers.1.self_attn.in_proj_weight shape: torch.Size([12, 4])
tensor([[ 0.5527,  0.3823,  0.5969, -0.1098],
        [ 0.5175,  0.6029,  0.2267,  0.0285],
        [-0.5421,  0.2452, -0.5845, -0.2410],
        [ 0.3238,  0.3059, -0.4285,  0.5016],
        [ 0.3140,  0.2831, -0.4714,  0.0456],
        [-0.2320, -0.4402, -0.3595,  0.2621],
        [-0.3104, -0.0449,  0.1951,  0.3830],
        [-0.4469, -0.2422, -0.4460, -0.5756],
        [-0.5627,  0.4255,  0.2016,  0.4596],
        [-0.0958, -0.5457,  0.5949,  0.6099],
        [ 0.2996,  0.1461,  0.5725,  0.1430],
        [ 0.2338,  0.0208, -0.0393,  0.5297]])
tensor([[ 0.5547,  0.3834,  0.5954, -0.1113],
        [ 0.5190,  0.6065,  0.2227,  0.0273],
        [-0.5440,  0.2473, -0.5860, -0.2397],
        [ 0.3236,  0.3061, -0.4286,  0.5018],
        [ 0.3139,  0.2841, -0.4725,  0.0456],
        [-0.2380, -0.4357, -0.3631,  0.2671],
        [-0.3145, -0.0415,  0.1913,  0.3875],
        [-0.4494, -0.2401, -0.4484, -0.5728],
        [-0.5495,  0.4365,  0.1872,  0.4498],
        [-0.0999, -0.5489,  0.5998,  0.6124],
        [ 0.3039,  0.1390,  0.5785,  0.1399],
        [ 0.2359,  0.0211, -0.0397,  0.5278]])
##########################################################
prot_encoder.encoder.layers.1.self_attn.in_proj_bias shape: torch.Size([12])
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
tensor([ 1.9948e-03,  3.9976e-03,  7.0356e-04, -2.3808e-05,  7.9182e-12,
        -1.3121e-12,  7.3929e-13,  7.3418e-12,  1.8954e-02, -5.4120e-03,
        -4.7042e-03,  1.7232e-03])
##########################################################
prot_encoder.encoder.layers.1.self_attn.out_proj.weight shape: torch.Size([4, 4])
tensor([[ 0.2631,  0.0741, -0.2957,  0.0992],
        [-0.4073,  0.3560, -0.4970,  0.0380],
        [ 0.2161, -0.2285,  0.1019,  0.3542],
        [-0.4660,  0.2238,  0.4130, -0.2437]])
tensor([[ 0.2458,  0.0342, -0.3072,  0.0987],
        [-0.3986,  0.3799, -0.4934,  0.0401],
        [ 0.2194, -0.2204,  0.1070,  0.3526],
        [-0.4606,  0.2317,  0.4157, -0.2437]])
##########################################################
prot_encoder.encoder.layers.1.self_attn.out_proj.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0355, -0.0201, -0.0080, -0.0075])
##########################################################
prot_encoder.encoder.layers.1.linear1.weight shape: torch.Size([8, 4])
tensor([[-0.3228,  0.2621, -0.3039,  0.5701],
        [ 0.6511,  0.0627,  0.5487, -0.6285],
        [-0.2111, -0.5096, -0.5550,  0.5106],
        [-0.2574, -0.6647,  0.6974, -0.3919],
        [-0.5164, -0.5250,  0.6348,  0.5295],
        [-0.1194,  0.2407, -0.0896,  0.6753],
        [-0.5689, -0.6430,  0.3907, -0.0310],
        [ 0.5148,  0.3381, -0.3501,  0.3471]])
tensor([[-0.3280,  0.2728, -0.3097,  0.5706],
        [ 0.6667,  0.0651,  0.5434, -0.6411],
        [-0.2110, -0.5104, -0.5546,  0.5110],
        [-0.2552, -0.6656,  0.6977, -0.3936],
        [-0.5167, -0.5251,  0.6350,  0.5297],
        [-0.1168,  0.2444, -0.0958,  0.6752],
        [-0.5684, -0.6431,  0.3904, -0.0310],
        [ 0.4999,  0.3364, -0.3438,  0.3575]])
##########################################################
prot_encoder.encoder.layers.1.linear1.bias shape: torch.Size([8])
tensor([-0.0768,  0.1680, -0.3049, -0.0418,  0.2326,  0.4874, -0.2523, -0.0361])
tensor([-0.0704,  0.1789, -0.3052, -0.0405,  0.2327,  0.4911, -0.2526, -0.0481])
##########################################################
prot_encoder.encoder.layers.1.linear2.weight shape: torch.Size([4, 8])
tensor([[-0.1954,  0.5433, -0.0591, -0.5300,  0.1084,  0.4821,  0.0626, -0.4114],
        [ 0.5332, -0.4579, -0.0772, -0.4919,  0.2535, -0.4953, -0.5570,  0.0491],
        [-0.1065, -0.0673, -0.3026,  0.3119, -0.3920,  0.5886, -0.0194, -0.1129],
        [-0.6921,  0.1553,  0.6517, -0.2078,  0.4096,  0.3282, -0.0776, -0.6015]])
tensor([[-0.1635,  0.5422, -0.0552, -0.5313,  0.1084,  0.5203,  0.0623, -0.3886],
        [ 0.5324, -0.4841, -0.0784, -0.4937,  0.2539, -0.4964, -0.5567,  0.0402],
        [-0.1225, -0.0712, -0.3050,  0.3119, -0.3927,  0.5695, -0.0196, -0.1229],
        [-0.7071,  0.1870,  0.6515, -0.2044,  0.4100,  0.3102, -0.0774, -0.6051]])
##########################################################
prot_encoder.encoder.layers.1.linear2.bias shape: torch.Size([4])
tensor([-0.1466, -0.2029, -0.1826,  0.2733])
tensor([-0.1152, -0.2197, -0.1991,  0.2757])
##########################################################
prot_encoder.encoder.layers.1.norm1.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9872, 1.0028, 1.0141, 0.9879])
##########################################################
prot_encoder.encoder.layers.1.norm1.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0299, -0.0185, -0.0076, -0.0034])
##########################################################
prot_encoder.encoder.layers.1.norm2.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9830, 1.0247, 1.0137, 0.9972])
##########################################################
prot_encoder.encoder.layers.1.norm2.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0285, -0.0290, -0.0194,  0.0051])
##########################################################
cross_encoder.encoder.layers.0.self_attn.in_proj_weight shape: torch.Size([12, 4])
tensor([[-0.3205,  0.2992,  0.4699,  0.0244],
        [-0.5547,  0.5446,  0.4113,  0.2785],
        [ 0.0665,  0.3468, -0.4311, -0.5670],
        [ 0.3199, -0.1701,  0.5869, -0.4535],
        [-0.2620, -0.1830, -0.4982, -0.3046],
        [ 0.4864, -0.4457,  0.5817, -0.3822],
        [ 0.1839,  0.4277, -0.3642, -0.2345],
        [-0.4031,  0.5161,  0.1089, -0.3021],
        [-0.2896,  0.5343, -0.5748,  0.6082],
        [ 0.4475, -0.1433, -0.2965, -0.0284],
        [-0.3975,  0.5971,  0.2064, -0.3666],
        [ 0.2717,  0.3438,  0.3000, -0.3095]])
tensor([[-0.3210,  0.3036,  0.4666,  0.0236],
        [-0.5511,  0.5079,  0.4376,  0.2868],
        [ 0.0663,  0.3447, -0.4289, -0.5668],
        [ 0.3192, -0.1754,  0.5914, -0.4519],
        [-0.2625, -0.1826, -0.4975, -0.3054],
        [ 0.4803, -0.4400,  0.5797, -0.3799],
        [ 0.1885,  0.4222, -0.3634, -0.2342],
        [-0.4051,  0.5194,  0.1082, -0.3028],
        [-0.2641,  0.5362, -0.6007,  0.6060],
        [ 0.4479, -0.1423, -0.2997, -0.0267],
        [-0.4253,  0.5974,  0.2307, -0.3625],
        [ 0.2784,  0.3399,  0.2998, -0.3122]])
##########################################################
cross_encoder.encoder.layers.0.self_attn.in_proj_bias shape: torch.Size([12])
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
tensor([ 2.7837e-03, -2.1931e-02, -1.9590e-03, -4.1390e-03, -3.2677e-12,
         9.2042e-12, -1.0868e-11,  7.4012e-12,  2.9107e-02,  1.3317e-03,
        -2.3896e-02,  1.0842e-03])
##########################################################
cross_encoder.encoder.layers.0.self_attn.out_proj.weight shape: torch.Size([4, 4])
tensor([[ 0.2658, -0.4054, -0.1638, -0.1512],
        [-0.3371, -0.4381,  0.2994, -0.0489],
        [-0.2823, -0.1476, -0.4634,  0.3841],
        [-0.3704, -0.2141,  0.0610, -0.2801]])
tensor([[ 0.2799, -0.3762, -0.1836, -0.1502],
        [-0.3638, -0.4617,  0.3215, -0.0492],
        [-0.2851, -0.1389, -0.4740,  0.3860],
        [-0.3548, -0.2278,  0.0689, -0.2827]])
##########################################################
cross_encoder.encoder.layers.0.self_attn.out_proj.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0464, -0.0389,  0.0064, -0.0130])
##########################################################
cross_encoder.encoder.layers.0.linear1.weight shape: torch.Size([8, 4])
tensor([[-0.0090, -0.6387, -0.4092, -0.2291],
        [ 0.3761,  0.4344, -0.6251, -0.5518],
        [-0.5434,  0.1041, -0.0706, -0.4230],
        [-0.4610, -0.0804, -0.4452,  0.3843],
        [ 0.3020, -0.2592, -0.2067, -0.2268],
        [ 0.4160, -0.3125, -0.2495, -0.3089],
        [ 0.6664, -0.0599,  0.3583,  0.0029],
        [-0.0847,  0.5386, -0.0506, -0.6339]])
tensor([[-0.0027, -0.6450, -0.4074, -0.2308],
        [ 0.3890,  0.4473, -0.6436, -0.5592],
        [-0.5507,  0.0981, -0.0658, -0.4145],
        [-0.4608, -0.0892, -0.4349,  0.3827],
        [ 0.3582, -0.2888, -0.2246, -0.2349],
        [ 0.4021, -0.2861, -0.2545, -0.3166],
        [ 0.6804, -0.0641,  0.3504,  0.0010],
        [-0.0914,  0.5530, -0.0581, -0.6341]])
##########################################################
cross_encoder.encoder.layers.0.linear1.bias shape: torch.Size([8])
tensor([-0.4269, -0.2766,  0.2449,  0.0400, -0.4325, -0.1937, -0.0784, -0.1399])
tensor([-0.4240, -0.2592,  0.2381,  0.0313, -0.3921, -0.2005, -0.0691, -0.1337])
##########################################################
cross_encoder.encoder.layers.0.linear2.weight shape: torch.Size([4, 8])
tensor([[-0.5856,  0.6155,  0.1933, -0.3306, -0.5923,  0.6453,  0.4315,  0.2194],
        [ 0.2764,  0.0209,  0.5411,  0.1428, -0.6744, -0.1395, -0.2251, -0.0883],
        [-0.5436,  0.4401, -0.5607,  0.2901,  0.6704,  0.6194, -0.2718, -0.5245],
        [ 0.0809,  0.6225, -0.0184, -0.3428,  0.5514, -0.4514,  0.5983, -0.3960]])
tensor([[-0.5921,  0.6404,  0.2090, -0.3215, -0.5989,  0.6346,  0.4170,  0.2422],
        [ 0.2725, -0.0185,  0.5316,  0.1326, -0.6807, -0.1546, -0.2363, -0.1087],
        [-0.5417,  0.4502, -0.5600,  0.2899,  0.6738,  0.6264, -0.2651, -0.5193],
        [ 0.0894,  0.6253, -0.0260, -0.3415,  0.5609, -0.4326,  0.6173, -0.4050]])
##########################################################
cross_encoder.encoder.layers.0.linear2.bias shape: torch.Size([4])
tensor([-0.3487, -0.0492,  0.1871, -0.3421])
tensor([-0.3388, -0.0883,  0.1982, -0.3250])
##########################################################
cross_encoder.encoder.layers.0.norm1.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([1.0073, 1.0031, 1.0056, 1.0112])
##########################################################
cross_encoder.encoder.layers.0.norm1.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0416, -0.0394, -0.0009, -0.0049])
##########################################################
cross_encoder.encoder.layers.0.norm2.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9761, 0.9758, 0.9953, 1.0028])
##########################################################
cross_encoder.encoder.layers.0.norm2.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0065, -0.0452,  0.0211,  0.0250])
##########################################################
cross_encoder.encoder.layers.1.self_attn.in_proj_weight shape: torch.Size([12, 4])
tensor([[-0.3205,  0.2992,  0.4699,  0.0244],
        [-0.5547,  0.5446,  0.4113,  0.2785],
        [ 0.0665,  0.3468, -0.4311, -0.5670],
        [ 0.3199, -0.1701,  0.5869, -0.4535],
        [-0.2620, -0.1830, -0.4982, -0.3046],
        [ 0.4864, -0.4457,  0.5817, -0.3822],
        [ 0.1839,  0.4277, -0.3642, -0.2345],
        [-0.4031,  0.5161,  0.1089, -0.3021],
        [-0.2896,  0.5343, -0.5748,  0.6082],
        [ 0.4475, -0.1433, -0.2965, -0.0284],
        [-0.3975,  0.5971,  0.2064, -0.3666],
        [ 0.2717,  0.3438,  0.3000, -0.3095]])
tensor([[-0.3217,  0.3033,  0.4681,  0.0232],
        [-0.5409,  0.5118,  0.4230,  0.2863],
        [ 0.0620,  0.3449, -0.4277, -0.5640],
        [ 0.3088, -0.1734,  0.5940, -0.4462],
        [-0.2700, -0.1772, -0.4997, -0.3010],
        [ 0.4705, -0.4330,  0.5781, -0.3756],
        [ 0.1930,  0.4162, -0.3609, -0.2352],
        [-0.4006,  0.5138,  0.1093, -0.3028],
        [-0.2881,  0.5396, -0.5762,  0.6027],
        [ 0.4498, -0.1452, -0.3018, -0.0235],
        [-0.4181,  0.6060,  0.2152, -0.3637],
        [ 0.2819,  0.3338,  0.3007, -0.3104]])
##########################################################
cross_encoder.encoder.layers.1.self_attn.in_proj_bias shape: torch.Size([12])
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
tensor([-8.7045e-05,  4.9400e-03, -4.1166e-03, -9.5076e-03,  1.2229e-11,
        -9.9910e-12, -1.9017e-12,  4.2492e-12, -8.5833e-04,  1.0636e-02,
        -2.2682e-02,  9.3400e-03])
##########################################################
cross_encoder.encoder.layers.1.self_attn.out_proj.weight shape: torch.Size([4, 4])
tensor([[ 0.2658, -0.4054, -0.1638, -0.1512],
        [-0.3371, -0.4381,  0.2994, -0.0489],
        [-0.2823, -0.1476, -0.4634,  0.3841],
        [-0.3704, -0.2141,  0.0610, -0.2801]])
tensor([[ 0.2661, -0.4058, -0.1449, -0.1466],
        [-0.3570, -0.4475,  0.3194, -0.0499],
        [-0.2822, -0.1391, -0.4892,  0.3836],
        [-0.3509, -0.2126,  0.0478, -0.2832]])
##########################################################
cross_encoder.encoder.layers.1.self_attn.out_proj.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([-0.0042, -0.0352,  0.0273,  0.0123])
##########################################################
cross_encoder.encoder.layers.1.linear1.weight shape: torch.Size([8, 4])
tensor([[-0.0231, -0.4494,  0.2539, -0.1756],
        [ 0.6733,  0.3882, -0.0188,  0.5300],
        [-0.3164, -0.3759,  0.5851, -0.0916],
        [ 0.0564,  0.3581,  0.3009, -0.6213],
        [ 0.0510, -0.4562, -0.6548,  0.6204],
        [ 0.2447,  0.3814, -0.1890,  0.4773],
        [-0.5157, -0.4118, -0.0515, -0.3095],
        [-0.4995, -0.4194, -0.4699, -0.2317]])
tensor([[-0.0392, -0.4240,  0.2424, -0.1731],
        [ 0.6698,  0.3848, -0.0157,  0.5338],
        [-0.3146, -0.3783,  0.5853, -0.0913],
        [ 0.0596,  0.3592,  0.2996, -0.6243],
        [ 0.0496, -0.4507, -0.6563,  0.6175],
        [ 0.2485,  0.3972, -0.1984,  0.4671],
        [-0.5205, -0.4014, -0.0563, -0.3103],
        [-0.5001, -0.4200, -0.4700, -0.2304]])
##########################################################
cross_encoder.encoder.layers.1.linear1.bias shape: torch.Size([8])
tensor([-0.4269, -0.2766,  0.2449,  0.0400, -0.4325, -0.1937, -0.0784, -0.1399])
tensor([-0.4439, -0.2794,  0.2464,  0.0432, -0.4365, -0.1827, -0.0847, -0.1387])
##########################################################
cross_encoder.encoder.layers.1.linear2.weight shape: torch.Size([4, 8])
tensor([[ 0.4668,  0.1220,  0.1958, -0.4172, -0.2954,  0.4446, -0.0866,  0.5784],
        [-0.4560,  0.2696,  0.0860, -0.4371,  0.1141, -0.3042, -0.1591,  0.2612],
        [-0.3835,  0.0840,  0.1036, -0.3258,  0.5403, -0.3892, -0.4368,  0.1604],
        [-0.1261,  0.3291,  0.2061, -0.4622, -0.4308, -0.3835, -0.4468,  0.6741]])
tensor([[ 0.4556,  0.1239,  0.1694, -0.4061, -0.2990,  0.4472, -0.0886,  0.5784],
        [-0.4603,  0.2623,  0.0774, -0.4542,  0.1116, -0.3067, -0.1591,  0.2611],
        [-0.3779,  0.0882,  0.1155, -0.3120,  0.5405, -0.3890, -0.4363,  0.1602],
        [-0.1163,  0.3304,  0.2292, -0.4696, -0.4250, -0.3839, -0.4452,  0.6743]])
##########################################################
cross_encoder.encoder.layers.1.linear2.bias shape: torch.Size([4])
tensor([-0.3487, -0.0492,  0.1871, -0.3421])
tensor([-0.3633, -0.0930,  0.2201, -0.3164])
##########################################################
cross_encoder.encoder.layers.1.norm1.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9759, 1.0021, 0.9990, 1.0062])
##########################################################
cross_encoder.encoder.layers.1.norm1.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([-0.0112, -0.0287,  0.0310,  0.0294])
##########################################################
cross_encoder.encoder.layers.1.norm2.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9932, 0.9511, 1.0309, 1.0358])
##########################################################
cross_encoder.encoder.layers.1.norm2.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([-0.0107, -0.0583,  0.0344,  0.0192])
##########################################################
ffn.dense1.weight shape: torch.Size([8, 4])
tensor([[-0.1917, -0.6609,  0.6140,  0.3482],
        [-0.0506,  0.3242, -0.0929, -0.5616],
        [-0.0826,  0.6521, -0.6863, -0.6714],
        [-0.4650,  0.5750, -0.3633, -0.6041],
        [ 0.3097,  0.4116, -0.1153, -0.2726],
        [-0.1625,  0.3127,  0.5290, -0.0353],
        [-0.2748,  0.3890,  0.3001, -0.0161],
        [-0.6841,  0.1792, -0.3927,  0.1578]])
tensor([[-0.2215, -0.6863,  0.6123,  0.3489],
        [-0.0440,  0.3208, -0.0934, -0.5581],
        [-0.0695,  0.6478, -0.6852, -0.6672],
        [-0.4361,  0.5630, -0.3575, -0.5919],
        [ 0.3000,  0.4301, -0.1164, -0.2827],
        [-0.1848,  0.3243,  0.5296, -0.0440],
        [-0.2958,  0.3857,  0.3023, -0.0219],
        [-0.7103,  0.1863, -0.4002,  0.1447]])
##########################################################
ffn.dense1.bias shape: torch.Size([8])
tensor([-0.3898, -0.2104,  0.3745,  0.3323,  0.2801, -0.1773, -0.3162,  0.0766])
tensor([-0.3671, -0.2153,  0.3603,  0.3074,  0.2934, -0.1606, -0.3063,  0.0977])
##########################################################
ffn.dense2.weight shape: torch.Size([4, 8])
tensor([[ 0.3198,  0.0296, -0.5080, -0.6820,  0.5185,  0.6167,  0.0051,  0.3861],
        [-0.5735, -0.0469, -0.2004,  0.1610,  0.2425, -0.0362, -0.4731,  0.1319],
        [-0.2685,  0.6184, -0.0033, -0.5722, -0.0615,  0.5181, -0.4260, -0.5312],
        [ 0.6505,  0.1862, -0.4162,  0.0264, -0.5054,  0.0392,  0.5402, -0.2520]])
tensor([[ 0.3228,  0.0436, -0.4550, -0.6183,  0.5379,  0.6285,  0.0176,  0.4325],
        [-0.5839, -0.0520, -0.2221,  0.1296,  0.2355, -0.0412, -0.4787,  0.0993],
        [-0.2709,  0.6152, -0.0152, -0.5877, -0.0689,  0.5141, -0.4295, -0.5441],
        [ 0.6602,  0.1806, -0.4356,  0.0096, -0.5104,  0.0363,  0.5368, -0.2529]])
##########################################################
ffn.dense2.bias shape: torch.Size([4])
tensor([ 0.0549,  0.1911, -0.1800, -0.1351])
tensor([ 0.1049,  0.1549, -0.2008, -0.1282])
##########################################################
addnorm.ln.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9935, 0.9520, 1.0236, 1.0096])
##########################################################
addnorm.ln.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0587, -0.0485, -0.0255,  0.0095])
##########################################################
decoder.embedding.weight shape: torch.Size([166, 4])
tensor([[-1.5392e-01,  1.9662e+00,  4.3708e-03,  4.6573e-01],
        [-1.7420e+00,  5.7167e-01, -7.2964e-01,  8.6548e-01],
        [-1.4423e+00,  4.9880e-01, -1.4067e+00, -9.4982e-01],
        [-1.5804e+00,  9.9604e-01,  8.2687e-02,  3.1327e-02],
        [-8.5599e-01, -9.7858e-01, -8.7915e-01,  5.9235e-01],
        [-7.9338e-01,  1.7069e-01,  5.7934e-01, -1.8099e+00],
        [-8.4444e-01, -6.1350e-01,  7.3280e-01,  3.3407e-01],
        [-1.7005e-01,  1.8939e+00,  1.1374e+00, -3.5013e-01],
        [ 4.7906e-02, -8.1229e-01,  7.5094e-01,  6.2083e-02],
        [ 1.3869e+00, -1.4589e+00,  2.2381e+00,  1.1269e+00],
        [ 1.0663e+00, -1.0777e+00,  3.0711e-01,  1.5358e+00],
        [-3.9336e-01, -1.7731e+00,  1.2241e+00,  1.1723e+00],
        [-1.8673e+00, -6.3706e-01,  1.0058e+00, -3.4125e-01],
        [-2.1788e-01, -3.9622e-01,  4.7977e-01, -1.2163e+00],
        [ 8.4247e-02,  3.1319e-01,  9.4604e-02,  4.0956e-01],
        [ 8.9041e-01, -2.7695e-01, -1.7065e-02, -6.7316e-01],
        [-1.4093e+00,  1.5023e+00,  1.0472e+00,  1.4315e+00],
        [-3.1592e+00,  1.3946e+00, -3.1014e-01, -1.8966e+00],
        [-1.3549e+00, -4.4924e-01,  3.3733e-01,  1.8688e+00],
        [-1.3100e-01, -1.7671e+00, -7.9573e-01, -9.7740e-01],
        [ 2.8317e+00, -3.0714e-01,  4.7946e-01,  1.8622e-01],
        [-6.3006e-01, -1.4878e+00, -1.1230e+00, -4.6429e-01],
        [ 7.9661e-01, -4.4602e-01,  1.2112e+00, -5.4672e-01],
        [-7.3975e-01, -1.1587e+00, -5.9145e-01, -4.7713e-01],
        [ 2.0568e-01,  1.0658e+00,  1.2742e+00,  5.3206e-01],
        [-3.0540e-01, -1.5433e-01, -6.8920e-01, -2.1767e+00],
        [-1.2807e-01,  8.9538e-01,  7.6957e-01, -1.2055e+00],
        [-5.0273e-02, -2.7437e+00, -2.2919e-01,  2.2300e+00],
        [ 1.0206e+00, -8.8699e-01,  2.1829e-01, -2.4754e+00],
        [-1.1456e+00,  4.9544e-01, -7.6020e-01,  1.7582e+00],
        [ 1.9514e+00,  9.9331e-01,  5.1073e-01,  6.6998e-01],
        [-6.2022e-01, -1.7383e-01, -8.3510e-01, -7.6423e-01],
        [ 6.1552e-01, -5.1951e-01, -7.9440e-01, -2.9282e-01],
        [-2.3441e+00, -2.0079e+00, -1.1470e+00, -7.9693e-01],
        [ 2.4394e-01,  3.1138e-01, -2.4606e+00,  1.8286e+00],
        [ 1.4949e-01,  9.3121e-01, -3.1019e-01, -3.5599e-01],
        [ 1.1570e+00,  9.0901e-01,  1.1268e+00, -1.4424e+00],
        [-2.2206e+00,  9.8483e-01, -4.7273e-01, -7.4309e-01],
        [ 2.3977e+00,  8.4480e-01,  2.2556e-01, -3.3063e-01],
        [ 5.8554e-01,  1.1719e+00,  2.5326e-01, -2.5272e-01],
        [-7.0813e-01,  2.9051e-01,  6.5103e-02, -1.1485e+00],
        [ 2.1476e+00,  1.8632e-01,  1.0363e+00, -1.0962e+00],
        [-8.9137e-01, -8.1675e-01, -6.2349e-01, -1.3240e+00],
        [ 5.2533e-01, -2.9093e+00, -2.3698e-01,  1.9100e-01],
        [-5.8208e-01, -8.6826e-01, -1.2316e+00, -4.5587e-01],
        [-1.2003e+00, -6.5162e-02, -2.0790e+00,  2.0642e+00],
        [-7.8421e-02, -1.5758e-01,  3.3443e-01,  1.0978e+00],
        [-5.5477e-01, -2.4942e+00, -1.7647e+00, -7.9525e-01],
        [-1.1875e+00,  2.7399e-01,  3.8756e-01, -2.6692e-01],
        [ 1.5209e+00,  3.5683e-01,  1.7367e-01, -1.0739e+00],
        [-5.8794e-02, -4.9426e-01,  1.3842e+00,  1.0118e+00],
        [ 1.4139e-01, -1.9980e+00, -5.4124e-01, -1.2875e+00],
        [ 1.3431e+00, -3.7617e-02, -2.6470e-01, -1.0442e+00],
        [ 3.4384e-01, -8.3028e-01, -1.8128e+00,  2.4155e-01],
        [ 1.1185e+00, -9.8457e-01,  6.7422e-01,  8.3727e-02],
        [-7.2421e-01, -6.0241e-01,  2.5696e-01,  1.1371e+00],
        [ 1.0345e+00,  6.4444e-01,  6.0515e-01,  8.7155e-01],
        [-2.1006e-01, -1.1464e+00, -6.2344e-01, -1.3588e+00],
        [-4.8806e-01, -1.0766e+00, -4.2053e-01, -5.5896e-01],
        [ 8.1056e-01, -1.0882e-01, -2.1843e-01, -1.1087e-01],
        [-1.5032e+00,  4.9076e-01,  1.3029e+00,  9.1388e-01],
        [ 1.1107e+00,  2.4167e+00,  7.3302e-01,  2.2673e-01],
        [-1.7477e-01,  1.5168e+00,  3.0933e-01,  2.8237e-01],
        [-7.7626e-01, -5.3729e-01, -1.5570e+00, -4.2412e-01],
        [-7.3826e-01,  1.6292e+00, -5.0544e-01, -5.3105e-01],
        [-9.6787e-01, -4.3219e-01, -1.5666e-02, -3.5728e-01],
        [-5.3961e-01,  8.2847e-01,  4.7212e-01,  1.1541e-01],
        [ 9.9415e-01,  1.1851e+00, -3.8951e-01,  6.4922e-01],
        [-2.8398e-01, -6.1927e-02,  1.1465e+00, -3.7438e+00],
        [-3.6633e-01, -1.0449e-01, -8.4789e-01, -2.6725e-01],
        [-5.8977e-01,  1.3443e+00,  5.4248e-01, -4.1783e-01],
        [ 1.3148e-01, -1.3058e+00,  1.5693e+00,  6.4657e-01],
        [ 1.6530e+00, -1.8419e+00, -3.9345e-01,  1.0455e+00],
        [ 1.6941e-01, -9.2381e-01, -4.7130e-01,  5.7015e-01],
        [-3.3147e-01, -1.1534e+00,  3.3407e-01,  8.2707e-01],
        [-2.0050e-01,  9.0196e-01,  1.0239e+00,  1.8918e-01],
        [ 5.4229e-02,  4.8889e-01,  2.0082e+00, -1.8028e+00],
        [-1.4794e-04, -7.9274e-02, -2.0011e+00, -3.6843e-01],
        [ 9.3899e-01, -1.3793e-01,  8.4789e-01,  6.0332e-01],
        [ 1.1894e+00, -2.9423e-01, -4.6866e-01,  6.4708e-01],
        [ 4.3837e-02, -1.6080e+00,  6.9132e-01, -2.7989e-01],
        [-5.8145e-01,  1.7656e-02,  2.1355e-01,  5.0129e-01],
        [-9.2855e-01,  1.5972e+00,  1.3117e+00,  1.1913e+00],
        [ 1.5772e-01,  4.4464e-01,  1.0621e+00,  1.3261e+00],
        [ 1.0101e+00, -9.6826e-01,  1.6814e+00, -1.5408e+00],
        [-1.5737e+00, -1.7045e+00, -1.2265e+00,  1.1482e+00],
        [ 1.7279e-01,  2.2247e+00, -1.8721e-02,  2.0352e+00],
        [-1.0499e+00,  1.4431e+00,  3.7134e-01, -9.8630e-01],
        [-5.9187e-01,  8.8200e-01, -1.8290e-01, -3.4429e-01],
        [-1.4329e+00, -1.0651e+00, -1.2295e+00,  1.2898e-01],
        [ 6.5135e-01,  3.2472e-01,  1.7932e+00,  4.6064e-01],
        [-4.2511e-01, -8.6620e-01,  6.0938e-01, -2.4286e-02],
        [-1.4662e+00, -1.7068e+00, -2.9605e-01,  1.2336e+00],
        [ 3.3360e-01,  3.7894e-01, -3.3400e-01, -4.1011e-01],
        [-1.6748e-01, -1.1723e+00,  1.3700e+00, -9.4080e-01],
        [ 1.0334e+00, -7.3776e-01,  3.2427e-01, -3.6565e-01],
        [ 5.0398e-01, -1.2589e+00,  3.4629e-01,  6.3578e-01],
        [ 3.8585e-01,  2.7086e-01,  2.2098e+00, -2.6068e-01],
        [-4.3929e-01, -1.0938e+00, -2.2015e-01,  5.2407e-01],
        [ 2.2601e-01,  4.0817e-01, -6.1140e-02,  1.0352e+00],
        [ 2.3247e-01, -2.2606e-01,  2.3874e-01,  1.7199e-02],
        [-1.0174e-01,  5.0448e-02,  1.9853e-01, -1.8731e+00],
        [ 1.7391e-01,  1.2458e+00, -1.0766e-01, -1.2248e+00],
        [-3.5533e-01, -1.6808e-01, -1.1412e+00,  7.1269e-01],
        [-6.9693e-01,  7.8807e-01,  3.1614e-01,  7.7400e-02],
        [-1.9536e-01,  7.5869e-01, -7.3229e-01, -7.6526e-01],
        [ 6.8120e-01,  2.2733e-01,  8.7259e-01, -3.6916e-01],
        [ 1.3006e+00, -1.9969e-01, -1.8331e-01, -1.5358e+00],
        [-3.4676e-01,  1.3720e+00,  1.0028e+00,  1.3385e-01],
        [-8.1109e-01,  5.0241e-01,  2.4442e-01,  7.5847e-01],
        [-3.7558e-01, -1.6850e+00,  1.6945e-01,  2.8210e-01],
        [-1.5557e-01, -3.6290e-01,  4.5142e-01, -7.4237e-01],
        [-1.2758e-01, -1.6926e+00, -1.3792e+00,  6.9151e-01],
        [ 9.6324e-01, -5.9090e-01, -1.2283e+00, -7.8357e-01],
        [-1.2220e+00,  5.2213e-02, -1.7397e-01, -9.2345e-02],
        [ 1.0484e+00, -1.1463e+00,  3.4998e-01,  8.3753e-01],
        [ 8.1789e-02,  9.5826e-01,  7.7649e-01, -2.6257e-01],
        [-6.9679e-02,  6.4037e-03,  1.3177e+00,  9.1501e-01],
        [-2.1275e-01, -5.3035e-01,  1.1046e+00,  3.2402e-02],
        [ 6.4636e-01, -1.0889e+00, -7.7646e-01,  4.7527e-01],
        [ 1.1618e+00,  9.6405e-01, -1.1495e+00,  7.7067e-01],
        [-3.4952e-01,  2.7744e-01,  1.1159e+00,  6.4222e-01],
        [ 6.2871e-02, -1.9411e+00,  4.4737e-01,  4.6805e-01],
        [ 2.9175e-01, -3.9926e-01,  6.5958e-01, -5.0707e-01],
        [-1.7783e-02,  1.1574e-02,  1.8969e-01,  4.5146e-01],
        [-3.4307e-02,  3.4565e-01, -1.2275e+00,  9.2217e-01],
        [ 1.6530e+00,  2.0672e-01, -6.7376e-02,  8.6134e-01],
        [-3.8905e-01,  7.9001e-02,  1.1250e+00, -4.2255e-02],
        [-1.9534e+00, -3.2582e-01,  4.6218e-01,  7.6578e-01],
        [-5.3962e-02,  1.0782e+00, -5.8165e-01, -1.5110e+00],
        [ 2.2513e-01, -1.8506e+00,  2.1627e-01,  1.5796e-01],
        [-3.5042e-01, -1.1156e+00, -4.1149e-01, -1.1972e+00],
        [ 5.4956e-02, -1.1903e+00,  1.5247e+00,  5.7367e-01],
        [ 6.4823e-01,  9.2296e-01,  1.0682e+00, -1.4511e-01],
        [ 6.9507e-01, -2.3051e+00, -1.2791e-01,  1.2213e+00],
        [ 2.1970e-01, -1.7096e+00,  3.0163e+00, -1.8997e+00],
        [-1.2174e+00,  1.1350e+00, -1.2366e+00,  2.5243e-01],
        [ 1.2752e-01, -3.9800e-01,  7.5292e-01,  3.3198e-01],
        [ 1.4245e+00,  1.0030e+00,  2.9553e-01,  5.6809e-01],
        [-7.8438e-01, -3.2947e-01,  7.3929e-01, -9.1393e-01],
        [ 5.7730e-01, -8.3587e-02, -4.4516e-01, -1.1011e+00],
        [-6.9741e-01, -1.0918e+00, -1.0541e+00,  9.6540e-01],
        [-4.9679e-01, -6.9698e-01, -1.4118e+00, -1.5802e+00],
        [-1.6078e+00, -6.4802e-01, -1.8449e+00,  4.0479e-01],
        [-7.6589e-01,  1.2140e-01, -2.5117e-01, -1.3329e+00],
        [ 1.0760e+00, -3.4363e-01,  1.6881e+00,  2.9187e-01],
        [ 1.1773e+00,  2.1496e-03, -5.3378e-01, -4.9525e-01],
        [-1.1638e+00, -8.2471e-02, -9.6908e-01,  1.6762e+00],
        [-1.2539e-01,  6.8477e-01, -4.0020e-01, -1.7274e+00],
        [-8.3801e-01, -4.3201e-01, -9.6859e-01,  6.0071e-01],
        [-5.3626e-01,  1.1974e+00,  6.8208e-02, -8.8043e-01],
        [ 1.3004e+00, -3.0042e-01, -1.2143e+00, -5.5225e-01],
        [ 1.0056e+00, -8.4292e-01, -3.7514e-01,  1.4745e-01],
        [-5.8301e-01,  3.6208e-01,  1.1567e+00,  5.2850e-01],
        [ 1.9126e-01,  2.2353e+00,  1.4875e+00, -3.4010e-01],
        [-1.5007e+00, -8.7576e-01, -9.0034e-02,  1.3316e+00],
        [ 7.8691e-01, -4.4966e-01, -6.0609e-01, -2.0613e+00],
        [ 3.4731e-01,  6.2353e-01, -3.0427e-01,  1.1764e+00],
        [ 2.9571e-01, -1.5750e+00, -5.0543e-01, -1.3431e+00],
        [-2.3209e-02, -1.1014e+00, -1.1234e+00,  3.5477e-01],
        [ 3.4956e-02,  5.2186e-01, -1.8759e-01, -6.9594e-01],
        [ 9.6249e-01, -7.3714e-01, -3.3516e-01,  4.1438e-02],
        [-2.1502e+00,  2.6645e-01, -4.0042e-01,  8.8200e-03],
        [-1.5723e-02, -1.3450e+00, -4.6489e-01, -4.1890e-01],
        [ 3.9053e-01, -5.5635e-01, -1.4833e-01, -7.3757e-01],
        [ 3.6689e-02,  1.2173e+00, -1.0056e+00, -1.0719e+00]])
tensor([[-1.5392e-01,  1.9662e+00,  4.3708e-03,  4.6573e-01],
        [-1.7420e+00,  5.7167e-01, -7.2964e-01,  8.6548e-01],
        [-1.4297e+00,  5.0614e-01, -1.3919e+00, -9.5692e-01],
        [-1.5804e+00,  9.9604e-01,  8.2687e-02,  3.1327e-02],
        [-8.3593e-01, -9.9193e-01, -9.0729e-01,  6.0338e-01],
        [-8.0960e-01,  2.0196e-01,  5.9760e-01, -1.8219e+00],
        [-8.4426e-01, -6.2510e-01,  6.8726e-01,  3.6434e-01],
        [-1.4448e-01,  1.8828e+00,  1.1590e+00, -3.3844e-01],
        [ 1.2026e-03, -8.0712e-01,  6.9623e-01,  1.0862e-01],
        [ 1.4072e+00, -1.4901e+00,  2.2521e+00,  1.0868e+00],
        [ 1.0722e+00, -1.1046e+00,  3.1102e-01,  1.5451e+00],
        [-4.2169e-01, -1.7374e+00,  1.1869e+00,  1.1963e+00],
        [-1.8590e+00, -7.0337e-01,  9.6907e-01, -2.3985e-01],
        [-2.0383e-01, -3.4357e-01,  4.8673e-01, -1.2724e+00],
        [ 9.3728e-02,  2.5238e-01,  9.1081e-02,  4.5439e-01],
        [ 8.9398e-01, -2.6911e-01, -9.0267e-03, -6.7679e-01],
        [-1.4171e+00,  1.5497e+00,  1.0683e+00,  1.4002e+00],
        [-3.1541e+00,  1.4297e+00, -2.8543e-01, -1.8999e+00],
        [-1.3570e+00, -4.4857e-01,  3.3815e-01,  1.8692e+00],
        [-1.2637e-01, -1.7712e+00, -7.9892e-01, -9.8287e-01],
        [ 2.8065e+00, -3.1078e-01,  4.9116e-01,  1.7141e-01],
        [-6.2023e-01, -1.4931e+00, -1.1274e+00, -4.6564e-01],
        [ 7.9693e-01, -4.4790e-01,  1.2085e+00, -5.4833e-01],
        [-7.3968e-01, -1.1596e+00, -5.9168e-01, -4.7512e-01],
        [ 2.0461e-01,  1.0691e+00,  1.2770e+00,  5.2907e-01],
        [-3.0488e-01, -1.5292e-01, -6.8878e-01, -2.1755e+00],
        [-1.2822e-01,  8.9725e-01,  7.7187e-01, -1.2069e+00],
        [-4.7115e-02, -2.7483e+00, -2.2826e-01,  2.2231e+00],
        [ 1.0194e+00, -8.8659e-01,  2.1958e-01, -2.4732e+00],
        [-1.1457e+00,  4.9540e-01, -7.6005e-01,  1.7583e+00],
        [ 1.9444e+00,  9.9198e-01,  5.1357e-01,  6.6517e-01],
        [-6.1977e-01, -1.7684e-01, -8.3575e-01, -7.6087e-01],
        [ 6.1597e-01, -5.1931e-01, -7.9547e-01, -2.9223e-01],
        [-2.3441e+00, -2.0078e+00, -1.1471e+00, -7.9691e-01],
        [ 2.4319e-01,  3.1137e-01, -2.4602e+00,  1.8279e+00],
        [ 1.4951e-01,  9.3148e-01, -3.1064e-01, -3.5618e-01],
        [ 1.1572e+00,  9.0872e-01,  1.1265e+00, -1.4423e+00],
        [-2.2195e+00,  9.8495e-01, -4.7237e-01, -7.4393e-01],
        [ 2.3974e+00,  8.4444e-01,  2.2554e-01, -3.3061e-01],
        [ 5.8552e-01,  1.1719e+00,  2.5326e-01, -2.5270e-01],
        [-7.0825e-01,  2.9061e-01,  6.5109e-02, -1.1485e+00],
        [ 2.1476e+00,  1.8624e-01,  1.0362e+00, -1.0961e+00],
        [-8.9129e-01, -8.1687e-01, -6.2344e-01, -1.3236e+00],
        [ 5.2533e-01, -2.9093e+00, -2.3713e-01,  1.9107e-01],
        [-5.8283e-01, -8.6770e-01, -1.2317e+00, -4.5622e-01],
        [-1.2003e+00, -6.5162e-02, -2.0790e+00,  2.0642e+00],
        [-7.8571e-02, -1.5739e-01,  3.3449e-01,  1.0977e+00],
        [-5.5477e-01, -2.4942e+00, -1.7647e+00, -7.9525e-01],
        [-1.1875e+00,  2.7400e-01,  3.8755e-01, -2.6694e-01],
        [ 1.5209e+00,  3.5683e-01,  1.7367e-01, -1.0739e+00],
        [-5.8794e-02, -4.9426e-01,  1.3842e+00,  1.0118e+00],
        [ 1.4139e-01, -1.9980e+00, -5.4124e-01, -1.2875e+00],
        [ 1.3431e+00, -3.7574e-02, -2.6469e-01, -1.0442e+00],
        [ 3.4381e-01, -8.3029e-01, -1.8128e+00,  2.4152e-01],
        [ 1.1185e+00, -9.8457e-01,  6.7422e-01,  8.3727e-02],
        [-7.2420e-01, -6.0242e-01,  2.5697e-01,  1.1371e+00],
        [ 1.0345e+00,  6.4444e-01,  6.0515e-01,  8.7155e-01],
        [-2.1006e-01, -1.1464e+00, -6.2344e-01, -1.3588e+00],
        [-4.8806e-01, -1.0766e+00, -4.2053e-01, -5.5896e-01],
        [ 8.1056e-01, -1.0882e-01, -2.1843e-01, -1.1087e-01],
        [-1.5032e+00,  4.9076e-01,  1.3029e+00,  9.1388e-01],
        [ 1.1107e+00,  2.4167e+00,  7.3302e-01,  2.2673e-01],
        [-1.7477e-01,  1.5168e+00,  3.0933e-01,  2.8237e-01],
        [-7.7626e-01, -5.3702e-01, -1.5569e+00, -4.2440e-01],
        [-7.3826e-01,  1.6292e+00, -5.0544e-01, -5.3105e-01],
        [-9.6787e-01, -4.3219e-01, -1.5666e-02, -3.5728e-01],
        [-5.3959e-01,  8.2846e-01,  4.7212e-01,  1.1541e-01],
        [ 9.9415e-01,  1.1851e+00, -3.8951e-01,  6.4922e-01],
        [-2.8401e-01, -6.1903e-02,  1.1465e+00, -3.7439e+00],
        [-3.6633e-01, -1.0449e-01, -8.4789e-01, -2.6725e-01],
        [-5.8977e-01,  1.3443e+00,  5.4248e-01, -4.1783e-01],
        [ 1.3148e-01, -1.3058e+00,  1.5693e+00,  6.4657e-01],
        [ 1.6530e+00, -1.8419e+00, -3.9345e-01,  1.0455e+00],
        [ 1.6941e-01, -9.2381e-01, -4.7130e-01,  5.7015e-01],
        [-3.3147e-01, -1.1534e+00,  3.3407e-01,  8.2707e-01],
        [-2.0050e-01,  9.0196e-01,  1.0239e+00,  1.8918e-01],
        [ 5.4229e-02,  4.8889e-01,  2.0082e+00, -1.8028e+00],
        [-1.4794e-04, -7.9274e-02, -2.0011e+00, -3.6843e-01],
        [ 9.3899e-01, -1.3793e-01,  8.4789e-01,  6.0332e-01],
        [ 1.1894e+00, -2.9423e-01, -4.6866e-01,  6.4708e-01],
        [ 4.3837e-02, -1.6080e+00,  6.9132e-01, -2.7989e-01],
        [-5.8145e-01,  1.7656e-02,  2.1355e-01,  5.0129e-01],
        [-9.2855e-01,  1.5972e+00,  1.3117e+00,  1.1913e+00],
        [ 1.5772e-01,  4.4464e-01,  1.0621e+00,  1.3261e+00],
        [ 1.0101e+00, -9.6826e-01,  1.6814e+00, -1.5408e+00],
        [-1.5737e+00, -1.7045e+00, -1.2265e+00,  1.1482e+00],
        [ 1.7279e-01,  2.2247e+00, -1.8721e-02,  2.0352e+00],
        [-1.0499e+00,  1.4431e+00,  3.7134e-01, -9.8630e-01],
        [-5.9187e-01,  8.8200e-01, -1.8290e-01, -3.4429e-01],
        [-1.4329e+00, -1.0651e+00, -1.2295e+00,  1.2898e-01],
        [ 6.5135e-01,  3.2472e-01,  1.7932e+00,  4.6064e-01],
        [-4.2511e-01, -8.6620e-01,  6.0938e-01, -2.4286e-02],
        [-1.4662e+00, -1.7068e+00, -2.9605e-01,  1.2336e+00],
        [ 3.3360e-01,  3.7894e-01, -3.3400e-01, -4.1011e-01],
        [-1.6748e-01, -1.1723e+00,  1.3700e+00, -9.4080e-01],
        [ 1.0334e+00, -7.3776e-01,  3.2427e-01, -3.6565e-01],
        [ 5.0398e-01, -1.2589e+00,  3.4629e-01,  6.3578e-01],
        [ 3.8585e-01,  2.7086e-01,  2.2098e+00, -2.6068e-01],
        [-4.3929e-01, -1.0938e+00, -2.2015e-01,  5.2407e-01],
        [ 2.2601e-01,  4.0817e-01, -6.1140e-02,  1.0352e+00],
        [ 2.3247e-01, -2.2606e-01,  2.3874e-01,  1.7199e-02],
        [-1.0174e-01,  5.0448e-02,  1.9853e-01, -1.8731e+00],
        [ 1.7391e-01,  1.2458e+00, -1.0766e-01, -1.2248e+00],
        [-3.5533e-01, -1.6808e-01, -1.1412e+00,  7.1269e-01],
        [-6.9693e-01,  7.8807e-01,  3.1614e-01,  7.7400e-02],
        [-1.9536e-01,  7.5869e-01, -7.3229e-01, -7.6526e-01],
        [ 6.8120e-01,  2.2733e-01,  8.7259e-01, -3.6916e-01],
        [ 1.3006e+00, -1.9969e-01, -1.8331e-01, -1.5358e+00],
        [-3.4676e-01,  1.3720e+00,  1.0028e+00,  1.3385e-01],
        [-8.1109e-01,  5.0241e-01,  2.4442e-01,  7.5847e-01],
        [-3.7558e-01, -1.6850e+00,  1.6945e-01,  2.8210e-01],
        [-1.5557e-01, -3.6290e-01,  4.5142e-01, -7.4237e-01],
        [-1.2758e-01, -1.6926e+00, -1.3792e+00,  6.9151e-01],
        [ 9.6324e-01, -5.9090e-01, -1.2283e+00, -7.8357e-01],
        [-1.2220e+00,  5.2213e-02, -1.7397e-01, -9.2345e-02],
        [ 1.0484e+00, -1.1463e+00,  3.4998e-01,  8.3753e-01],
        [ 8.1789e-02,  9.5826e-01,  7.7649e-01, -2.6257e-01],
        [-6.9679e-02,  6.4037e-03,  1.3177e+00,  9.1501e-01],
        [-2.1275e-01, -5.3035e-01,  1.1046e+00,  3.2402e-02],
        [ 6.4636e-01, -1.0889e+00, -7.7646e-01,  4.7527e-01],
        [ 1.1618e+00,  9.6405e-01, -1.1495e+00,  7.7067e-01],
        [-3.4952e-01,  2.7744e-01,  1.1159e+00,  6.4222e-01],
        [ 6.2871e-02, -1.9411e+00,  4.4737e-01,  4.6805e-01],
        [ 2.9175e-01, -3.9926e-01,  6.5958e-01, -5.0707e-01],
        [-1.7783e-02,  1.1574e-02,  1.8969e-01,  4.5146e-01],
        [-3.4307e-02,  3.4565e-01, -1.2275e+00,  9.2217e-01],
        [ 1.6530e+00,  2.0672e-01, -6.7376e-02,  8.6134e-01],
        [-3.8905e-01,  7.9001e-02,  1.1250e+00, -4.2255e-02],
        [-1.9534e+00, -3.2582e-01,  4.6218e-01,  7.6578e-01],
        [-5.3962e-02,  1.0782e+00, -5.8165e-01, -1.5110e+00],
        [ 2.2513e-01, -1.8506e+00,  2.1627e-01,  1.5796e-01],
        [-3.5042e-01, -1.1156e+00, -4.1149e-01, -1.1972e+00],
        [ 5.4956e-02, -1.1903e+00,  1.5247e+00,  5.7367e-01],
        [ 6.4823e-01,  9.2296e-01,  1.0682e+00, -1.4511e-01],
        [ 6.9507e-01, -2.3051e+00, -1.2791e-01,  1.2213e+00],
        [ 2.1970e-01, -1.7096e+00,  3.0163e+00, -1.8997e+00],
        [-1.2174e+00,  1.1350e+00, -1.2366e+00,  2.5243e-01],
        [ 1.2752e-01, -3.9800e-01,  7.5292e-01,  3.3198e-01],
        [ 1.4245e+00,  1.0030e+00,  2.9553e-01,  5.6809e-01],
        [-7.8438e-01, -3.2947e-01,  7.3929e-01, -9.1393e-01],
        [ 5.7730e-01, -8.3587e-02, -4.4516e-01, -1.1011e+00],
        [-6.9741e-01, -1.0918e+00, -1.0541e+00,  9.6540e-01],
        [-4.9679e-01, -6.9698e-01, -1.4118e+00, -1.5802e+00],
        [-1.6078e+00, -6.4802e-01, -1.8449e+00,  4.0479e-01],
        [-7.6589e-01,  1.2140e-01, -2.5117e-01, -1.3329e+00],
        [ 1.0760e+00, -3.4363e-01,  1.6881e+00,  2.9187e-01],
        [ 1.1773e+00,  2.1496e-03, -5.3378e-01, -4.9525e-01],
        [-1.1638e+00, -8.2471e-02, -9.6908e-01,  1.6762e+00],
        [-1.2539e-01,  6.8477e-01, -4.0020e-01, -1.7274e+00],
        [-8.3801e-01, -4.3201e-01, -9.6859e-01,  6.0071e-01],
        [-5.3626e-01,  1.1974e+00,  6.8208e-02, -8.8043e-01],
        [ 1.3004e+00, -3.0042e-01, -1.2143e+00, -5.5225e-01],
        [ 1.0056e+00, -8.4292e-01, -3.7514e-01,  1.4745e-01],
        [-5.8301e-01,  3.6208e-01,  1.1567e+00,  5.2850e-01],
        [ 1.9126e-01,  2.2353e+00,  1.4875e+00, -3.4010e-01],
        [-1.5007e+00, -8.7576e-01, -9.0034e-02,  1.3316e+00],
        [ 7.8691e-01, -4.4966e-01, -6.0609e-01, -2.0613e+00],
        [ 3.4731e-01,  6.2353e-01, -3.0427e-01,  1.1764e+00],
        [ 2.9571e-01, -1.5750e+00, -5.0543e-01, -1.3431e+00],
        [-2.3209e-02, -1.1014e+00, -1.1234e+00,  3.5477e-01],
        [ 3.4956e-02,  5.2186e-01, -1.8759e-01, -6.9594e-01],
        [ 9.6249e-01, -7.3714e-01, -3.3516e-01,  4.1438e-02],
        [-2.1502e+00,  2.6645e-01, -4.0042e-01,  8.8200e-03],
        [-1.5723e-02, -1.3450e+00, -4.6489e-01, -4.1890e-01],
        [ 3.9053e-01, -5.5635e-01, -1.4833e-01, -7.3757e-01],
        [ 3.6689e-02,  1.2173e+00, -1.0056e+00, -1.0719e+00]])
##########################################################
decoder.blks.block0.attention1.W_q.weight shape: torch.Size([4, 4])
tensor([[-0.1010, -0.3440, -0.0261,  0.7982],
        [ 0.2088, -0.0088,  0.0383,  0.8522],
        [ 0.1195,  0.3385, -0.6039,  0.0463],
        [ 0.2745, -0.8608, -0.1536,  0.5339]])
tensor([[-0.1762, -0.2745,  0.0337,  0.7936],
        [ 0.2290,  0.1228,  0.0524,  0.8287],
        [ 0.1387,  0.3309, -0.5667,  0.0811],
        [ 0.1716, -0.8921, -0.0808,  0.6159]])
##########################################################
decoder.blks.block0.attention1.W_k.weight shape: torch.Size([4, 4])
tensor([[ 0.7982, -0.1517, -0.7620,  0.6988],
        [ 0.8341,  0.0942,  0.0026,  0.8610],
        [ 0.3610,  0.1261,  0.2950,  0.6031],
        [-0.4520,  0.8108,  0.6142,  0.1845]])
tensor([[ 0.7295, -0.2030, -0.7970,  0.7012],
        [ 0.8274,  0.0942,  0.0141,  0.8590],
        [ 0.3188,  0.0770,  0.2253,  0.6319],
        [-0.4054,  0.8458,  0.6700,  0.1744]])
##########################################################
decoder.blks.block0.attention1.W_v.weight shape: torch.Size([4, 4])
tensor([[-0.3479, -0.7595, -0.5549, -0.3764],
        [-0.8647,  0.0434,  0.7126,  0.1762],
        [-0.2279, -0.5721, -0.5926,  0.6877],
        [-0.0193,  0.7091,  0.6245,  0.5812]])
tensor([[-0.2123, -0.8357, -0.5171, -0.2581],
        [-0.8120,  0.0209,  0.7043,  0.2223],
        [-0.0283, -0.7118, -0.5974,  0.7421],
        [-0.0572,  0.7315,  0.6228,  0.5744]])
##########################################################
decoder.blks.block0.attention1.W_o.weight shape: torch.Size([4, 4])
tensor([[-0.6671, -0.2816,  0.1999,  0.3170],
        [-0.2604,  0.6791, -0.2290, -0.0140],
        [ 0.1913, -0.3730,  0.7292,  0.1111],
        [ 0.8404, -0.4904, -0.2724,  0.4620]])
tensor([[-0.6309, -0.2204,  0.2171,  0.3225],
        [-0.2818,  0.6231, -0.2758,  0.0189],
        [ 0.1990, -0.3243,  0.8214,  0.0363],
        [ 0.8177, -0.5431, -0.3350,  0.4959]])
##########################################################
decoder.blks.block0.addnorm1.ln.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([1.0426, 0.9187, 0.9515, 1.0724])
##########################################################
decoder.blks.block0.addnorm1.ln.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0034,  0.0205, -0.0771,  0.0526])
##########################################################
decoder.blks.block0.attention2.W_q.weight shape: torch.Size([4, 4])
tensor([[ 0.1081, -0.1881,  0.2681, -0.6764],
        [-0.1944, -0.2206,  0.5079, -0.6966],
        [ 0.7393,  0.0884, -0.0885, -0.5167],
        [-0.1510,  0.4388, -0.5462,  0.2776]])
tensor([[ 0.1301, -0.2002,  0.2555, -0.6715],
        [-0.1939, -0.2209,  0.5095, -0.6986],
        [ 0.7401,  0.0893, -0.0868, -0.5206],
        [-0.1348,  0.4446, -0.5488,  0.2572]])
##########################################################
decoder.blks.block0.attention2.W_k.weight shape: torch.Size([4, 4])
tensor([[ 0.5849, -0.5343, -0.6361,  0.3138],
        [ 0.3391,  0.4110,  0.1580,  0.1642],
        [-0.5780,  0.5701,  0.8011, -0.4690],
        [ 0.5413,  0.5191,  0.4266, -0.7002]])
tensor([[ 0.5864, -0.5282, -0.6393,  0.3093],
        [ 0.3302,  0.4258,  0.1540,  0.1617],
        [-0.5669,  0.5921,  0.7853, -0.4872],
        [ 0.5403,  0.5164,  0.4274, -0.6972]])
##########################################################
decoder.blks.block0.attention2.W_v.weight shape: torch.Size([4, 4])
tensor([[ 0.6872, -0.3625,  0.4643, -0.2886],
        [ 0.4889, -0.4501, -0.5930, -0.4807],
        [-0.0659, -0.8593, -0.5777, -0.5164],
        [-0.5871, -0.5287,  0.2896,  0.2356]])
tensor([[ 0.7007, -0.3707,  0.4708, -0.3012],
        [ 0.4746, -0.4200, -0.6138, -0.4756],
        [-0.0696, -0.8358, -0.5884, -0.5263],
        [-0.4926, -0.6177,  0.3170,  0.2006]])
##########################################################
decoder.blks.block0.attention2.W_o.weight shape: torch.Size([4, 4])
tensor([[ 0.3903,  0.1440,  0.7054,  0.7522],
        [-0.5836,  0.7689,  0.3773, -0.0822],
        [-0.8390,  0.0861, -0.2387, -0.4290],
        [-0.6698, -0.7880, -0.6975,  0.8163]])
tensor([[ 0.3954,  0.1426,  0.6996,  0.7467],
        [-0.5622,  0.8004,  0.4100, -0.0970],
        [-0.9161,  0.0212, -0.3177, -0.4177],
        [-0.6099, -0.7477, -0.6392,  0.8263]])
##########################################################
decoder.blks.block0.addnorm2.ln.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9929, 0.9446, 1.0143, 1.0207])
##########################################################
decoder.blks.block0.addnorm2.ln.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0257, -0.0295, -0.0486,  0.0017])
##########################################################
decoder.blks.block0.ffn.dense1.weight shape: torch.Size([8, 4])
tensor([[ 0.4119,  0.1874, -0.5712,  0.1685],
        [ 0.1124, -0.3766,  0.0232, -0.3879],
        [ 0.0663, -0.3643, -0.2055,  0.4372],
        [-0.2487,  0.4328,  0.6677,  0.5496],
        [-0.5334,  0.0222, -0.6554, -0.6993],
        [ 0.5514, -0.6501, -0.2300, -0.0246],
        [-0.1113,  0.5245,  0.3181, -0.4575],
        [-0.3969,  0.4293,  0.2994, -0.3008]])
tensor([[ 0.3566,  0.2209, -0.5493,  0.1682],
        [ 0.1418, -0.3876,  0.0192, -0.4032],
        [ 0.1533, -0.3792, -0.2371,  0.3938],
        [-0.2466,  0.4304,  0.6664,  0.5513],
        [-0.5270,  0.0301, -0.6399, -0.7298],
        [ 0.5368, -0.6295, -0.2546, -0.0069],
        [-0.1385,  0.5083,  0.3238, -0.4165],
        [-0.4408,  0.4219,  0.2983, -0.2451]])
##########################################################
decoder.blks.block0.ffn.dense1.bias shape: torch.Size([8])
tensor([ 0.1083,  0.3145,  0.3485,  0.1650, -0.1352, -0.4305, -0.2088, -0.0328])
tensor([ 0.0817,  0.3213,  0.4134,  0.1604, -0.1220, -0.4307, -0.2272, -0.0542])
##########################################################
decoder.blks.block0.ffn.dense2.weight shape: torch.Size([4, 8])
tensor([[-0.1015, -0.6192, -0.0050, -0.1298,  0.7058,  0.4734,  0.1533,  0.0083],
        [-0.5662, -0.0052,  0.4855,  0.0462, -0.0290,  0.6101,  0.2201,  0.0007],
        [ 0.3497, -0.3166, -0.5122,  0.2204,  0.0500,  0.6004, -0.3209, -0.5646],
        [ 0.4506, -0.4484, -0.0193, -0.2324, -0.4232, -0.6224,  0.2672,  0.3932]])
tensor([[-0.0852, -0.6241,  0.0392, -0.1290,  0.7072,  0.4761,  0.1800,  0.0324],
        [-0.5422,  0.0303,  0.4924,  0.0499, -0.0506,  0.6425,  0.1889, -0.0196],
        [ 0.3189, -0.3257, -0.5581,  0.2130,  0.0783,  0.5514, -0.2918, -0.5659],
        [ 0.4414, -0.4696, -0.0243, -0.2296, -0.4309, -0.6082,  0.2431,  0.3910]])
##########################################################
decoder.blks.block0.ffn.dense2.bias shape: torch.Size([4])
tensor([-0.3515,  0.2068,  0.0262, -0.0061])
tensor([-0.3279,  0.2287,  0.0017, -0.0263])
##########################################################
decoder.blks.block0.addnorm3.ln.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9762, 0.9730, 1.0048, 1.0300])
##########################################################
decoder.blks.block0.addnorm3.ln.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0109, -0.0065, -0.0204, -0.0878])
##########################################################
decoder.blks.block1.attention1.W_q.weight shape: torch.Size([4, 4])
tensor([[ 0.2770, -0.6588,  0.3021,  0.3589],
        [ 0.7125, -0.0070,  0.4377,  0.5364],
        [ 0.6605,  0.2365, -0.0797, -0.4736],
        [ 0.3122, -0.5754, -0.3160,  0.6375]])
tensor([[ 2.9167e-01, -6.5000e-01,  3.0493e-01,  3.3081e-01],
        [ 7.2570e-01, -1.9448e-05,  4.3994e-01,  5.1230e-01],
        [ 6.6723e-01,  2.4125e-01, -8.0361e-02, -4.8533e-01],
        [ 3.1630e-01, -5.7389e-01, -3.1548e-01,  6.3071e-01]])
##########################################################
decoder.blks.block1.attention1.W_k.weight shape: torch.Size([4, 4])
tensor([[ 0.7841, -0.3579,  0.4296, -0.3228],
        [-0.2827, -0.5637,  0.1157, -0.6662],
        [-0.2891,  0.2487, -0.1934, -0.3661],
        [-0.3671, -0.3248,  0.6216, -0.0401]])
tensor([[ 0.7797, -0.3625,  0.4221, -0.3056],
        [-0.2842, -0.5646,  0.1140, -0.6618],
        [-0.2804,  0.2583, -0.1746, -0.4044],
        [-0.3733, -0.3315,  0.6089, -0.0136]])
##########################################################
decoder.blks.block1.attention1.W_v.weight shape: torch.Size([4, 4])
tensor([[ 0.4358,  0.3613,  0.1271,  0.8478],
        [ 0.6271,  0.4995,  0.4434,  0.6869],
        [ 0.0466,  0.4514,  0.1382,  0.7498],
        [ 0.3481, -0.1304, -0.7187,  0.3418]])
tensor([[ 0.4315,  0.3609,  0.1884,  0.7953],
        [ 0.6110,  0.4883,  0.4279,  0.7282],
        [ 0.0236,  0.4170,  0.1756,  0.7754],
        [ 0.3527, -0.1219, -0.7250,  0.3338]])
##########################################################
decoder.blks.block1.attention1.W_o.weight shape: torch.Size([4, 4])
tensor([[ 0.5794,  0.3298,  0.2180, -0.0829],
        [-0.6066, -0.3126, -0.7854, -0.1858],
        [-0.8323,  0.5850, -0.6605, -0.0814],
        [ 0.5638, -0.1381,  0.5977, -0.4374]])
tensor([[ 0.5712,  0.3261,  0.2128, -0.0894],
        [-0.6126, -0.3149, -0.7914, -0.1715],
        [-0.8011,  0.5963, -0.6602, -0.0768],
        [ 0.5476, -0.1432,  0.6082, -0.4482]])
##########################################################
decoder.blks.block1.addnorm1.ln.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9456, 1.0069, 0.9763, 1.0092])
##########################################################
decoder.blks.block1.addnorm1.ln.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.0042,  0.0361, -0.0163, -0.0194])
##########################################################
decoder.blks.block1.attention2.W_q.weight shape: torch.Size([4, 4])
tensor([[ 0.1014, -0.0037,  0.3135,  0.0646],
        [-0.2683, -0.4580,  0.1060,  0.2267],
        [ 0.4161, -0.8003,  0.5321, -0.4659],
        [ 0.0026, -0.3079,  0.6808, -0.2763]])
tensor([[ 0.0944,  0.0166,  0.2610,  0.1039],
        [-0.2521, -0.4921,  0.2074,  0.1423],
        [ 0.4217, -0.7966,  0.5290, -0.4724],
        [-0.0062, -0.2982,  0.6594, -0.2553]])
##########################################################
decoder.blks.block1.attention2.W_k.weight shape: torch.Size([4, 4])
tensor([[-0.4313,  0.6015,  0.5674,  0.2840],
        [ 0.6320, -0.7104, -0.2193,  0.5767],
        [-0.7482, -0.5689, -0.7371,  0.4672],
        [ 0.6414, -0.2932,  0.6826, -0.2374]])
tensor([[-0.4204,  0.5820,  0.5715,  0.2891],
        [ 0.6396, -0.7189, -0.2200,  0.5787],
        [-0.7659, -0.5284, -0.7543,  0.4603],
        [ 0.6282, -0.2627,  0.6714, -0.2445]])
##########################################################
decoder.blks.block1.attention2.W_v.weight shape: torch.Size([4, 4])
tensor([[-0.4992,  0.7260,  0.2989,  0.2807],
        [-0.1749, -0.8005,  0.7220,  0.6829],
        [ 0.1353, -0.8587,  0.0183, -0.8423],
        [-0.6142, -0.7058,  0.1404, -0.6195]])
tensor([[-0.5556,  0.7384,  0.3114,  0.3143],
        [-0.1719, -0.8081,  0.7245,  0.6851],
        [ 0.1277, -0.8125,  0.0015, -0.8662],
        [-0.6416, -0.6742,  0.1363, -0.6196]])
##########################################################
decoder.blks.block1.attention2.W_o.weight shape: torch.Size([4, 4])
tensor([[ 0.7821, -0.4519,  0.4991,  0.2181],
        [-0.8197,  0.1185, -0.3157, -0.4686],
        [ 0.6703,  0.1438, -0.4719,  0.0726],
        [ 0.0792,  0.3184, -0.6204, -0.2253]])
tensor([[ 0.7923, -0.4656,  0.4737,  0.2019],
        [-0.8579,  0.0942, -0.2548, -0.4567],
        [ 0.6892,  0.1786, -0.4780,  0.0904],
        [ 0.0750,  0.3174, -0.6322, -0.2353]])
##########################################################
decoder.blks.block1.addnorm2.ln.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([0.9372, 1.0236, 1.0071, 1.0197])
##########################################################
decoder.blks.block1.addnorm2.ln.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([-0.0085,  0.0603, -0.0379, -0.0217])
##########################################################
decoder.blks.block1.ffn.dense1.weight shape: torch.Size([8, 4])
tensor([[-0.1534, -0.2813,  0.1569,  0.0917],
        [-0.6192,  0.0928, -0.1815, -0.3186],
        [ 0.5156,  0.4612,  0.3075,  0.5516],
        [-0.4322, -0.6838, -0.1161,  0.3494],
        [-0.1869, -0.4269,  0.4673,  0.5850],
        [-0.1964, -0.0280,  0.2280, -0.7029],
        [ 0.0940, -0.4903, -0.6502,  0.2114],
        [-0.3570,  0.2178,  0.4078,  0.0749]])
tensor([[-0.1529, -0.2813,  0.1565,  0.0917],
        [-0.6391,  0.0913, -0.1721, -0.3051],
        [ 0.5156,  0.4612,  0.3075,  0.5516],
        [-0.4313, -0.6854, -0.1130,  0.3471],
        [-0.1872, -0.4267,  0.4671,  0.5852],
        [-0.1525, -0.0368,  0.2345, -0.7451],
        [ 0.0865, -0.4890, -0.6549,  0.2226],
        [-0.3570,  0.2198,  0.4081,  0.0727]])
##########################################################
decoder.blks.block1.ffn.dense1.bias shape: torch.Size([8])
tensor([-0.1803,  0.4012, -0.4582,  0.1415,  0.3991, -0.1187,  0.0563,  0.0138])
tensor([-0.1807,  0.3971, -0.4582,  0.1393,  0.3993, -0.0982,  0.0567,  0.0153])
##########################################################
decoder.blks.block1.ffn.dense2.weight shape: torch.Size([4, 8])
tensor([[-0.5579,  0.3071, -0.4563, -0.4665, -0.3726, -0.3485, -0.3401,  0.1108],
        [ 0.6565,  0.1303, -0.1826, -0.4967, -0.6016,  0.4990, -0.3319, -0.6698],
        [ 0.4941, -0.0175, -0.6359, -0.4029, -0.5353,  0.2798, -0.6774, -0.3639],
        [-0.5337,  0.5733,  0.6385, -0.1892, -0.4837,  0.5598, -0.3155,  0.2466]])
tensor([[-0.5579,  0.3208, -0.4563, -0.4643, -0.3711, -0.3653, -0.3154,  0.1127],
        [ 0.6565,  0.1427, -0.1826, -0.4948, -0.6003,  0.5310, -0.3462, -0.6682],
        [ 0.4941, -0.0460, -0.6359, -0.4052, -0.5377,  0.2471, -0.6880, -0.3675],
        [-0.5337,  0.5698,  0.6385, -0.1910, -0.4842,  0.5750, -0.3164,  0.2467]])
##########################################################
decoder.blks.block1.ffn.dense2.bias shape: torch.Size([4])
tensor([0.0627, 0.2267, 0.0256, 0.2036])
tensor([ 0.0528,  0.2839, -0.0180,  0.1894])
##########################################################
decoder.blks.block1.addnorm3.ln.weight shape: torch.Size([4])
tensor([1., 1., 1., 1.])
tensor([1.2959, 1.9131, 2.4349, 1.4671])
##########################################################
decoder.blks.block1.addnorm3.ln.bias shape: torch.Size([4])
tensor([0., 0., 0., 0.])
tensor([ 0.4490,  0.7787, -1.1089, -0.6173])
##########################################################
decoder.dense.weight shape: torch.Size([166, 4])
tensor([[ 0.1166, -0.1316, -0.1849, -0.1206],
        [-0.1757, -0.0727,  0.1454, -0.1447],
        [-0.1463, -0.1008,  0.0802,  0.1591],
        [-0.1165, -0.1557, -0.1460, -0.1604],
        [-0.1618, -0.0651, -0.1526, -0.0324],
        [-0.0430, -0.1143,  0.0547,  0.1104],
        [ 0.1524, -0.0161, -0.0335, -0.1723],
        [-0.1073,  0.0762, -0.1146, -0.0648],
        [-0.0388, -0.0387, -0.0439, -0.1314],
        [ 0.1381,  0.0818,  0.0413, -0.0989],
        [ 0.1172, -0.1113, -0.1126,  0.1482],
        [ 0.1624, -0.0848,  0.0507,  0.0667],
        [ 0.1024,  0.1362, -0.0317, -0.0914],
        [ 0.0888, -0.0827, -0.1714,  0.0904],
        [-0.0367,  0.0768, -0.1269,  0.0345],
        [ 0.0055, -0.1088, -0.0236, -0.1387],
        [ 0.0030,  0.0087, -0.1671,  0.1751],
        [-0.1367,  0.1423,  0.1129,  0.0003],
        [ 0.1159, -0.0353,  0.1180, -0.0241],
        [ 0.0262, -0.0522,  0.0024, -0.1494],
        [ 0.0892,  0.0323,  0.0549, -0.0331],
        [ 0.1393,  0.0768, -0.1753,  0.0139],
        [-0.0497, -0.1424,  0.0551, -0.0454],
        [ 0.0270,  0.0002, -0.0891,  0.1451],
        [-0.1180, -0.0036, -0.0674,  0.0699],
        [ 0.1342,  0.0228, -0.0147,  0.1812],
        [-0.1299,  0.1790,  0.0658, -0.0014],
        [-0.0037, -0.0872, -0.1611, -0.0981],
        [-0.0809,  0.0864,  0.0159, -0.0242],
        [ 0.1649, -0.0924,  0.1335, -0.1275],
        [ 0.0696, -0.0822,  0.1665,  0.1637],
        [ 0.1162, -0.1378,  0.1693,  0.0270],
        [ 0.1569, -0.1459, -0.0371,  0.0599],
        [-0.0783,  0.1680,  0.0611, -0.1291],
        [ 0.0628,  0.0118,  0.0735, -0.1364],
        [ 0.0567,  0.0505,  0.0274, -0.1793],
        [-0.1520,  0.1294,  0.1258, -0.0507],
        [ 0.1237, -0.1803,  0.0543, -0.0586],
        [-0.0540, -0.0859, -0.0015,  0.0235],
        [-0.1004,  0.0691,  0.1040, -0.1082],
        [ 0.0372, -0.1212, -0.1710, -0.0792],
        [-0.0588, -0.0313,  0.0566,  0.0531],
        [-0.1238, -0.1693,  0.0265, -0.0062],
        [ 0.1631,  0.0301, -0.0248, -0.0206],
        [-0.1082, -0.0162, -0.1291, -0.1649],
        [-0.1273, -0.0231, -0.1544, -0.0189],
        [-0.1518,  0.1284, -0.0065, -0.1873],
        [ 0.1210,  0.0450, -0.1339,  0.1179],
        [-0.1069, -0.0453, -0.1731, -0.1125],
        [-0.0578, -0.0732,  0.1334,  0.0147],
        [-0.0008,  0.0966,  0.0804, -0.0366],
        [-0.0901,  0.1011, -0.1166, -0.0145],
        [ 0.1030, -0.1718, -0.0149,  0.0061],
        [ 0.1397, -0.0151, -0.0648, -0.0136],
        [ 0.0424,  0.0154, -0.0085, -0.0276],
        [-0.1604, -0.0177, -0.0971, -0.0336],
        [ 0.1211, -0.0235, -0.1458,  0.0623],
        [-0.0240, -0.1530,  0.1644, -0.0474],
        [ 0.1141,  0.1162,  0.0475, -0.0764],
        [ 0.1100, -0.1742, -0.0711,  0.1550],
        [ 0.0469, -0.1530, -0.0638,  0.0176],
        [ 0.1766, -0.1712,  0.0828,  0.0501],
        [-0.0570, -0.0891,  0.0995, -0.0411],
        [ 0.1772, -0.1296,  0.0318,  0.0242],
        [ 0.1250,  0.0362,  0.0213, -0.0296],
        [-0.0190,  0.0606, -0.0090, -0.0785],
        [ 0.1378,  0.0988,  0.0700,  0.1476],
        [ 0.0838, -0.1381, -0.0809, -0.1141],
        [ 0.0572, -0.1130, -0.1523, -0.0480],
        [ 0.0526,  0.1549,  0.1324, -0.0095],
        [-0.0273, -0.1249,  0.0656, -0.0388],
        [-0.0126,  0.1796, -0.1353, -0.0975],
        [-0.1680,  0.1297,  0.0214,  0.1410],
        [ 0.0960,  0.1659, -0.1461,  0.0383],
        [-0.0762,  0.0924,  0.0556, -0.1297],
        [ 0.0881, -0.0561, -0.1615,  0.0959],
        [-0.0627,  0.0603,  0.1113, -0.1270],
        [-0.0418,  0.1212,  0.1088,  0.1639],
        [ 0.1878,  0.0904, -0.0382, -0.1804],
        [-0.1524, -0.1604,  0.0500, -0.0316],
        [-0.0097, -0.1662,  0.0602,  0.1721],
        [ 0.0348, -0.1412,  0.1234, -0.1065],
        [-0.1516,  0.1760,  0.1653,  0.1809],
        [ 0.1106, -0.1491, -0.1461, -0.0827],
        [-0.1804,  0.0697, -0.0130,  0.0827],
        [ 0.1079,  0.0348,  0.0938,  0.1816],
        [-0.1173,  0.0930,  0.1691,  0.1197],
        [ 0.1008,  0.1409,  0.0275, -0.1038],
        [ 0.0494, -0.1875,  0.0688,  0.1585],
        [-0.0538, -0.1444,  0.1266, -0.0158],
        [ 0.1297, -0.1688, -0.0865,  0.0925],
        [-0.1818, -0.0806,  0.0598, -0.1752],
        [ 0.0254, -0.1100, -0.1706, -0.1419],
        [-0.0321, -0.1096, -0.1329, -0.1585],
        [ 0.0770,  0.0469, -0.0098, -0.1733],
        [-0.0903,  0.0157,  0.1324,  0.1543],
        [ 0.0550, -0.0078, -0.0287, -0.0245],
        [ 0.1460,  0.1389,  0.1727,  0.0924],
        [ 0.0659, -0.1738, -0.0386, -0.1426],
        [ 0.0448, -0.0216, -0.1539, -0.1343],
        [ 0.1377,  0.1135, -0.0884,  0.1088],
        [ 0.1820,  0.0039, -0.1613,  0.0587],
        [-0.0488, -0.0182,  0.0661, -0.0813],
        [-0.1730,  0.1395,  0.1728,  0.0967],
        [-0.0213, -0.0126, -0.0099, -0.0015],
        [ 0.0581,  0.1737,  0.0479,  0.0249],
        [-0.0203, -0.0286, -0.1737, -0.0607],
        [ 0.0731,  0.0527, -0.0905,  0.0265],
        [-0.0863, -0.1434,  0.0541,  0.0896],
        [-0.0496,  0.1845, -0.1694,  0.0881],
        [-0.0882, -0.1216, -0.1207, -0.0392],
        [ 0.0310, -0.1462,  0.1099,  0.1074],
        [-0.0258, -0.1330, -0.0380,  0.1009],
        [ 0.1421,  0.0268,  0.1705,  0.0398],
        [-0.0443, -0.0648, -0.0325,  0.0725],
        [ 0.1651,  0.1098,  0.1343, -0.0711],
        [-0.0890,  0.1598, -0.1875,  0.0157],
        [ 0.0331,  0.0362,  0.0634, -0.1067],
        [-0.1641,  0.0849, -0.0287, -0.0432],
        [-0.0788, -0.0828,  0.0534, -0.1087],
        [-0.0305, -0.1082, -0.0356, -0.0603],
        [-0.1179,  0.0613, -0.1752,  0.1682],
        [ 0.0077,  0.1485, -0.1767,  0.1282],
        [-0.1393,  0.0956,  0.0626,  0.0188],
        [-0.0404,  0.1677, -0.0188,  0.1049],
        [ 0.1464, -0.1731,  0.0454, -0.1081],
        [-0.0745, -0.0741,  0.0906,  0.0514],
        [-0.0170, -0.1761, -0.1298, -0.1832],
        [ 0.1151,  0.0358,  0.0423,  0.1792],
        [ 0.1122, -0.0670, -0.0467, -0.0100],
        [ 0.0788,  0.1151, -0.1266,  0.1762],
        [ 0.1170, -0.1621, -0.0770, -0.0700],
        [ 0.0535, -0.1572,  0.0277, -0.1100],
        [-0.1411, -0.1421, -0.0048,  0.1425],
        [-0.1768, -0.1462,  0.0021,  0.1414],
        [ 0.0969, -0.1279,  0.1221, -0.0537],
        [ 0.1614,  0.1863,  0.0777, -0.0622],
        [-0.1791,  0.1243,  0.0116,  0.1761],
        [-0.0657, -0.1660,  0.1744,  0.1568],
        [ 0.1603,  0.0868,  0.1398,  0.1364],
        [ 0.0986, -0.0986, -0.0576, -0.1320],
        [-0.0788,  0.0939, -0.0811,  0.0085],
        [ 0.0585,  0.0673, -0.1539, -0.1037],
        [ 0.0506, -0.0950, -0.0287, -0.1513],
        [ 0.0766, -0.1632,  0.1485,  0.1771],
        [ 0.1285, -0.1156,  0.0595, -0.1001],
        [-0.1709, -0.1870, -0.0149, -0.1355],
        [-0.1766,  0.0333,  0.1523,  0.0227],
        [-0.0083, -0.1665,  0.1341,  0.0578],
        [ 0.0008, -0.0009,  0.1298,  0.1117],
        [ 0.1586,  0.0240,  0.1506,  0.0459],
        [-0.0667,  0.0111,  0.1635,  0.1844],
        [ 0.1148,  0.0017, -0.0379, -0.0467],
        [-0.1176,  0.0254, -0.1492, -0.0283],
        [-0.1087, -0.1691,  0.0144, -0.0691],
        [-0.1503,  0.0692,  0.0083,  0.1205],
        [-0.0780, -0.0466, -0.1163,  0.0575],
        [ 0.0106,  0.1697,  0.1742, -0.1375],
        [-0.0826, -0.1487,  0.0435,  0.1313],
        [ 0.1000,  0.1778, -0.0706,  0.0242],
        [-0.1326,  0.1236, -0.0386,  0.0252],
        [-0.1541,  0.0327,  0.0816, -0.0904],
        [ 0.0304,  0.1276, -0.1385,  0.1694],
        [ 0.1436,  0.1688, -0.1365, -0.0546],
        [-0.1645, -0.0301, -0.0671,  0.1717],
        [-0.1749, -0.1016,  0.0540,  0.0601]])
tensor([[ 0.0840, -0.1973, -0.0945, -0.0887],
        [-0.1886, -0.0988,  0.1787, -0.1307],
        [-0.1583, -0.1272,  0.1158,  0.1709],
        [ 0.0426,  0.1978, -0.5031, -0.3982],
        [ 0.0762,  0.5747, -0.8299, -0.3398],
        [ 0.1464,  0.4193, -0.8340,  0.0623],
        [ 0.4753,  0.6126, -0.3961, -0.7710],
        [ 0.3557,  0.3802, -0.6947, -0.4490],
        [ 0.2431,  0.3057, -0.7827, -0.2435],
        [ 0.3650,  0.6557, -0.5169, -0.4473],
        [ 0.3078,  0.2457, -0.6446, -0.0154],
        [ 0.3088,  0.3424, -0.5829, -0.0357],
        [ 0.2234,  0.5346, -0.5604, -0.2120],
        [ 0.1964,  0.3228, -0.5057, -0.1441],
        [ 0.1563,  0.3701, -0.6488, -0.0908],
        [ 0.2050,  0.2415, -0.6490, -0.2547],
        [ 0.1134,  0.2667, -0.4956,  0.0517],
        [ 0.0785,  0.5906, -0.2879, -0.3493],
        [ 0.2886,  0.3569, -0.3121, -0.2632],
        [ 0.1459,  0.1600, -0.4144, -0.2014],
        [ 0.1540,  0.1687, -0.1869, -0.0693],
        [ 0.1508,  0.1472, -0.2677, -0.0022],
        [-0.0072, -0.0363, -0.1253, -0.0704],
        [ 0.0098, -0.0366, -0.0598,  0.1726],
        [-0.1078,  0.0231, -0.1206,  0.0679],
        [ 0.1620,  0.0872, -0.1389,  0.1730],
        [-0.1113,  0.2564, -0.0764,  0.0016],
        [ 0.0236, -0.0223, -0.2584, -0.1237],
        [-0.0908,  0.0836, -0.0100,  0.0010],
        [ 0.1617, -0.0913,  0.1295, -0.1241],
        [ 0.0750, -0.0632,  0.1162,  0.1712],
        [ 0.1440, -0.0714,  0.0506,  0.0133],
        [ 0.1603, -0.1469, -0.0501,  0.0633],
        [-0.0951,  0.1295,  0.1084, -0.1099],
        [ 0.0419, -0.0280,  0.1123, -0.1071],
        [ 0.0181, -0.0220,  0.1131, -0.1337],
        [-0.1348,  0.2286,  0.0082, -0.0786],
        [ 0.1050, -0.2130,  0.0935, -0.0373],
        [-0.0778, -0.1369,  0.0659,  0.0477],
        [-0.1190,  0.0303,  0.1531, -0.0876],
        [ 0.0078, -0.1835, -0.0872, -0.0498],
        [-0.0734, -0.0516,  0.0709,  0.0754],
        [-0.1381, -0.1972,  0.0611,  0.0096],
        [ 0.1146, -0.0641,  0.1017,  0.0279],
        [-0.1291, -0.0672, -0.0823, -0.1313],
        [-0.1603, -0.0941, -0.0567,  0.0129],
        [-0.1883,  0.0510,  0.0930, -0.1476],
        [ 0.0885, -0.0238, -0.0375,  0.1484],
        [-0.1369, -0.1088, -0.0856, -0.0835],
        [-0.0695, -0.0970,  0.1646,  0.0268],
        [-0.0202,  0.0565,  0.1333, -0.0165],
        [-0.1227,  0.0307, -0.0202,  0.0175],
        [ 0.0706, -0.2357,  0.0694,  0.0392],
        [ 0.1100, -0.0754,  0.0155,  0.0169],
        [ 0.0203, -0.0301,  0.0528, -0.0053],
        [-0.1813, -0.0634, -0.0360, -0.0124],
        [ 0.0900, -0.0886, -0.0550,  0.0918],
        [-0.0386, -0.1815,  0.2010, -0.0318],
        [ 0.0777,  0.0430,  0.1436, -0.0382],
        [ 0.0772, -0.2414,  0.0216,  0.1860],
        [ 0.0277, -0.1921, -0.0101,  0.0361],
        [ 0.1636, -0.1964,  0.1165,  0.0632],
        [-0.0837, -0.1425,  0.1688, -0.0131],
        [ 0.1473, -0.1880,  0.1095,  0.0544],
        [ 0.0970, -0.0202,  0.0966, -0.0010],
        [-0.0519, -0.0075,  0.0814, -0.0444],
        [ 0.1104,  0.0417,  0.1455,  0.1757],
        [ 0.0567, -0.1919, -0.0082, -0.0868],
        [ 0.0280, -0.1726, -0.0696, -0.0202],
        [ 0.0257,  0.0997,  0.2043,  0.0190],
        [-0.0441, -0.1586,  0.1102, -0.0215],
        [-0.0501,  0.0997, -0.0266, -0.0600],
        [-0.1889,  0.0826,  0.0855,  0.1614],
        [ 0.0522,  0.0727, -0.0177,  0.0809],
        [-0.1064,  0.0301,  0.1366, -0.0975],
        [ 0.0635, -0.1083, -0.0878,  0.1187],
        [-0.0797,  0.0257,  0.1562, -0.1089],
        [-0.0671,  0.0663,  0.1820,  0.1895],
        [ 0.1383, -0.0071,  0.0900, -0.1284],
        [-0.1753, -0.2077,  0.1121, -0.0080],
        [-0.0325, -0.2139,  0.1244,  0.1943],
        [ 0.0203, -0.1691,  0.1595, -0.0912],
        [-0.1651,  0.1456,  0.2057,  0.1946],
        [ 0.0782, -0.2141, -0.0571, -0.0509],
        [-0.2011,  0.0235,  0.0499,  0.1031],
        [ 0.0929,  0.0035,  0.1362,  0.1963],
        [-0.1347,  0.0553,  0.2186,  0.1377],
        [ 0.0572,  0.0530,  0.1425, -0.0577],
        [ 0.0367, -0.2135,  0.1040,  0.1708],
        [-0.0731, -0.1830,  0.1767,  0.0045],
        [ 0.0920, -0.2447,  0.0176,  0.1288],
        [-0.1986, -0.1150,  0.1044, -0.1573],
        [-0.0052, -0.1724, -0.0851, -0.1118],
        [-0.0765, -0.1993, -0.0130, -0.1133],
        [ 0.0395, -0.0280,  0.0885, -0.1338],
        [-0.1029, -0.0116,  0.1688,  0.1669],
        [ 0.0215, -0.0763,  0.0631,  0.0095],
        [ 0.1148,  0.0761,  0.2545,  0.1251],
        [ 0.0459, -0.2131,  0.0139, -0.1222],
        [-0.0019, -0.1167, -0.0257, -0.0873],
        [ 0.1100,  0.0546, -0.0065,  0.1353],
        [ 0.1488, -0.0650, -0.0649,  0.0901],
        [-0.0651, -0.0515,  0.1099, -0.0643],
        [-0.1926,  0.0964,  0.2290,  0.1172],
        [-0.0428, -0.0575,  0.0506,  0.0200],
        [ 0.0304,  0.1156,  0.1254,  0.0533],
        [-0.0730, -0.1381, -0.0254, -0.0085],
        [ 0.0408, -0.0151,  0.0024,  0.0582],
        [-0.1028, -0.1780,  0.1006,  0.1059],
        [-0.0870,  0.1018, -0.0537,  0.1235],
        [-0.1098, -0.1672, -0.0581, -0.0182],
        [ 0.0189, -0.1707,  0.1426,  0.1196],
        [-0.0433, -0.1699,  0.0129,  0.1176],
        [ 0.1184, -0.0198,  0.2311,  0.0647],
        [-0.0699, -0.1190,  0.0413,  0.0976],
        [ 0.1292,  0.0396,  0.2248, -0.0325],
        [-0.1240,  0.0826, -0.0799,  0.0491],
        [-0.0014, -0.0327,  0.1530, -0.0700],
        [-0.2000,  0.0078,  0.0738, -0.0065],
        [-0.1057, -0.1369,  0.1239, -0.0803],
        [-0.0555, -0.1592,  0.0329, -0.0351],
        [-0.1658, -0.0462, -0.0254,  0.2126],
        [-0.0537,  0.0151,  0.0068,  0.1871],
        [-0.1608,  0.0492,  0.1242,  0.0409],
        [-0.0703,  0.1023,  0.0701,  0.1344],
        [ 0.1100, -0.2413,  0.1342, -0.0699],
        [-0.0921, -0.1107,  0.1391,  0.0693],
        [-0.0567, -0.2549, -0.0245, -0.1429],
        [ 0.0932, -0.0099,  0.1047,  0.2004],
        [ 0.0780, -0.1358,  0.0459,  0.0242],
        [ 0.0292,  0.0084,  0.0210,  0.2232],
        [ 0.0819, -0.2310,  0.0158, -0.0348],
        [ 0.0366, -0.1903,  0.0716, -0.0925],
        [-0.1538, -0.1700,  0.0336,  0.1545],
        [-0.1883, -0.1717,  0.0371,  0.1523],
        [ 0.0734, -0.1728,  0.1803, -0.0289],
        [ 0.1256,  0.1142,  0.1721, -0.0245],
        [-0.1962,  0.0852,  0.0654,  0.1925],
        [-0.0749, -0.1852,  0.1997,  0.1661],
        [ 0.1425,  0.0503,  0.1885,  0.1545],
        [ 0.0680, -0.1592,  0.0235, -0.1008],
        [-0.1058,  0.0355, -0.0012,  0.0351],
        [ 0.0112, -0.0304, -0.0215, -0.0563],
        [ 0.0074, -0.1794,  0.0818, -0.1062],
        [ 0.0607, -0.1951,  0.1909,  0.1930],
        [ 0.0912, -0.1865,  0.1515, -0.0607],
        [-0.1856, -0.2172,  0.0252, -0.1204],
        [-0.1891,  0.0066,  0.1872,  0.0358],
        [-0.0265, -0.2029,  0.1816,  0.0765],
        [-0.0221, -0.0484,  0.1924,  0.1351],
        [ 0.1304, -0.0314,  0.2228,  0.0754],
        [-0.0808, -0.0194,  0.2040,  0.1987],
        [ 0.0813, -0.0660,  0.0530, -0.0127],
        [-0.1457, -0.0354, -0.0652, -0.0011],
        [-0.1242, -0.2008,  0.0565, -0.0533],
        [-0.1840, -0.0054,  0.1084,  0.1540],
        [-0.1121, -0.1197, -0.0158,  0.0902],
        [-0.0282,  0.0924,  0.2717, -0.0945],
        [-0.1040, -0.1940,  0.1044,  0.1522],
        [ 0.0498,  0.0728,  0.0708,  0.0747],
        [-0.1608,  0.0619,  0.0450,  0.0534],
        [-0.1817, -0.0248,  0.1561, -0.0610],
        [-0.0029,  0.0546, -0.0358,  0.2003],
        [ 0.0975,  0.0728, -0.0058, -0.0087],
        [-0.1944, -0.0974,  0.0258,  0.1998],
        [-0.1970, -0.1490,  0.1169,  0.0823]])
##########################################################
decoder.dense.bias shape: torch.Size([166])
tensor([-3.0950e-01, -3.4233e-01, -1.9371e-01,  9.1360e-02, -3.4476e-01,
         1.4161e-01,  2.7867e-01,  3.9398e-01, -2.1922e-01, -4.4386e-01,
         2.2113e-01,  1.0391e-01, -4.1267e-01,  4.9425e-01, -4.0961e-01,
        -3.3053e-01,  4.5430e-01,  8.0705e-05,  3.4677e-01,  4.3578e-01,
        -1.2131e-01,  3.1328e-01,  2.9970e-01,  4.1282e-01, -1.1500e-01,
        -4.8773e-01, -4.1752e-01, -1.2574e-01,  4.6604e-01, -2.1053e-01,
         3.5505e-01, -6.0300e-02, -4.7014e-01, -3.5985e-01,  9.0151e-02,
         3.6198e-01, -3.5089e-01, -5.1258e-02,  1.7862e-01, -2.4520e-01,
        -9.7665e-02, -1.5059e-01, -4.2274e-02,  4.4935e-01, -5.1029e-02,
        -2.6183e-02,  1.0718e-01, -2.6140e-01, -2.4278e-01, -4.2617e-01,
        -4.3637e-01, -2.2255e-01,  4.2657e-01, -1.8623e-01, -4.0094e-01,
        -3.4100e-01, -2.7249e-01, -7.3001e-02,  3.2361e-02,  4.0880e-01,
        -3.4814e-01, -4.8298e-01,  3.9618e-01,  2.8343e-01, -1.7262e-01,
         1.5334e-02,  6.2532e-02, -2.0133e-01, -1.8984e-01, -4.8683e-02,
        -1.9270e-01, -4.3521e-01, -1.6909e-01, -1.9550e-01,  2.0690e-02,
        -4.7134e-01, -4.7500e-01,  1.9740e-01,  6.4161e-02,  3.5095e-01,
         4.5753e-01, -3.4029e-01, -3.4397e-01, -1.2669e-01, -2.0633e-01,
        -4.4869e-01,  9.4026e-04,  1.5842e-01, -2.4247e-01,  2.1214e-01,
         4.4265e-01, -2.6994e-01, -3.1229e-01,  3.2927e-01, -1.5519e-02,
        -3.0340e-01,  9.4063e-02,  2.7415e-01, -3.9482e-01,  1.1031e-01,
        -4.9659e-01, -3.6661e-01, -4.4863e-01,  9.8545e-02, -2.7893e-01,
        -2.3949e-01,  4.0639e-01, -2.0283e-01,  1.4906e-02, -2.9676e-01,
        -3.2840e-01, -3.0483e-01, -2.3905e-01,  8.3705e-02,  8.6484e-02,
         1.9500e-01, -4.2343e-01,  2.1437e-01,  2.3815e-01,  2.2867e-01,
        -4.1594e-02,  4.6963e-01,  4.6971e-01, -1.3139e-01, -1.0649e-01,
         4.8120e-01, -8.4904e-03,  2.6275e-01, -1.6481e-01,  1.3258e-01,
         3.1986e-01,  1.8924e-01, -3.9509e-01, -3.0788e-01, -3.4900e-01,
         1.7317e-01, -8.2393e-02, -3.6563e-01, -2.3086e-01, -3.4014e-01,
        -9.9471e-02, -3.3152e-01, -2.8618e-02,  4.4337e-01,  1.5140e-01,
         4.7618e-01, -3.7750e-01, -3.5012e-01,  2.3558e-01,  2.4584e-01,
         2.3638e-01, -7.6482e-02, -4.5176e-02, -3.3561e-01, -2.7282e-01,
         4.6951e-01,  2.0158e-01,  3.8163e-01,  3.4112e-01,  1.7122e-01,
        -1.3265e-01,  2.2700e-01, -2.4832e-01, -2.6232e-01,  3.7953e-01,
         3.4744e-01])
tensor([-0.3452, -0.3580, -0.2097,  0.2301,  0.1092,  0.5924,  0.6356,  0.7643,
         0.0863, -0.1442,  0.4347,  0.3341, -0.2099,  0.6640, -0.2250, -0.1307,
         0.5715,  0.1788,  0.5036,  0.5428, -0.0649,  0.3253,  0.3380,  0.3873,
        -0.1092, -0.4605, -0.3889, -0.1043,  0.4549, -0.2171,  0.3564, -0.0366,
        -0.4731, -0.3824,  0.0653,  0.3174, -0.3225, -0.0718,  0.1485, -0.2677,
        -0.1325, -0.1636, -0.0599,  0.3937, -0.0784, -0.0650,  0.0640, -0.2984,
        -0.2773, -0.4404, -0.4588, -0.2603,  0.3881, -0.2196, -0.4262, -0.3661,
        -0.3077, -0.0908, -0.0089,  0.3692, -0.3703, -0.4980,  0.3636,  0.2483,
        -0.2043, -0.0228,  0.0297, -0.2316, -0.2231, -0.0802, -0.2127, -0.4769,
        -0.1954, -0.2448, -0.0146, -0.4994, -0.4947,  0.1654,  0.0099,  0.3221,
         0.4280, -0.3571, -0.3615, -0.1628, -0.2321, -0.4666, -0.0215,  0.1089,
        -0.2582,  0.1882,  0.3981, -0.2900, -0.3462,  0.2783, -0.0575, -0.3196,
         0.0553,  0.2371, -0.4172,  0.0579, -0.5279, -0.4034, -0.4677,  0.0731,
        -0.3041, -0.2715,  0.3451, -0.2397, -0.0060, -0.3401, -0.3536, -0.3197,
        -0.2603,  0.0558,  0.0553,  0.1540, -0.4635,  0.1744,  0.1947,  0.1965,
        -0.0708,  0.4096,  0.3959, -0.1577, -0.1423,  0.4395, -0.0304,  0.2176,
        -0.1908,  0.0934,  0.2608,  0.1494, -0.4144, -0.3243, -0.3641,  0.1457,
        -0.1226, -0.3874, -0.2431, -0.3611, -0.1337, -0.3630, -0.0814,  0.3938,
         0.1313,  0.4332, -0.3950, -0.3659,  0.2128,  0.2173,  0.2032, -0.0950,
        -0.0831, -0.3682, -0.2914,  0.4262,  0.1605,  0.3361,  0.3135,  0.1136,
        -0.1665,  0.1934, -0.2872, -0.3131,  0.3406,  0.3187])
##########################################################
{'prot_encoder.embedding.weight': tensor(0.0001), 'smi_encoder.embedding.weight': tensor(0.0002), 'prot_encoder.conv1d.bias': tensor(0.0002), 'prot_encoder.conv1d.weight': tensor(0.0002), 'smi_encoder.encoder.layers.1.self_attn.in_proj_bias': tensor(0.0003), 'smi_encoder.encoder.layers.0.self_attn.in_proj_bias': tensor(0.0004), 'smi_encoder.encoder.layers.1.self_attn.in_proj_weight': tensor(0.0010), 'smi_encoder.encoder.layers.0.linear1.bias': tensor(0.0016), 'smi_encoder.encoder.layers.1.self_attn.out_proj.weight': tensor(0.0016), 'smi_encoder.encoder.layers.0.norm2.bias': tensor(0.0017), 'smi_encoder.encoder.layers.0.self_attn.out_proj.bias': tensor(0.0019), 'smi_encoder.encoder.layers.1.self_attn.out_proj.bias': tensor(0.0019), 'prot_encoder.ffn.dense1.weight': tensor(0.0021), 'prot_encoder.encoder.layers.0.self_attn.in_proj_weight': tensor(0.0021), 'smi_encoder.encoder.layers.0.linear1.weight': tensor(0.0022), 'decoder.embedding.weight': tensor(0.0024), 'smi_encoder.encoder.layers.0.linear2.weight': tensor(0.0027), 'prot_encoder.ffn.dense1.bias': tensor(0.0027), 'smi_encoder.encoder.layers.1.norm1.bias': tensor(0.0028), 'prot_encoder.encoder.layers.0.self_attn.in_proj_bias': tensor(0.0029), 'prot_encoder.ffn.dense2.weight': tensor(0.0030), 'smi_encoder.encoder.layers.0.linear2.bias': tensor(0.0031), 'prot_encoder.encoder.layers.1.self_attn.in_proj_bias': tensor(0.0031), 'smi_encoder.encoder.layers.0.self_attn.in_proj_weight': tensor(0.0032), 'smi_encoder.encoder.layers.1.linear2.weight': tensor(0.0032), 'prot_encoder.encoder.layers.1.self_attn.in_proj_weight': tensor(0.0033), 'prot_encoder.encoder.layers.1.linear1.weight': tensor(0.0035), 'decoder.blks.block1.ffn.dense1.bias': tensor(0.0037), 'smi_encoder.encoder.layers.1.linear2.bias': tensor(0.0037), 'smi_encoder.encoder.layers.1.linear1.weight': tensor(0.0039), 'smi_encoder.encoder.layers.1.linear1.bias': tensor(0.0040), 'prot_encoder.encoder.layers.0.self_attn.out_proj.weight': tensor(0.0041), 'prot_encoder.encoder.layers.1.linear1.bias': tensor(0.0044), 'cross_encoder.encoder.layers.1.linear1.weight': tensor(0.0049), 'prot_encoder.addnorm.ln.bias': tensor(0.0050), 'cross_encoder.encoder.layers.1.self_attn.in_proj_bias': tensor(0.0052), 'smi_encoder.encoder.layers.0.norm1.bias': tensor(0.0054), 'cross_encoder.encoder.layers.0.self_attn.in_proj_weight': tensor(0.0056), 'smi_encoder.encoder.layers.0.self_attn.out_proj.weight': tensor(0.0057), 'decoder.blks.block1.ffn.dense1.weight': tensor(0.0058), 'cross_encoder.encoder.layers.1.linear1.bias': tensor(0.0058), 'cross_encoder.encoder.layers.1.linear2.weight': tensor(0.0059), 'smi_encoder.encoder.layers.0.norm1.weight': tensor(0.0061), 'cross_encoder.encoder.layers.1.self_attn.in_proj_weight': tensor(0.0062), 'smi_encoder.encoder.layers.0.norm2.weight': tensor(0.0064), 'decoder.blks.block0.attention2.W_q.weight': tensor(0.0068), 'cross_encoder.encoder.layers.0.norm1.weight': tensor(0.0068), 'smi_encoder.encoder.layers.1.norm1.weight': tensor(0.0068), 'prot_encoder.encoder.layers.0.linear2.weight': tensor(0.0070), 'decoder.blks.block1.ffn.dense2.weight': tensor(0.0071), 'cross_encoder.encoder.layers.0.self_attn.in_proj_bias': tensor(0.0072), 'decoder.blks.block0.attention2.W_k.weight': tensor(0.0075), 'prot_encoder.addnorm.ln.weight': tensor(0.0082), 'prot_encoder.encoder.layers.1.linear2.weight': tensor(0.0083), 'cross_encoder.encoder.layers.1.norm1.weight': tensor(0.0084), 'decoder.blks.block1.attention1.W_q.weight': tensor(0.0086), 'prot_encoder.encoder.layers.1.self_attn.out_proj.weight': tensor(0.0089), 'decoder.blks.block1.attention1.W_o.weight': tensor(0.0089), 'prot_encoder.encoder.layers.0.linear1.weight': tensor(0.0091), 'cross_encoder.encoder.layers.1.self_attn.out_proj.weight': tensor(0.0092), 'smi_encoder.encoder.layers.1.norm2.bias': tensor(0.0095), 'prot_encoder.encoder.layers.0.linear1.bias': tensor(0.0097), 'prot_encoder.encoder.layers.0.self_attn.out_proj.bias': tensor(0.0099), 'prot_encoder.encoder.layers.0.norm1.weight': tensor(0.0100), 'ffn.dense1.weight': tensor(0.0101), 'prot_encoder.encoder.layers.0.norm2.weight': tensor(0.0101), 'prot_encoder.encoder.layers.1.norm1.weight': tensor(0.0104), 'cross_encoder.encoder.layers.0.linear1.weight': tensor(0.0105), 'prot_encoder.encoder.layers.0.norm1.bias': tensor(0.0106), 'cross_encoder.encoder.layers.0.linear2.weight': tensor(0.0106), 'decoder.blks.block1.attention1.W_k.weight': tensor(0.0106), 'prot_encoder.encoder.layers.0.norm2.bias': tensor(0.0111), 'cross_encoder.encoder.layers.0.linear1.bias': tensor(0.0123), 'prot_encoder.encoder.layers.0.linear2.bias': tensor(0.0124), 'cross_encoder.encoder.layers.0.self_attn.out_proj.weight': tensor(0.0125), 'decoder.blks.block1.attention2.W_k.weight': tensor(0.0127), 'cross_encoder.encoder.layers.0.norm2.weight': tensor(0.0139), 'prot_encoder.encoder.layers.1.norm2.weight': tensor(0.0145), 'ffn.dense2.weight': tensor(0.0146), 'prot_encoder.encoder.layers.1.norm1.bias': tensor(0.0149), 'prot_encoder.ffn.dense2.bias': tensor(0.0156), 'ffn.dense1.bias': tensor(0.0160), 'prot_encoder.encoder.layers.1.linear2.bias': tensor(0.0168), 'smi_encoder.encoder.layers.1.norm2.weight': tensor(0.0178), 'prot_encoder.encoder.layers.1.self_attn.out_proj.bias': tensor(0.0178), 'decoder.blks.block1.attention2.W_v.weight': tensor(0.0180), 'decoder.blks.block0.ffn.dense2.weight': tensor(0.0182), 'decoder.blks.block1.addnorm1.ln.bias': tensor(0.0190), 'decoder.blks.block1.attention2.W_o.weight': tensor(0.0191), 'cross_encoder.encoder.layers.0.linear2.bias': tensor(0.0193), 'decoder.blks.block0.ffn.dense1.bias': tensor(0.0195), 'cross_encoder.encoder.layers.1.self_attn.out_proj.bias': tensor(0.0198), 'prot_encoder.encoder.layers.1.norm2.bias': tensor(0.0205), 'decoder.blks.block0.addnorm3.ln.weight': tensor(0.0214), 'decoder.blks.block0.ffn.dense1.weight': tensor(0.0216), 'cross_encoder.encoder.layers.0.norm1.bias': tensor(0.0217), 'decoder.blks.block1.attention1.W_v.weight': tensor(0.0219), 'addnorm.ln.weight': tensor(0.0219), 'decoder.blks.block0.ffn.dense2.bias': tensor(0.0225), 'decoder.blks.block1.addnorm1.ln.weight': tensor(0.0236), 'decoder.blks.block0.addnorm2.ln.weight': tensor(0.0244), 'cross_encoder.encoder.layers.0.norm2.bias': tensor(0.0244), 'cross_encoder.encoder.layers.1.norm1.bias': tensor(0.0251), 'decoder.blks.block0.attention2.W_v.weight': tensor(0.0253), 'cross_encoder.encoder.layers.0.self_attn.out_proj.bias': tensor(0.0262), 'decoder.blks.block0.addnorm2.ln.bias': tensor(0.0264), 'decoder.blks.block1.attention2.W_q.weight': tensor(0.0272), 'decoder.blks.block1.addnorm2.ln.weight': tensor(0.0283), 'ffn.dense2.bias': tensor(0.0285), 'cross_encoder.encoder.layers.1.linear2.bias': tensor(0.0293), 'cross_encoder.encoder.layers.1.norm2.weight': tensor(0.0306), 'cross_encoder.encoder.layers.1.norm2.bias': tensor(0.0307), 'decoder.blks.block1.ffn.dense2.bias': tensor(0.0312), 'decoder.blks.block0.addnorm3.ln.bias': tensor(0.0314), 'decoder.blks.block1.addnorm2.ln.bias': tensor(0.0321), 'decoder.blks.block0.attention1.W_k.weight': tensor(0.0322), 'decoder.blks.block0.attention2.W_o.weight': tensor(0.0324), 'addnorm.ln.bias': tensor(0.0356), 'decoder.blks.block0.addnorm1.ln.bias': tensor(0.0384), 'decoder.blks.block0.attention1.W_o.weight': tensor(0.0420), 'decoder.blks.block0.attention1.W_q.weight': tensor(0.0491), 'decoder.dense.bias': tensor(0.0528), 'decoder.blks.block0.attention1.W_v.weight': tensor(0.0603), 'decoder.blks.block0.addnorm1.ln.weight': tensor(0.0612), 'decoder.dense.weight': tensor(0.0764), 'decoder.blks.block1.addnorm3.ln.bias': tensor(0.7385), 'decoder.blks.block1.addnorm3.ln.weight': tensor(0.7778)}