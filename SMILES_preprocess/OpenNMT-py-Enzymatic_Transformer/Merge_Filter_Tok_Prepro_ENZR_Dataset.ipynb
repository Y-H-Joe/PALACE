{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This notebook is used to merge exported data from Reaxys, \n",
    "    clean the data, filter, tokenize and preprocess the dataset \n",
    "    for the training of the Enzymatic Transformer available at\n",
    "    https://github.com/reymond-group/OpenNMT-py\n",
    "\n",
    "    The environment is detailed on GitHub.\n",
    "\n",
    "    Initial .xls Reaxys extracted files are placed in /data\n",
    "    Dataset is output in /dataset\n",
    "\n",
    "    (The code is not perfectly clean and optimized, some steps might take \n",
    "    some time. Need around 4 min for an 83k reaction initial Reaxys extract)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total enteries in merged DF: \t83946\n"
     ]
    }
   ],
   "source": [
    "'''     COLLECT ALL EXCELS CSV SUB FILES AND MERGE THEM INTO A UNIQUE DATABASE             '''\n",
    "\n",
    "directory = str(os.getcwd()) + \"/Data\"\n",
    "iteration= 0\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".xls\"):\n",
    "        if iteration == 0:\n",
    "            df = pd.read_csv('Data/' + filename, sep='\\t')\n",
    "        else:\n",
    "            df2 = pd.read_csv('Data/' + filename, sep='\\t')\n",
    "            df = df.append(df2, 2)\n",
    "        iteration +=1\n",
    "\n",
    "print(\"Total enteries in merged DF: \\t\" + str(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2362\t no braket at all, means not a reaction at all\n",
      "0\t no '>>' together, means potential reactants between both '>'\n",
      "1461\t no reactant at all before '>>\n",
      "7446\t no products at all after '>>'\n",
      "775\t no catalyst or reagent description\n",
      "\n",
      "83945\t total enteries\n",
      "71901\t remaining enteries\n",
      "57781\t unique reaction SMILES\n"
     ]
    }
   ],
   "source": [
    "'''         FILTER REACTIONS THAT ARE NOT COMPLETE:           '''\n",
    "#Copy:\n",
    "df_filter = df.reset_index(drop=True)\n",
    "df_filter['Drop'] = ''\n",
    "\n",
    "count_no_Brak = 0\n",
    "count_not_2 = 0\n",
    "count_no_reactant = 0\n",
    "count_no_product = 0\n",
    "count_no_catalyst_nor_reagent_text = 0\n",
    "\n",
    "remaining = 0\n",
    "\n",
    "totalenteries = 0\n",
    "reaction_smiles = []\n",
    "\n",
    "for item in range(0, len(df_filter)-1):\n",
    "    reaction = df_filter.at[int(item), 'Reaction']\n",
    "    reagents = str(df_filter.at[int(item), 'Reagent'])\n",
    "    catalysts = str(df_filter.at[int(item), 'Catalyst'])\n",
    "\n",
    "    totalenteries += 1\n",
    "    if not \">\" in str(reaction):            #at least one >\n",
    "        count_no_Brak += 1\n",
    "        df_filter['Drop'][item] = '1'\n",
    "        continue\n",
    "    if not \">>\" in str(reaction):           #\n",
    "        count_not_2 += 1\n",
    "        df_filter['Drop'][item] = '1'\n",
    "        continue\n",
    "    if str(reaction).split(\">>\")[0] == \"\":\n",
    "        count_no_reactant += 1\n",
    "        df_filter['Drop'][item] = '1'\n",
    "        continue\n",
    "    if str(reaction).split(\">>\")[1] == \"\":\n",
    "        count_no_product += 1\n",
    "        df_filter['Drop'][item] = '1'\n",
    "        continue\n",
    "\n",
    "    if reagents == \"nan\":\n",
    "        reagents = \"\"\n",
    "    if catalysts == \"nan\":\n",
    "        catalysts = \"\"\n",
    "\n",
    "    if reagents == \"\" and catalysts == \"\":\n",
    "        count_no_catalyst_nor_reagent_text += 1\n",
    "        df_filter['Drop'][item] = '1'\n",
    "        continue\n",
    "\n",
    "    remaining += 1\n",
    "    reaction_smiles.append(reaction)\n",
    "    \n",
    "print(str(count_no_Brak) + \"\\t no braket at all, means not a reaction at all\")\n",
    "print(str(count_not_2) + \"\\t no '>>' together, means potential reactants between both '>'\")\n",
    "print(str(count_no_reactant) + \"\\t no reactant at all before '>>\")\n",
    "print(str(count_no_product) + \"\\t no products at all after '>>'\")\n",
    "print(str(count_no_catalyst_nor_reagent_text) + \"\\t no catalyst or reagent description\")\n",
    "print('')\n",
    "print(str(totalenteries) + \"\\t total enteries\")\n",
    "print(str(remaining) + \"\\t remaining enteries\")\n",
    "print(str(len(set(reaction_smiles))) + \"\\t unique reaction SMILES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(70096, 27)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "'''         Delete useless columns      '''\n",
    "\n",
    "df_filter2 = df_filter[df_filter['Drop'] == ''].reset_index(drop=True)\n",
    "del df_filter\n",
    "df_filter2 = df_filter2.drop_duplicates(subset=[\"Catalyst\", \"Reagent\", \"Reaction\"]).reset_index(drop=True)\n",
    "\n",
    "del df_filter2[\"Reaction: Links to Reaxys\"]\n",
    "del df_filter2[\"Data Count\"]\n",
    "del df_filter2[\"Number of Reaction Details\"]\n",
    "del df_filter2[\"Reaction Rank\"]\n",
    "del df_filter2[\"Record Type\"]\n",
    "del df_filter2[\"Bin\"]\n",
    "del df_filter2[\"Reaction Details: Reaction Classification\"]\n",
    "del df_filter2[\"Example label\"]\n",
    "del df_filter2[\"Multi-step Scheme\"]\n",
    "del df_filter2[\"Multi-step Details\"]\n",
    "del df_filter2[\"Named Reaction\"]\n",
    "del df_filter2[\"Type of reaction description (Reaction Details)\"]\n",
    "del df_filter2[\"Location\"]\n",
    "del df_filter2[\"References\"]\n",
    "del df_filter2[\"Unnamed: 41\"]\n",
    "del df_filter2[\"Links to Reaxys\"]\n",
    "\n",
    "df_filter2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''         CREATE NEW COLUMN: Add to \"Enzyme Keyword\" the FULL \";\" splitted element from Reagent or Catalyst           '''\n",
    "\n",
    "df_filter4 = df_filter2\n",
    "df_filter4[\"Enzyme Keyword\"] = \"\"\n",
    "\n",
    "#Keyword that are allowed to pass the filter:\n",
    "White_List = [\"Ase\", \"ase\", \"lysozyme\"]\n",
    "\n",
    "#For each reaction entery:\n",
    "for item in range(0, len(df_filter4)):\n",
    "    reagents = str(df_filter4.at[int(item), 'Reagent'])\n",
    "    catalysts = str(df_filter4.at[int(item), 'Catalyst'])\n",
    "    list_reag_cat = []\n",
    "    \n",
    "    #Concatenate Catalysts and Reagents:\n",
    "    for words in reagents.split(\"; \"):\n",
    "        list_reag_cat.append(words)\n",
    "    for words in catalysts.split(\"; \"):\n",
    "        list_reag_cat.append(words)\n",
    "\n",
    "    for white in White_List:\n",
    "        for element in list_reag_cat:\n",
    "            if str(white).casefold() in str(element).casefold():\n",
    "                if not str(element).casefold() in df_filter4[\"Enzyme Keyword\"][item]:\n",
    "                    if df_filter4[\"Enzyme Keyword\"][item] == \"\":\n",
    "                        df_filter4[\"Enzyme Keyword\"][item] = [str(element).casefold()]\n",
    "                    else:\n",
    "                        df_filter4[\"Enzyme Keyword\"][item].append(str(element).casefold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Enzyme presents:  57860\n",
      "UNIQUE Enzyme presents:  1722\n"
     ]
    }
   ],
   "source": [
    "'''         CREATE NEW COLUMN: EXTRACTED ENZYME SINGLE NAME ONLY            '''\n",
    "\n",
    "df_filter5 = df_filter4\n",
    "df_filter5[\"Enzyme Name\"] = \"\"\n",
    "\n",
    "#Keyword that are allowed to pass the filter:\n",
    "White_List = [\"Ase\", \"ase\", \"lysozyme\"]\n",
    "\n",
    "#List of ENZYME SINGLE WORD:\n",
    "Enzyme_ASE = []\n",
    "\n",
    "#For each reaction entery:\n",
    "for item in range(0, len(df_filter5)):\n",
    "    reagents = str(df_filter5.at[int(item), 'Reagent'])\n",
    "    catalysts = str(df_filter5.at[int(item), 'Catalyst'])\n",
    "\n",
    "    list_reag_cat = []\n",
    "    \n",
    "    #Concatenate Catalysts and Reagents:\n",
    "    for sentenses in reagents.split(\"; \"):\n",
    "        for word in sentenses.split(\" \"):\n",
    "            list_reag_cat.append(word)\n",
    "    for sentenses in catalysts.split(\"; \"):\n",
    "        for word in sentenses.split(\" \"):\n",
    "            list_reag_cat.append(word)\n",
    "\n",
    "    #for each element in the White List:\n",
    "    for white in White_List:\n",
    "        for element in list_reag_cat:\n",
    "            if str(white).casefold() in str(element).casefold():\n",
    "                \n",
    "                if not str(element).casefold() in df_filter5[\"Enzyme Name\"][item]:\n",
    "                    Enzyme_ASE.append(element)\n",
    "                    if df_filter5[\"Enzyme Name\"][item] == \"\":\n",
    "                        df_filter5[\"Enzyme Name\"][item] = [str(element).casefold()]\n",
    "                    else:\n",
    "                        df_filter5[\"Enzyme Name\"][item].append(str(element).casefold())\n",
    "\n",
    "print(\"Enzyme presents: \", len(Enzyme_ASE))\n",
    "print(\"UNIQUE Enzyme presents: \", len(set(Enzyme_ASE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 70096/70096 [00:19<00:00, 3506.28it/s]\n",
      "100%|██████████| 70096/70096 [00:16<00:00, 4143.05it/s]\n"
     ]
    }
   ],
   "source": [
    "'''     Replace list by a string for \"Enzyme Name\"         '''\n",
    "\n",
    "df_filter5_2 = df_filter5\n",
    "\n",
    "for element in tqdm(range(0, len(df_filter5_2[\"Enzyme Name\"]))):\n",
    "    df_filter5_2[\"Enzyme Name\"][element] = ' '.join(df_filter5_2[\"Enzyme Name\"][element])\n",
    "for element in tqdm(range(0, len(df_filter5_2[\"Enzyme Keyword\"]))):\n",
    "    df_filter5_2[\"Enzyme Keyword\"][element] = ' '.join(df_filter5_2[\"Enzyme Keyword\"][element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 70096/70096 [00:19<00:00, 3526.09it/s]\n",
      "100%|██████████| 70096/70096 [00:20<00:00, 3458.98it/s]\n"
     ]
    }
   ],
   "source": [
    "'''     Cleaning Names        '''\n",
    "\n",
    "df_filter6 = df_filter5_2\n",
    "\n",
    "for element in tqdm(range(0, len(df_filter6[\"Enzyme Name\"]))):\n",
    "    if \"(\" in df_filter6[\"Enzyme Name\"][element]:\n",
    "        if not \")\" in df_filter6[\"Enzyme Name\"][element]:\n",
    "            df_filter6[\"Enzyme Name\"][element] = df_filter6[\"Enzyme Name\"][element].replace(\"(\", \"\")\n",
    "    elif \")\" in df_filter6[\"Enzyme Name\"][element]:\n",
    "        if not \"(\" in df_filter6[\"Enzyme Name\"][element]:\n",
    "            df_filter6[\"Enzyme Name\"][element] = df_filter6[\"Enzyme Name\"][element].replace(\")\", \"\")\n",
    "\n",
    "    df_filter6[\"Enzyme Name\"][element] = df_filter6[\"Enzyme Name\"][element].casefold()\n",
    "\n",
    "    if \"ase,\" in df_filter6[\"Enzyme Name\"][element]:\n",
    "        df_filter6[\"Enzyme Name\"][element] = df_filter6[\"Enzyme Name\"][element].replace(\"ase,\", \"ase\")\n",
    "    if \"ases,\" in df_filter6[\"Enzyme Name\"][element]:\n",
    "        df_filter6[\"Enzyme Name\"][element] = df_filter6[\"Enzyme Name\"][element].replace(\"ases\", \"ase\")\n",
    "\n",
    "for element in tqdm(range(0, len(df_filter6[\"Enzyme Keyword\"]))):\n",
    "    if \"(\" in df_filter6[\"Enzyme Keyword\"][element]:\n",
    "        if not \")\" in df_filter6[\"Enzyme Keyword\"][element]:\n",
    "            df_filter6[\"Enzyme Keyword\"][element] = df_filter6[\"Enzyme Keyword\"][element].replace(\"(\", \"\")\n",
    "    elif \")\" in df_filter6[\"Enzyme Keyword\"][element]:\n",
    "        if not \"(\" in df_filter6[\"Enzyme Keyword\"][element]:\n",
    "            df_filter6[\"Enzyme Keyword\"][element] = df_filter6[\"Enzyme Keyword\"][element].replace(\")\", \"\")\n",
    "\n",
    "    df_filter6[\"Enzyme Keyword\"][element] = df_filter6[\"Enzyme Keyword\"][element].casefold()\n",
    "\n",
    "    if \"ase,\" in df_filter6[\"Enzyme Keyword\"][element]:\n",
    "        df_filter6[\"Enzyme Keyword\"][element] = df_filter6[\"Enzyme Keyword\"][element].replace(\"ase,\", \"ase\")\n",
    "    if \"ases,\" in df_filter6[\"Enzyme Keyword\"][element]:\n",
    "        df_filter6[\"Enzyme Keyword\"][element] = df_filter6[\"Enzyme Keyword\"][element].replace(\"ases\", \"ase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''         PREPROCESSING DATASETS            '''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from tqdm import tqdm \n",
    "\n",
    "from itertools import groupby\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer2 = Tokenizer(models.BPE())\n",
    "\n",
    "# Customize pre-tokenization and decoding\n",
    "tokenizer2.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer2.decoder = decoders.ByteLevel()\n",
    "tokenizer2.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "# And then train\n",
    "trainer = trainers.BpeTrainer(vocab_size=9000, min_frequency=2, limit_alphabet=55, special_tokens=['ase', 'hydro', 'mono', 'cyclo', 'thermo', 'im'])\n",
    "tokenizer2.train(trainer, [\"Tokenizer/Enzyme_Name_ForTocken.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule or reaction\n",
    "    \"\"\"\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def enzyme_sentence_tokenizer(sentence):\n",
    "    '''\n",
    "    Tokenize a sentenze, optimized for enzyme-like descriptions & names\n",
    "    '''\n",
    "    encoded = tokenizer2.encode(sentence)\n",
    "    my_list = [item for item in encoded.tokens if 'Ġ' != item]\n",
    "    my_list = [item.replace('Ġ', '_') for item in my_list]\n",
    "    my_list = ' '.join(my_list)\n",
    "    return my_list\n",
    "\n",
    "def Canonicalize_Reaction(smiles):\n",
    "    \n",
    "    for elements in smiles.replace('>>', '>').replace('>', '.').split('.'):\n",
    "        m = Chem.MolFromSmiles(elements, sanitize=False)\n",
    "        if m is None:\n",
    "            return False\n",
    "    \n",
    "    precursors = smiles.split('>')[0]\n",
    "    reactants = smiles.split('>')[1]\n",
    "    products = smiles.split('>')[2]\n",
    "\n",
    "    can_precursors = []\n",
    "    can_reactants = []\n",
    "    can_products = []\n",
    "\n",
    "    for precurs in precursors.split('.'):\n",
    "        mol = Chem.MolFromSmiles(precurs)\n",
    "        can_precursors.append(Chem.MolToSmiles(mol, canonical=True))\n",
    "    for reactant in reactants.split('.'):\n",
    "        mol = Chem.MolFromSmiles(reactant)\n",
    "        can_reactants.append(Chem.MolToSmiles(mol, canonical=True))\n",
    "    for product in products.split('.'):\n",
    "        mol = Chem.MolFromSmiles(product)\n",
    "        can_products.append(Chem.MolToSmiles(mol, canonical=True))\n",
    "\n",
    "    canon = '.'.join(can_precursors) + '>' + '.'.join(can_reactants) + '>' + '.'.join(can_products)\n",
    "\n",
    "    return canon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(70096, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "'''         FILTER DATAFRAME      \n",
    "\n",
    "SELECTION_ENZYME_DESC = 'Enzyme Keyword'    for full sentences        \n",
    "SELECTION_ENZYME_DESC = 'Enzyme Name'       for \"-ase\" word only  \n",
    "\n",
    "'''\n",
    "\n",
    "SELECTION_ENZYME_DESC = 'Enzyme Keyword'        \n",
    "\n",
    "df_filter = df_filter6[['Reaction', SELECTION_ENZYME_DESC]].copy()\n",
    "df_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(48304, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "'''         Remove reaction with no Enzyme Name     '''\n",
    "\n",
    "df_filter.dropna(subset = [SELECTION_ENZYME_DESC], inplace=True)\n",
    "df_filter = df_filter[df_filter['Enzyme Keyword'].str.len() > 0]\n",
    "df_filter.drop_duplicates(inplace=True)\n",
    "df_filter.reset_index(inplace=True)\n",
    "del df_filter['index']\n",
    "df_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 48304/48304 [00:57<00:00, 836.28it/s]\n",
      "100  invalid smiles reactions removed\n"
     ]
    }
   ],
   "source": [
    "'''             CANONICALIZE ALL REACTIONS AND REMOVE INVALIDS         '''\n",
    "count = 0\n",
    "for element in tqdm(range(0, len(df_filter))):\n",
    "    try:\n",
    "        reaction = Canonicalize_Reaction(df_filter['Reaction'][element])\n",
    "        if reaction != False:\n",
    "            df_filter['Reaction'][element] = reaction\n",
    "    except:\n",
    "        df_filter['Reaction'][element] = 'invalid'\n",
    "        count += 1\n",
    "\n",
    "df_filter = df_filter[~df_filter['Reaction'].str.contains('invalid')].reset_index(drop=True)\n",
    "print(count, \" invalid smiles reactions removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before  48204\n",
      "After  48203\n"
     ]
    }
   ],
   "source": [
    "'''         CHECK AGAIN FOR CANONICAL DUPLICATES       '''\n",
    "print(\"Before \", len(df_filter))\n",
    "df_filter = df_filter.drop_duplicates(subset=[\"Enzyme Keyword\", \"Reaction\"]).reset_index(drop=True)\n",
    "print(\"After \", len(df_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Removed:  16022  reactions\n",
      "Initially: 63664 reactions\n",
      "After removing 'NaN' Enzyme Names, remains: 27953 unique reactions, 32181 reactions in total\n"
     ]
    }
   ],
   "source": [
    "'''                 Remove reaction with Multiple products                                      '''\n",
    "count = 0\n",
    "\n",
    "df_filter['Reaction_Multi'] = \"\"\n",
    "for element in range(0, len(df_filter)):\n",
    "    if '.' in df_filter['Reaction'][element].split('>>')[1]:\n",
    "        df_filter['Reaction_Multi'][element] = 'DROP'\n",
    "        count += 1  \n",
    "\n",
    "print('Removed: ', count, ' reactions')\n",
    "\n",
    "indexNames = df_filter[df_filter['Reaction_Multi'] == 'DROP'].index\n",
    "df_filter.drop(indexNames, inplace=True)\n",
    "\n",
    "df_filter.reset_index(inplace=True)\n",
    "del df_filter['Reaction_Multi']\n",
    "del df_filter['index']\n",
    "\n",
    "print(\"Initially: \" + str(len(set(df['Reaction']))) + \" reactions\")\n",
    "print(\"After removing 'NaN' Enzyme Names, remains: \" + str(len(set(df_filter['Reaction']))) + \" unique reactions, \" + str(len(df_filter)) + \" reactions in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32181/32181 [00:26<00:00, 1228.58it/s]\n"
     ]
    }
   ],
   "source": [
    "'''     Combine Enzyme Name and Reaction SMILES     '''\n",
    "\n",
    "df_filter['Product'] = ''\n",
    "df_filter['TransformerIn'] = ''\n",
    "df_filter['TransformerOut'] = ''\n",
    "\n",
    "for index in tqdm(range(0, len(df_filter))):\n",
    "    df_filter['Product'][index] = df_filter['Reaction'][index].split('>>')[1]\n",
    "    df_filter['TransformerIn'][index] = smi_tokenizer(df_filter['Reaction'][index].split('>>')[0]) + \" > \" + enzyme_sentence_tokenizer(df_filter[SELECTION_ENZYME_DESC][index])\n",
    "    df_filter['TransformerOut'][index] = smi_tokenizer(df_filter['Product'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23335 different Products are presents\n"
     ]
    }
   ],
   "source": [
    "print(str(len(set(df_filter['Product']))) + \" different Products are presents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 23335/23335 [00:02<00:00, 8068.61it/s]\n",
      "Train proportion:  0.7997 %\n",
      "Test proportion:  0.1002 %\n",
      "Val proportion:  0.1001 %\n"
     ]
    }
   ],
   "source": [
    "'''         Distribute the PRODUCTS with weight            '''\n",
    "\n",
    "Products_org = df_filter['Product']\n",
    "\n",
    "count_train = 0\n",
    "count_test = 0\n",
    "count_val = 0\n",
    "\n",
    "Products_org = pd.DataFrame.from_dict(Counter(Products_org), orient='index').reset_index().rename(columns={0: 'count'})\n",
    "df_shuffled = Products_org.sample(len(Products_org)).reset_index()\n",
    "\n",
    "df_shuffled[\"set\"] = \"\"\n",
    "\n",
    "for item in tqdm(range(0, len(df_shuffled))):\n",
    "    \n",
    "    #In case \n",
    "    if count_train != 0 and count_test != 0 and count_val != 0:\n",
    "        ratio_train = (count_train / (count_train + count_test + count_val)) / 0.8\n",
    "        ratio_test = (count_test / (count_train + count_test + count_val)) / 0.1\n",
    "        ratio_val = (count_val / (count_train + count_test + count_val)) / 0.1\n",
    "\n",
    "        if ratio_train < 1:\n",
    "            count_train += df_shuffled['count'][item]\n",
    "            df_shuffled['set'][item] = \"Train\"\n",
    "        elif ratio_test < 1:\n",
    "            count_test += df_shuffled['count'][item]\n",
    "            df_shuffled['set'][item] = \"Test\"\n",
    "        elif ratio_val < 1:\n",
    "            count_val += df_shuffled['count'][item]\n",
    "            df_shuffled['set'][item] = \"Val\"\n",
    "        else:\n",
    "            assignation = random.randint(1, 3)\n",
    "\n",
    "            if assignation == 1:\n",
    "                count_train += df_shuffled['count'][item]\n",
    "                df_shuffled['set'][item] = \"Train\"\n",
    "            elif assignation == 2:\n",
    "                count_test += df_shuffled['count'][item]\n",
    "                df_shuffled['set'][item] = \"Test\"\n",
    "            elif assignation == 3:\n",
    "                count_val += df_shuffled['count'][item]\n",
    "                df_shuffled['set'][item] = \"Val\"\n",
    "\n",
    "    #In case no assignment yet:\n",
    "    else:\n",
    "        assignation = random.randint(1, 3)\n",
    "\n",
    "        if assignation == 1:\n",
    "            count_train += df_shuffled['count'][item]\n",
    "            df_shuffled['set'][item] = \"Train\"\n",
    "        elif assignation == 2:\n",
    "            count_test += df_shuffled['count'][item]\n",
    "            df_shuffled['set'][item] = \"Test\"\n",
    "        elif assignation == 3:\n",
    "            count_val += df_shuffled['count'][item]\n",
    "            df_shuffled['set'][item] = \"Val\"\n",
    "\n",
    "\n",
    "if count_train != 0 and count_test != 0 and count_val != 0:\n",
    "    print('Train proportion: ', str(round(count_train / (count_train + count_test + count_val), 4)), '%')\n",
    "    print('Test proportion: ', str(round(count_test / (count_train + count_test + count_val), 4)), '%')\n",
    "    print('Val proportion: ', str(round(count_val / (count_train + count_test + count_val), 4)), '%')\n",
    "\n",
    "df_shuffled = df_shuffled.set_index(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32181/32181 [00:16<00:00, 1899.49it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  Reaction                                     Enzyme Keyword  \\\n",
       "0  CC(=O)C(C)O>>CC(O)C(C)O  glucose dehydrogenase (r)-specific alcohol deh...   \n",
       "1  CC(=O)C(C)O>>CC(O)C(C)O  rabbit 3-hydroxyhexobarbital dehydrogenase (ak...   \n",
       "2  CC(=O)C(C)O>>CC(O)C(C)O  2,3-butanediol dehydrogenase from taiwanofungu...   \n",
       "3  O=C1CCCCC1>>O=C1CCCCCO1                              glucose dehydrogenase   \n",
       "4  O=C1CCCCC1>>O=C1CCCCCO1  recombinant fusion protein cyclohexanonemonoox...   \n",
       "\n",
       "       Product                                      TransformerIn  \\\n",
       "0   CC(O)C(C)O  C C ( = O ) C ( C ) O > _glucose _de hydro _ge...   \n",
       "1   CC(O)C(C)O  C C ( = O ) C ( C ) O > _rabbit _3 - hydro _x ...   \n",
       "2   CC(O)C(C)O  C C ( = O ) C ( C ) O > _2 , 3 - butanediol _d...   \n",
       "3  O=C1CCCCCO1  O = C 1 C C C C C 1 > _glucose _de hydro _gen ase   \n",
       "4  O=C1CCCCCO1  O = C 1 C C C C C 1 > _recombinant _fusion _pr...   \n",
       "\n",
       "          TransformerOut    Set  \n",
       "0    C C ( O ) C ( C ) O  Train  \n",
       "1    C C ( O ) C ( C ) O  Train  \n",
       "2    C C ( O ) C ( C ) O  Train  \n",
       "3  O = C 1 C C C C C O 1    Val  \n",
       "4  O = C 1 C C C C C O 1    Val  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Reaction</th>\n      <th>Enzyme Keyword</th>\n      <th>Product</th>\n      <th>TransformerIn</th>\n      <th>TransformerOut</th>\n      <th>Set</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CC(=O)C(C)O&gt;&gt;CC(O)C(C)O</td>\n      <td>glucose dehydrogenase (r)-specific alcohol deh...</td>\n      <td>CC(O)C(C)O</td>\n      <td>C C ( = O ) C ( C ) O &gt; _glucose _de hydro _ge...</td>\n      <td>C C ( O ) C ( C ) O</td>\n      <td>Train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CC(=O)C(C)O&gt;&gt;CC(O)C(C)O</td>\n      <td>rabbit 3-hydroxyhexobarbital dehydrogenase (ak...</td>\n      <td>CC(O)C(C)O</td>\n      <td>C C ( = O ) C ( C ) O &gt; _rabbit _3 - hydro _x ...</td>\n      <td>C C ( O ) C ( C ) O</td>\n      <td>Train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CC(=O)C(C)O&gt;&gt;CC(O)C(C)O</td>\n      <td>2,3-butanediol dehydrogenase from taiwanofungu...</td>\n      <td>CC(O)C(C)O</td>\n      <td>C C ( = O ) C ( C ) O &gt; _2 , 3 - butanediol _d...</td>\n      <td>C C ( O ) C ( C ) O</td>\n      <td>Train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>O=C1CCCCC1&gt;&gt;O=C1CCCCCO1</td>\n      <td>glucose dehydrogenase</td>\n      <td>O=C1CCCCCO1</td>\n      <td>O = C 1 C C C C C 1 &gt; _glucose _de hydro _gen ase</td>\n      <td>O = C 1 C C C C C O 1</td>\n      <td>Val</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>O=C1CCCCC1&gt;&gt;O=C1CCCCCO1</td>\n      <td>recombinant fusion protein cyclohexanonemonoox...</td>\n      <td>O=C1CCCCCO1</td>\n      <td>O = C 1 C C C C C 1 &gt; _recombinant _fusion _pr...</td>\n      <td>O = C 1 C C C C C O 1</td>\n      <td>Val</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "'''     Assign the assignment of the Product to the initial DF (df_filter)       '''\n",
    "\n",
    "df_filter[\"Set\"] = ''\n",
    "\n",
    "for item_toset in tqdm(range(0, len(df_filter))):\n",
    "    product = df_filter['Product'][item_toset]\n",
    "    df_filter['Set'][item_toset] = df_shuffled.loc[[product]]['set'][0]\n",
    "\n",
    "df_filter.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7996954724837637\n",
      "0.10024548646717008\n",
      "0.10005904104906622\n"
     ]
    }
   ],
   "source": [
    "'''     Distribute the Train / Test / Val splits into lists AND write into files          '''\n",
    "\n",
    "count_train_2 = 0\n",
    "count_test_2 = 0\n",
    "count_val_2 = 0\n",
    "\n",
    "TRAIN = []\n",
    "TEST = []\n",
    "VAL = []\n",
    "for item_toset in range(0, len(df_filter)):\n",
    "    if df_filter['Set'][item_toset] == 'Train':\n",
    "        count_train_2+=1\n",
    "        TRAIN.append(df_filter['TransformerIn'][item_toset] + '¢' + df_filter['TransformerOut'][item_toset])\n",
    "\n",
    "    if df_filter['Set'][item_toset] == 'Test':\n",
    "        count_test_2+=1\n",
    "        TEST.append(df_filter['TransformerIn'][item_toset] + '¢' + df_filter['TransformerOut'][item_toset])\n",
    "\n",
    "    if df_filter['Set'][item_toset] == 'Val':\n",
    "        count_val_2+=1\n",
    "        VAL.append(df_filter['TransformerIn'][item_toset] + '¢' + df_filter['TransformerOut'][item_toset])\n",
    "\n",
    "np.random.shuffle(TRAIN)\n",
    "np.random.shuffle(TEST)\n",
    "np.random.shuffle(VAL)\n",
    "\n",
    "src_train = []\n",
    "tgt_train = []\n",
    "\n",
    "src_test = []\n",
    "tgt_test = []\n",
    "\n",
    "src_val = []\n",
    "tgt_val = []\n",
    "\n",
    "\n",
    "for element in TRAIN:\n",
    "    src_train.append(element.split('¢')[0])\n",
    "    tgt_train.append(element.split('¢')[1])\n",
    "for element in TEST:\n",
    "    src_test.append(element.split('¢')[0])\n",
    "    tgt_test.append(element.split('¢')[1])\n",
    "for element in VAL:\n",
    "    src_val.append(element.split('¢')[0])\n",
    "    tgt_val.append(element.split('¢')[1])\n",
    "\n",
    "print(count_train_2 / (count_train_2 + count_test_2 + count_val_2))\n",
    "print(count_test_2 / (count_train_2 + count_test_2 + count_val_2))\n",
    "print(count_val_2 / (count_train_2 + count_test_2 + count_val_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''             WRITE TO FILES          '''\n",
    "\n",
    "target_folder_name = 'dataset/ENZR_Dataset_Full_Sentences/'\n",
    "\n",
    "#WRITE INTO FILES:\n",
    "with open(target_folder_name + 'src_train.txt', 'w') as f:\n",
    "    for item in src_train:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(target_folder_name + 'tgt_train.txt', 'w') as f:\n",
    "    for item in tgt_train:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(target_folder_name + 'src_test.txt', 'w') as f:\n",
    "    for item in src_test:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(target_folder_name + 'tgt_test.txt', 'w') as f:\n",
    "    for item in tgt_test:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(target_folder_name + 'src_val.txt', 'w') as f:\n",
    "    for item in src_val:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(target_folder_name + 'tgt_val.txt', 'w') as f:\n",
    "    for item in tgt_val:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}